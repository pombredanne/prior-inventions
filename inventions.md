
EXHIBIT A: LIST OF PRIOR INVENTIONS AND ORIGINAL WORKS OF AUTHORSHIP EXCLUDED UNDER SECTION 4(a) CONFLICTING AGREEMENTS DISCLOSED UNDER SECTION 10(B)
================================================================================================


The following is a list of (i) all inventions that belong solely to me or belong to me jointly with others, and
that relate in any way to any of the Companyâ€™s actual or proposed businesses, products, services, or
research and development, and which are not assigned to the Company pursuant to this Agreement and (ii)
all agreements, if any, with a current or former client, employer, or any other person or entity, that may
restrict my ability to accept employment with the Company or my ability to recruit or engage customers or
service providers on behalf of the Company, or otherwise relate to or restrict my ability to perform my
duties for the Company or any obligation I may have to the Company:





Transparency Camp 2013 Tweets
------------------------------------------------------------------------------------------------
**Title**: Transparency Camp 2013 Tweets

**Web address**: https://classic.scraperwiki.com/scrapers/transparency_camp_2013_tweets/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

Cornell Police Daily Crime Log
------------------------------------------------------------------------------------------------
**Title**: Cornell Police Daily Crime Log

**Web address**: https://classic.scraperwiki.com/scrapers/cornell_police_daily_crime_log/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

cast
------------------------------------------------------------------------------------------------
**Title**: cast

**Web address**: https://classic.scraperwiki.com/scrapers/cast/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

argentinacompra
------------------------------------------------------------------------------------------------
**Title**: argentinacompra

**Web address**: https://classic.scraperwiki.com/scrapers/argentinacompra/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

python-cgi
------------------------------------------------------------------------------------------------
**Title**: python-cgi

**Web address**: https://classic.scraperwiki.com/views/python-cgi/

**Date**: October 2013

**Description**: 
        
            This view has no description. 
                Sign in to add one.
            
        
        

dumptruck-web
------------------------------------------------------------------------------------------------
**Title**: dumptruck-web

**Web address**: https://classic.scraperwiki.com/scrapers/dumptruck-web/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

scraperwiki_business_card
------------------------------------------------------------------------------------------------
**Title**: scraperwiki_business_card

**Web address**: https://classic.scraperwiki.com/views/scraperwiki_business_card/

**Date**: October 2013

**Description**: 
        
            This view has no description. 
                Sign in to add one.
            
        
        

businesscard
------------------------------------------------------------------------------------------------
**Title**: businesscard

**Web address**: https://classic.scraperwiki.com/scrapers/businesscard/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

Basic Twitter Scraper
------------------------------------------------------------------------------------------------
**Title**: Basic Twitter Scraper

**Web address**: https://classic.scraperwiki.com/scrapers/basic_twitter_scraper_432/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

scraperwiki_local
------------------------------------------------------------------------------------------------
**Title**: scraperwiki_local

**Web address**: https://classic.scraperwiki.com/scrapers/scraperwiki_local/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

webcron
------------------------------------------------------------------------------------------------
**Title**: webcron

**Web address**: https://classic.scraperwiki.com/scrapers/webcron/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

tweets_about_me
------------------------------------------------------------------------------------------------
**Title**: tweets_about_me

**Web address**: https://classic.scraperwiki.com/scrapers/tweets_about_me/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

r
------------------------------------------------------------------------------------------------
**Title**: r

**Web address**: https://classic.scraperwiki.com/scrapers/r/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

hiztegia
------------------------------------------------------------------------------------------------
**Title**: hiztegia

**Web address**: https://classic.scraperwiki.com/scrapers/hiztegia/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

scraperwiki_tags
------------------------------------------------------------------------------------------------
**Title**: scraperwiki_tags

**Web address**: https://classic.scraperwiki.com/scrapers/scraperwiki_tags/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

enda
------------------------------------------------------------------------------------------------
**Title**: enda

**Web address**: https://classic.scraperwiki.com/scrapers/enda/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

edna
------------------------------------------------------------------------------------------------
**Title**: edna

**Web address**: https://classic.scraperwiki.com/scrapers/edna/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

42
------------------------------------------------------------------------------------------------
**Title**: 42

**Web address**: https://classic.scraperwiki.com/scrapers/42/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

codecampruby
------------------------------------------------------------------------------------------------
**Title**: codecampruby

**Web address**: https://classic.scraperwiki.com/scrapers/codecampruby/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

codecamp
------------------------------------------------------------------------------------------------
**Title**: codecamp

**Web address**: https://classic.scraperwiki.com/scrapers/codecamp/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

romney_v_obama
------------------------------------------------------------------------------------------------
**Title**: romney_v_obama

**Web address**: https://classic.scraperwiki.com/scrapers/romney_v_obama/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

postliste-nrk
------------------------------------------------------------------------------------------------
**Title**: postliste-nrk

**Web address**: https://classic.scraperwiki.com/scrapers/postliste-nrk/

**Date**: October 2013

**Description**: 
        
             

YAML-tagger:
Type: unknown
Status: unfinished
Name: Norsk Rikskringkasting AS (NRK)

        
        

Yoga teachers per capita
------------------------------------------------------------------------------------------------
**Title**: Yoga teachers per capita

**Web address**: https://classic.scraperwiki.com/scrapers/yoga_teachers_per_capita/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

Inserting complex types
------------------------------------------------------------------------------------------------
**Title**: Inserting complex types

**Web address**: https://classic.scraperwiki.com/scrapers/inserting_complex_types/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

hpkcc youth programs database
------------------------------------------------------------------------------------------------
**Title**: hpkcc youth programs database

**Web address**: https://classic.scraperwiki.com/scrapers/hpkcc_youth_programs_database/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

jobs_for_youth_chicago
------------------------------------------------------------------------------------------------
**Title**: jobs_for_youth_chicago

**Web address**: https://classic.scraperwiki.com/scrapers/jobs_for_youth_chicago/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

Chicago is the World Directory of Youth Organizations
------------------------------------------------------------------------------------------------
**Title**: Chicago is the World Directory of Youth Organizations

**Web address**: https://classic.scraperwiki.com/scrapers/chicago_is_the_world/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

spaces in table names
------------------------------------------------------------------------------------------------
**Title**: spaces in table names

**Web address**: https://classic.scraperwiki.com/scrapers/spaces_in_table_names_1/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

spaces in table names
------------------------------------------------------------------------------------------------
**Title**: spaces in table names

**Web address**: https://classic.scraperwiki.com/scrapers/spaces_in_table_names/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

homepage_experiment_analysis
------------------------------------------------------------------------------------------------
**Title**: homepage_experiment_analysis

**Web address**: https://classic.scraperwiki.com/scrapers/homepage_experiment_analysis/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

git_deploy_demo2
------------------------------------------------------------------------------------------------
**Title**: git_deploy_demo2

**Web address**: https://classic.scraperwiki.com/views/git_deploy_demo2/

**Date**: October 2013

**Description**: 
        
            This view has no description. 
                Sign in to add one.
            
        
        

git_deploy_demo
------------------------------------------------------------------------------------------------
**Title**: git_deploy_demo

**Web address**: https://classic.scraperwiki.com/views/git_deploy_demo_1/

**Date**: October 2013

**Description**: 
        
            This view has no description. 
                Sign in to add one.
            
        
        

git_deploy_demo
------------------------------------------------------------------------------------------------
**Title**: git_deploy_demo

**Web address**: https://classic.scraperwiki.com/views/git_deploy_demo/

**Date**: October 2013

**Description**: 
        
            This view has no description. 
                Sign in to add one.
            
        
        

usps_lookup
------------------------------------------------------------------------------------------------
**Title**: usps_lookup

**Web address**: https://classic.scraperwiki.com/views/usps_lookup/

**Date**: October 2013

**Description**: 
        
            This view has no description. 
                Sign in to add one.
            
        
        

ON CONFLICT
------------------------------------------------------------------------------------------------
**Title**: ON CONFLICT

**Web address**: https://classic.scraperwiki.com/scrapers/on_conflict/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

tcamp_tweets
------------------------------------------------------------------------------------------------
**Title**: tcamp_tweets

**Web address**: https://classic.scraperwiki.com/scrapers/tcamp_tweets/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

twitter_search
------------------------------------------------------------------------------------------------
**Title**: twitter_search

**Web address**: https://classic.scraperwiki.com/scrapers/twitter_search/

**Date**: October 2013

**Description**: 
        
            	This makes scraping twitter for multiple search terms a piece of cake.

	Just start a new Python scraper, and type this at the top:

	from scraperwiki import swimport
swimport('twitter_search').search(['some_phrase', 'from:some_user'])

	Set your scraper to run daily and it'll automatically fill its datastore with the latest tweets.
        
        

Transparency Camp Attendees
------------------------------------------------------------------------------------------------
**Title**: Transparency Camp Attendees

**Web address**: https://classic.scraperwiki.com/scrapers/transparency_camp_attendees/

**Date**: October 2013

**Description**: 
        
            	Transparency camp attendees by year scraped from Eventbrite and then aggregated by Twitter handle
        
        

ncr5
------------------------------------------------------------------------------------------------
**Title**: ncr5

**Web address**: https://classic.scraperwiki.com/scrapers/ncr5/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

Estimates
------------------------------------------------------------------------------------------------
**Title**: Estimates

**Web address**: https://classic.scraperwiki.com/views/estimates/

**Date**: October 2013

**Description**: 
        
            This view has no description. 
                Sign in to add one.
            
        
        

What does show_tables return?
------------------------------------------------------------------------------------------------
**Title**: What does show_tables return?

**Web address**: https://classic.scraperwiki.com/scrapers/what_does_show_tables_return/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

nedbank_branches2
------------------------------------------------------------------------------------------------
**Title**: nedbank_branches2

**Web address**: https://classic.scraperwiki.com/scrapers/nedbank_branches2/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

strip_address
------------------------------------------------------------------------------------------------
**Title**: strip_address

**Web address**: https://classic.scraperwiki.com/scrapers/strip_address/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

Primus Platinum audits
------------------------------------------------------------------------------------------------
**Title**: Primus Platinum audits

**Web address**: https://classic.scraperwiki.com/scrapers/primus_platinum_audits/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

Things happen when you don't commit.
------------------------------------------------------------------------------------------------
**Title**: Things happen when you don't commit.

**Web address**: https://classic.scraperwiki.com/scrapers/things_happen_when_you_do_not_commit/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

gmail
------------------------------------------------------------------------------------------------
**Title**: gmail

**Web address**: https://classic.scraperwiki.com/scrapers/gmail/

**Date**: October 2013

**Description**: 
        
            I log in to email and open pdf attachments. I could be expanded to do a lot more.


# Log in
g = Gmail('thomas@scraperwiki.com', 'uo9a8d,9.832')

# List of urls
message_urls = g.urls_of_emails_with_attachments():

# Get the first pdf attachment from the third email with an attachment
g.get_first_pdf_attachment(message_urls[2])

        
        

gmail-attachment-downloader
------------------------------------------------------------------------------------------------
**Title**: gmail-attachment-downloader

**Web address**: https://classic.scraperwiki.com/scrapers/gmail-attachment-downloader_1/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

cornell_gifting
------------------------------------------------------------------------------------------------
**Title**: cornell_gifting

**Web address**: https://classic.scraperwiki.com/scrapers/cornell_gifting/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

Ithala Finance Corporates - Branches
------------------------------------------------------------------------------------------------
**Title**: Ithala Finance Corporates - Branches

**Web address**: https://classic.scraperwiki.com/scrapers/ithala_finance_corporates_-_branches/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

retrieve-mix-scrapers
------------------------------------------------------------------------------------------------
**Title**: retrieve-mix-scrapers

**Web address**: https://classic.scraperwiki.com/views/retrieve-mix-scrapers/

**Date**: October 2013

**Description**: 
        
            This view has no description. 
                Sign in to add one.
            
        
        

mix_backup_db
------------------------------------------------------------------------------------------------
**Title**: mix_backup_db

**Web address**: https://classic.scraperwiki.com/scrapers/mix_backup_db/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

mix_backup
------------------------------------------------------------------------------------------------
**Title**: mix_backup

**Web address**: https://classic.scraperwiki.com/views/mix_backup/

**Date**: October 2013

**Description**: 
        
            This view has no description. 
                Sign in to add one.
            
        
        

safaricom
------------------------------------------------------------------------------------------------
**Title**: safaricom

**Web address**: https://classic.scraperwiki.com/scrapers/safaricom/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

combine_mix_scraper_spreadsheets
------------------------------------------------------------------------------------------------
**Title**: combine_mix_scraper_spreadsheets

**Web address**: https://classic.scraperwiki.com/scrapers/combine_mix_scraper_spreadsheets_1/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

cpu-py
------------------------------------------------------------------------------------------------
**Title**: cpu-py

**Web address**: https://classic.scraperwiki.com/scrapers/cpu-py/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

sasra2
------------------------------------------------------------------------------------------------
**Title**: sasra2

**Web address**: https://classic.scraperwiki.com/scrapers/sasra2/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

ncr4
------------------------------------------------------------------------------------------------
**Title**: ncr4

**Web address**: https://classic.scraperwiki.com/scrapers/ncr4/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

hsph faculty
------------------------------------------------------------------------------------------------
**Title**: hsph faculty

**Web address**: https://classic.scraperwiki.com/scrapers/hsph_faculty_1/

**Date**: October 2013

**Description**: 
        
            	Jack Dennerlein is leaving next year, so who could be adviser? People with similar faculty profile web pages might be a place to start looking.
        
        

uk visas
------------------------------------------------------------------------------------------------
**Title**: uk visas

**Web address**: https://classic.scraperwiki.com/scrapers/uk_visas/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

Untitledaoeuaoeu
------------------------------------------------------------------------------------------------
**Title**: Untitledaoeuaoeu

**Web address**: https://classic.scraperwiki.com/scrapers/untitledaoeuaoeu/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

F000010
------------------------------------------------------------------------------------------------
**Title**: F000010

**Web address**: https://classic.scraperwiki.com/scrapers/f000010/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

csv2sw_example
------------------------------------------------------------------------------------------------
**Title**: csv2sw_example

**Web address**: https://classic.scraperwiki.com/scrapers/csv2sw_example/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

tom_birthday_emails
------------------------------------------------------------------------------------------------
**Title**: tom_birthday_emails

**Web address**: https://classic.scraperwiki.com/scrapers/tom_birthday_emails/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

jdcdc_stream_c_python_pdfs
------------------------------------------------------------------------------------------------
**Title**: jdcdc_stream_c_python_pdfs

**Web address**: https://classic.scraperwiki.com/scrapers/jdcdc_stream_c_python_pdfs/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

jdcdc_stream_c_ruby
------------------------------------------------------------------------------------------------
**Title**: jdcdc_stream_c_ruby

**Web address**: https://classic.scraperwiki.com/scrapers/jdcdc_stream_c_ruby/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

jdcdc_stream_c_python
------------------------------------------------------------------------------------------------
**Title**: jdcdc_stream_c_python

**Web address**: https://classic.scraperwiki.com/scrapers/jdcdc_stream_c_python/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

JDCDC Guest list
------------------------------------------------------------------------------------------------
**Title**: JDCDC Guest list

**Web address**: https://classic.scraperwiki.com/scrapers/jdcdc_guest_list/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

hsph faculty
------------------------------------------------------------------------------------------------
**Title**: hsph faculty

**Web address**: https://classic.scraperwiki.com/scrapers/hsph_faculty/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

bucketwheel_get_test
------------------------------------------------------------------------------------------------
**Title**: bucketwheel_get_test

**Web address**: https://classic.scraperwiki.com/scrapers/bucketwheel_get_test/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

bucketwheel (stack only stores urls)
------------------------------------------------------------------------------------------------
**Title**: bucketwheel (stack only stores urls)

**Web address**: https://classic.scraperwiki.com/scrapers/bucketwheel_get/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

bucketwheel test
------------------------------------------------------------------------------------------------
**Title**: bucketwheel test

**Web address**: https://classic.scraperwiki.com/scrapers/bucketwheel_test/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

bucketwheel
------------------------------------------------------------------------------------------------
**Title**: bucketwheel

**Web address**: https://classic.scraperwiki.com/scrapers/bucketwheel/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

Geocode all mix scrapers except Standard Bank foreign exchanges for South Africa
------------------------------------------------------------------------------------------------
**Title**: Geocode all mix scrapers except Standard Bank foreign exchanges for South Africa

**Web address**: https://classic.scraperwiki.com/scrapers/geocode_all_mix_scrapers_except_standard_bank_fore/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

Geocode all mix scrapers final for South Africa
------------------------------------------------------------------------------------------------
**Title**: Geocode all mix scrapers final for South Africa

**Web address**: https://classic.scraperwiki.com/scrapers/geocode_all_mix_scrapers_final_for_south_africa/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

absa3
------------------------------------------------------------------------------------------------
**Title**: absa3

**Web address**: https://classic.scraperwiki.com/scrapers/absa3/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

Blue Financial Services
------------------------------------------------------------------------------------------------
**Title**: Blue Financial Services

**Web address**: https://classic.scraperwiki.com/scrapers/blue_financial_services/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

Geocode all mix scrapers
------------------------------------------------------------------------------------------------
**Title**: Geocode all mix scrapers

**Web address**: https://classic.scraperwiki.com/scrapers/geocode_all_mix_scrapers_3/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

Geocode all mix scrapers
------------------------------------------------------------------------------------------------
**Title**: Geocode all mix scrapers

**Web address**: https://classic.scraperwiki.com/scrapers/geocode_all_mix_scrapers_2/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

Sara's cool thing
------------------------------------------------------------------------------------------------
**Title**: Sara's cool thing

**Web address**: https://classic.scraperwiki.com/views/saras_cool_thing/

**Date**: October 2013

**Description**: 
        
            This view has no description. 
                Sign in to add one.
            
        
        

Basic Twitter Scraper
------------------------------------------------------------------------------------------------
**Title**: Basic Twitter Scraper

**Web address**: https://classic.scraperwiki.com/scrapers/basic_twitter_scraper_373/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

exhaust
------------------------------------------------------------------------------------------------
**Title**: exhaust

**Web address**: https://classic.scraperwiki.com/scrapers/exhaust/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

Geocode all mix scrapers
------------------------------------------------------------------------------------------------
**Title**: Geocode all mix scrapers

**Web address**: https://classic.scraperwiki.com/scrapers/geocode_all_mix_scrapers_1/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

uob
------------------------------------------------------------------------------------------------
**Title**: uob

**Web address**: https://classic.scraperwiki.com/scrapers/uob/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

Bill duplicate removal
------------------------------------------------------------------------------------------------
**Title**: Bill duplicate removal

**Web address**: https://classic.scraperwiki.com/scrapers/bill_duplicate_removal/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

wtf
------------------------------------------------------------------------------------------------
**Title**: wtf

**Web address**: https://classic.scraperwiki.com/scrapers/wtf_3/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

realpeople
------------------------------------------------------------------------------------------------
**Title**: realpeople

**Web address**: https://classic.scraperwiki.com/scrapers/realpeople/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

thuthukani_financial_services
------------------------------------------------------------------------------------------------
**Title**: thuthukani_financial_services

**Web address**: https://classic.scraperwiki.com/scrapers/thuthukani_financial_services/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

wtf
------------------------------------------------------------------------------------------------
**Title**: wtf

**Web address**: https://classic.scraperwiki.com/scrapers/wtf_2/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

nyda
------------------------------------------------------------------------------------------------
**Title**: nyda

**Web address**: https://classic.scraperwiki.com/scrapers/nyda/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

rwandamicrofinance
------------------------------------------------------------------------------------------------
**Title**: rwandamicrofinance

**Web address**: https://classic.scraperwiki.com/scrapers/rwandamicrofinance/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

sasra
------------------------------------------------------------------------------------------------
**Title**: sasra

**Web address**: https://classic.scraperwiki.com/scrapers/sasra_1/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

amfikenya
------------------------------------------------------------------------------------------------
**Title**: amfikenya

**Web address**: https://classic.scraperwiki.com/scrapers/amfikenya/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

DC Sex Offenders
------------------------------------------------------------------------------------------------
**Title**: DC Sex Offenders

**Web address**: https://classic.scraperwiki.com/scrapers/dc_sex_offenders/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

Geocode all mix scrapers
------------------------------------------------------------------------------------------------
**Title**: Geocode all mix scrapers

**Web address**: https://classic.scraperwiki.com/scrapers/geocode_all_mix_scrapers/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

What happens to columns with the same name on joins?
------------------------------------------------------------------------------------------------
**Title**: What happens to columns with the same name on joins?

**Web address**: https://classic.scraperwiki.com/scrapers/what_happens_to_columns_with_the_same_name_on_join/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

Combine mix scraper spreadsheets
------------------------------------------------------------------------------------------------
**Title**: Combine mix scraper spreadsheets

**Web address**: https://classic.scraperwiki.com/views/combine_mix_scraper_spreadsheets/

**Date**: October 2013

**Description**: 
        
            This view has no description. 
                Sign in to add one.
            
        
        

MC 5317 SXSW overview
------------------------------------------------------------------------------------------------
**Title**: MC 5317 SXSW overview

**Web address**: https://classic.scraperwiki.com/scrapers/mc_5317_sxsw_overview/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

Does anyone use joins? And how much Twitter?
------------------------------------------------------------------------------------------------
**Title**: Does anyone use joins? And how much Twitter?

**Web address**: https://classic.scraperwiki.com/scrapers/does_anyone_use_joins/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

Wellcommons
------------------------------------------------------------------------------------------------
**Title**: Wellcommons

**Web address**: https://classic.scraperwiki.com/scrapers/wellcommons_1/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

gsa
------------------------------------------------------------------------------------------------
**Title**: gsa

**Web address**: https://classic.scraperwiki.com/scrapers/gsa/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

ptil_test_csv
------------------------------------------------------------------------------------------------
**Title**: ptil_test_csv

**Web address**: https://classic.scraperwiki.com/scrapers/ptil_test_csv/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

Health Users
------------------------------------------------------------------------------------------------
**Title**: Health Users

**Web address**: https://classic.scraperwiki.com/scrapers/health/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

South Africa Geocoding Validation
------------------------------------------------------------------------------------------------
**Title**: South Africa Geocoding Validation

**Web address**: https://classic.scraperwiki.com/scrapers/south_africa_geocoding_validation/

**Date**: October 2013

**Description**: 
        
            	select count(*), `number-of-matches` from `accuracy` where `address-column` = "town-country" group by `number-of-matches`
        
        

nursing_homes_in_illinois
------------------------------------------------------------------------------------------------
**Title**: nursing_homes_in_illinois

**Web address**: https://classic.scraperwiki.com/scrapers/nursing_homes_in_illinois/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

Health
------------------------------------------------------------------------------------------------
**Title**: Health

**Web address**: https://classic.scraperwiki.com/scrapers/wellcommons/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

nicar_stream_c_python
------------------------------------------------------------------------------------------------
**Title**: nicar_stream_c_python

**Web address**: https://classic.scraperwiki.com/scrapers/nicar_stream_c_python/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

nicar_stream_c_ruby
------------------------------------------------------------------------------------------------
**Title**: nicar_stream_c_ruby

**Web address**: https://classic.scraperwiki.com/scrapers/nicar_stream_c_ruby/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

trello hacks
------------------------------------------------------------------------------------------------
**Title**: trello hacks

**Web address**: https://classic.scraperwiki.com/views/trello_hacks/

**Date**: October 2013

**Description**: 
        
            This view has no description. 
                Sign in to add one.
            
        
        

trello hacks
------------------------------------------------------------------------------------------------
**Title**: trello hacks

**Web address**: https://classic.scraperwiki.com/views/trello_hacks_1/

**Date**: October 2013

**Description**: 
        
            This view has no description. 
                Sign in to add one.
            
        
        

find
------------------------------------------------------------------------------------------------
**Title**: find

**Web address**: https://classic.scraperwiki.com/scrapers/find/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

unionreportspy
------------------------------------------------------------------------------------------------
**Title**: unionreportspy

**Web address**: https://classic.scraperwiki.com/scrapers/unionreportspy/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

Merge to stable
------------------------------------------------------------------------------------------------
**Title**: Merge to stable

**Web address**: https://classic.scraperwiki.com/views/merge_to_stable/

**Date**: October 2013

**Description**: 
        
            This view has no description. 
                Sign in to add one.
            
        
        

unionreports
------------------------------------------------------------------------------------------------
**Title**: unionreports

**Web address**: https://classic.scraperwiki.com/scrapers/unionreports/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

Central Bank of Kenya
------------------------------------------------------------------------------------------------
**Title**: Central Bank of Kenya

**Web address**: https://classic.scraperwiki.com/scrapers/central_bank_of_kenya/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

sasra
------------------------------------------------------------------------------------------------
**Title**: sasra

**Web address**: https://classic.scraperwiki.com/scrapers/sasra/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

Banque Commerciale Du Rwanda Ltd.
------------------------------------------------------------------------------------------------
**Title**: Banque Commerciale Du Rwanda Ltd.

**Web address**: https://classic.scraperwiki.com/scrapers/banque_commerciale_du_rwanda_ltd/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

national_bank_of_rwanda
------------------------------------------------------------------------------------------------
**Title**: national_bank_of_rwanda

**Web address**: https://classic.scraperwiki.com/scrapers/national_bank_of_rwanda/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

Banque Populaire du Rwanda Ltd
------------------------------------------------------------------------------------------------
**Title**: Banque Populaire du Rwanda Ltd

**Web address**: https://classic.scraperwiki.com/scrapers/banque_populaire_du_rwanda_ltd/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

South Africa Nominatim Geocoding Validation
------------------------------------------------------------------------------------------------
**Title**: South Africa Nominatim Geocoding Validation

**Web address**: https://classic.scraperwiki.com/scrapers/south_africa_nominatim_geocoding_validation/

**Date**: October 2013

**Description**: 
        
            	Number of non-matchesselect `address-column`, count(*) as "Non-matches" from `geocoded` where number_of_matches = 0 group by `address-column`
        
        

GP Surgeries & Staff
------------------------------------------------------------------------------------------------
**Title**: GP Surgeries & Staff

**Web address**: https://classic.scraperwiki.com/scrapers/gp_surgeries_staff_1/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

crunchbase_newly_funded
------------------------------------------------------------------------------------------------
**Title**: crunchbase_newly_funded

**Web address**: https://classic.scraperwiki.com/scrapers/crunchbase_newly_funded/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

ncr3
------------------------------------------------------------------------------------------------
**Title**: ncr3

**Web address**: https://classic.scraperwiki.com/scrapers/ncr3/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

Ithaca Art Trail Artists
------------------------------------------------------------------------------------------------
**Title**: Ithaca Art Trail Artists

**Web address**: https://classic.scraperwiki.com/scrapers/ithaca_art_trail_artists/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

capitec_bank_test
------------------------------------------------------------------------------------------------
**Title**: capitec_bank_test

**Web address**: https://classic.scraperwiki.com/scrapers/capitec_bank_test/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

wtf
------------------------------------------------------------------------------------------------
**Title**: wtf

**Web address**: https://classic.scraperwiki.com/scrapers/wtf_1/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

Bank of Kigali Limited
------------------------------------------------------------------------------------------------
**Title**: Bank of Kigali Limited

**Web address**: https://classic.scraperwiki.com/scrapers/bank_of_kigali/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

wtf
------------------------------------------------------------------------------------------------
**Title**: wtf

**Web address**: https://classic.scraperwiki.com/scrapers/wtf/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

Libraries to install
------------------------------------------------------------------------------------------------
**Title**: Libraries to install

**Web address**: https://classic.scraperwiki.com/views/libraries_to_install/

**Date**: October 2013

**Description**: 
        
            	I check whether library versions are installed and keep a record of that. Add library names to the LIBNAMES variable in order to test more libraries.
        
        

mikulski_python_scraper2
------------------------------------------------------------------------------------------------
**Title**: mikulski_python_scraper2

**Web address**: https://classic.scraperwiki.com/scrapers/mikulski_python_scraper2_1/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

Do floats break the DataTable?
------------------------------------------------------------------------------------------------
**Title**: Do floats break the DataTable?

**Web address**: https://classic.scraperwiki.com/scrapers/do_floats_break_the_datatable/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

Coal mining glossary
------------------------------------------------------------------------------------------------
**Title**: Coal mining glossary

**Web address**: https://classic.scraperwiki.com/scrapers/coal_mining_glossary/

**Date**: October 2013

**Description**: 
        
            	This is a glossary of mining terms from the Kentucky Coal and Energy Education Project.

	Cool mining terms

	These might make fun names for something.

	
		blasting cap
		carbide bit
		anemometer
		highwall
		mudcap
		torque wrench
		jackrock
	
        
        

Do URLs break the DataTable display?
------------------------------------------------------------------------------------------------
**Title**: Do URLs break the DataTable display?

**Web address**: https://classic.scraperwiki.com/scrapers/do_urls_break_the_datatable_display/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

Wheel Tractor-Scraper Models
------------------------------------------------------------------------------------------------
**Title**: Wheel Tractor-Scraper Models

**Web address**: https://classic.scraperwiki.com/scrapers/wheel_tractor-scrapers/

**Date**: October 2013

**Description**: 
        
            	Caterpillar wheel tractor-scraper models

	Brand inspiration

	People

	
		Mine engineer
		* Operator
		Earthmoving *
		Deisel mechanic
		Heavy Civil Project Estimator
		More
	

	Vehicle

	
		Tournahoppers
		R.G. LeTourneau
		Earthmoving
		Heavy equipment
		Hydrolics
	

	Disciplines/Projects
	
		Cut and fill
		Transportation
		Glossary of mining terms
	
        
        

Can we catch the CPU time exceeded exception?
------------------------------------------------------------------------------------------------
**Title**: Can we catch the CPU time exceeded exception?

**Web address**: https://classic.scraperwiki.com/scrapers/can_we_catch_the_cpu_time_exceeded_exception/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

nyc_lobbyist_directory_status
------------------------------------------------------------------------------------------------
**Title**: nyc_lobbyist_directory_status

**Web address**: https://classic.scraperwiki.com/views/nyc_lobbyist_directory_status/

**Date**: October 2013

**Description**: 
        
            This view has no description. 
                Sign in to add one.
            
        
        

randomsleep
------------------------------------------------------------------------------------------------
**Title**: randomsleep

**Web address**: https://classic.scraperwiki.com/scrapers/randomsleep/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

NYC Lobbyist Search
------------------------------------------------------------------------------------------------
**Title**: NYC Lobbyist Search

**Web address**: https://classic.scraperwiki.com/scrapers/nyc_lobbyist_directory_browser/

**Date**: October 2013

**Description**: 
        
            New York lobbyist search, likely to contain many millions of records within hundreds of thousands of pages.
Most of these might be duplicates, but I don't see any way of finding those except by assuming that identical records are duplicates
It'll take about 17,000 runs to finish. Here is how far along it is.


So I'd better make this faster.
        
        

SQF
------------------------------------------------------------------------------------------------
**Title**: SQF

**Web address**: https://classic.scraperwiki.com/scrapers/sqf/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

NYS Legislator Data
------------------------------------------------------------------------------------------------
**Title**: NYS Legislator Data

**Web address**: https://classic.scraperwiki.com/scrapers/nys_legislator_data/

**Date**: October 2013

**Description**: 
        
            	Run this SQL to find matches:

	select `disclosures_cleaned`.`clean` as "Entity" from `licenses_cleaned` join `disclosures_cleaned` on `disclosures_cleaned`.clean=`licenses_cleaned`.clean
        
        

Suffolk County Restaurant Inspections
------------------------------------------------------------------------------------------------
**Title**: Suffolk County Restaurant Inspections

**Web address**: https://classic.scraperwiki.com/scrapers/suffolk_county_restaurant_inspections/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

census
------------------------------------------------------------------------------------------------
**Title**: census

**Web address**: https://classic.scraperwiki.com/scrapers/census/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

Open Book New York
------------------------------------------------------------------------------------------------
**Title**: Open Book New York

**Web address**: https://classic.scraperwiki.com/scrapers/open_book_new_york/

**Date**: October 2013

**Description**: 
        
            	Try this: select sum(Spending_to_Date) as "spending", Agency from `swdata` group by Vendor order by spending limit 20
        
        

#jdnyc Stream C: Ruby
------------------------------------------------------------------------------------------------
**Title**: #jdnyc Stream C: Ruby

**Web address**: https://classic.scraperwiki.com/scrapers/jdnyc_stream_c_ruby/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

#jdnyc Stream C: Python
------------------------------------------------------------------------------------------------
**Title**: #jdnyc Stream C: Python

**Web address**: https://classic.scraperwiki.com/scrapers/jdnyc_stream_c_python/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

j-Tom
------------------------------------------------------------------------------------------------
**Title**: j-Tom

**Web address**: https://classic.scraperwiki.com/scrapers/j-tom/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

p-Tom
------------------------------------------------------------------------------------------------
**Title**: p-Tom

**Web address**: https://classic.scraperwiki.com/scrapers/p-tom/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

watup
------------------------------------------------------------------------------------------------
**Title**: watup

**Web address**: https://classic.scraperwiki.com/scrapers/watup/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

stream_c_python2
------------------------------------------------------------------------------------------------
**Title**: stream_c_python2

**Web address**: https://classic.scraperwiki.com/scrapers/stream_c_python2/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

stream_c_ruby
------------------------------------------------------------------------------------------------
**Title**: stream_c_ruby

**Web address**: https://classic.scraperwiki.com/scrapers/stream_c_ruby/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

stream_c_python
------------------------------------------------------------------------------------------------
**Title**: stream_c_python

**Web address**: https://classic.scraperwiki.com/scrapers/stream_c_python/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

absa2
------------------------------------------------------------------------------------------------
**Title**: absa2

**Web address**: https://classic.scraperwiki.com/scrapers/absa2/

**Date**: October 2013

**Description**: 
        
            	This takes a few runs to finish; if it gives an error, run it again.
        
        

saccos2
------------------------------------------------------------------------------------------------
**Title**: saccos2

**Web address**: https://classic.scraperwiki.com/scrapers/saccos2/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

south_africa_postbank
------------------------------------------------------------------------------------------------
**Title**: south_africa_postbank

**Web address**: https://classic.scraperwiki.com/scrapers/south_africa_postbank/

**Date**: October 2013

**Description**: 
        
            	Connect the tables like so.select * from `branch_info` join `branch_names` on `branch_info`.`branchId`=`branch_names`.`branchId`;
        
        

ncr2
------------------------------------------------------------------------------------------------
**Title**: ncr2

**Web address**: https://classic.scraperwiki.com/scrapers/ncr2/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

ncr
------------------------------------------------------------------------------------------------
**Title**: ncr

**Web address**: https://classic.scraperwiki.com/scrapers/ncr/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

landbank_branches
------------------------------------------------------------------------------------------------
**Title**: landbank_branches

**Web address**: https://classic.scraperwiki.com/scrapers/landbank_branches/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

standardbank
------------------------------------------------------------------------------------------------
**Title**: standardbank

**Web address**: https://classic.scraperwiki.com/scrapers/standardbank/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

absa
------------------------------------------------------------------------------------------------
**Title**: absa

**Web address**: https://classic.scraperwiki.com/scrapers/absa/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

fnb_south_africa
------------------------------------------------------------------------------------------------
**Title**: fnb_south_africa

**Web address**: https://classic.scraperwiki.com/scrapers/fnb_south_africa/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

capitec_bank
------------------------------------------------------------------------------------------------
**Title**: capitec_bank

**Web address**: https://classic.scraperwiki.com/scrapers/capitec_bank/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

african_bank
------------------------------------------------------------------------------------------------
**Title**: african_bank

**Web address**: https://classic.scraperwiki.com/scrapers/african_bank/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

wacky_table_rb
------------------------------------------------------------------------------------------------
**Title**: wacky_table_rb

**Web address**: https://classic.scraperwiki.com/scrapers/wacky_table_rb/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

wacky_table_py
------------------------------------------------------------------------------------------------
**Title**: wacky_table_py

**Web address**: https://classic.scraperwiki.com/scrapers/wacky_table_py/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

ecobank_branches
------------------------------------------------------------------------------------------------
**Title**: ecobank_branches

**Web address**: https://classic.scraperwiki.com/scrapers/ecobank_branches/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

landbank_satellite_branches
------------------------------------------------------------------------------------------------
**Title**: landbank_satellite_branches

**Web address**: https://classic.scraperwiki.com/scrapers/landbank_satellite_branches/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

sd-state-employee
------------------------------------------------------------------------------------------------
**Title**: sd-state-employee

**Web address**: https://classic.scraperwiki.com/scrapers/sd-state-employee/

**Date**: October 2013

**Description**: 
        
            	"This information would be used for a number of state government reporting projects; the first one would be to analyze this database against a list of reported lobbyists to see the extent of any correlation between the two."

	We're searching using a list of Lobbyist surnames.

	By request.

	It looks like the server limits us to ~200 requests a day... I'm currently manually changing the list of surnames to reflect where it is up to.
        
        

csv2sw
------------------------------------------------------------------------------------------------
**Title**: csv2sw

**Web address**: https://classic.scraperwiki.com/scrapers/csv2sw/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

south_dakota_lobbyists
------------------------------------------------------------------------------------------------
**Title**: south_dakota_lobbyists

**Web address**: https://classic.scraperwiki.com/scrapers/south_dakota_lobbyists/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

saccos
------------------------------------------------------------------------------------------------
**Title**: saccos

**Web address**: https://classic.scraperwiki.com/scrapers/saccos/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

gbsbank
------------------------------------------------------------------------------------------------
**Title**: gbsbank

**Web address**: https://classic.scraperwiki.com/scrapers/gbsbank/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

options
------------------------------------------------------------------------------------------------
**Title**: options

**Web address**: https://classic.scraperwiki.com/scrapers/options/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

nedbank_branches
------------------------------------------------------------------------------------------------
**Title**: nedbank_branches

**Web address**: https://classic.scraperwiki.com/scrapers/nedbank_branches/

**Date**: October 2013

**Description**: 
        
            	Because this website's data span many pages, this scraper needs to be run about nine times to make a full scrape.
        
        

sasfin
------------------------------------------------------------------------------------------------
**Title**: sasfin

**Web address**: https://classic.scraperwiki.com/scrapers/sasfin/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

finabank_cash
------------------------------------------------------------------------------------------------
**Title**: finabank_cash

**Web address**: https://classic.scraperwiki.com/scrapers/finabank_cash/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

mix_scraper_spreadsheets
------------------------------------------------------------------------------------------------
**Title**: mix_scraper_spreadsheets

**Web address**: https://classic.scraperwiki.com/views/mix_scraper_spreadsheets/

**Date**: October 2013

**Description**: 
        
            	The geocoding test is here.
        
        

bidvest_bank
------------------------------------------------------------------------------------------------
**Title**: bidvest_bank

**Web address**: https://classic.scraperwiki.com/scrapers/bidvest_bank/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

vbsmutual
------------------------------------------------------------------------------------------------
**Title**: vbsmutual

**Web address**: https://classic.scraperwiki.com/scrapers/vbsmutual/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

khula
------------------------------------------------------------------------------------------------
**Title**: khula

**Web address**: https://classic.scraperwiki.com/scrapers/khula/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

Production server table engines
------------------------------------------------------------------------------------------------
**Title**: Production server table engines

**Web address**: https://classic.scraperwiki.com/views/production_server_table_engines/

**Date**: October 2013

**Description**: 
        
            This view has no description. 
                Sign in to add one.
            
        
        

Senator District Offices
------------------------------------------------------------------------------------------------
**Title**: Senator District Offices

**Web address**: https://classic.scraperwiki.com/views/senator_district_offices/

**Date**: October 2013

**Description**: 
        
            This view has no description. 
                Sign in to add one.
            
        
        

US Senators backup
------------------------------------------------------------------------------------------------
**Title**: US Senators backup

**Web address**: https://classic.scraperwiki.com/scrapers/us_senators_backup/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

Attendees to the NYTM rally to stop SOPA and PIPA
------------------------------------------------------------------------------------------------
**Title**: Attendees to the NYTM rally to stop SOPA and PIPA

**Web address**: https://classic.scraperwiki.com/scrapers/nytm_sos_attendees/

**Date**: October 2013

**Description**: 
        
            	I scrape attendees to the emergency meetup to stop SOPA and PIPA. There's something wrong with Meetup or the RSVP list parser; I count fewer attendees than the page does.

	Select the current table like so:SELECT * from (SELECT value_blob FROM `swvariables` WHERE name="nytm_sos_current");
        
        

Meetup
------------------------------------------------------------------------------------------------
**Title**: Meetup

**Web address**: https://classic.scraperwiki.com/scrapers/meetup/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

keyify
------------------------------------------------------------------------------------------------
**Title**: keyify

**Web address**: https://classic.scraperwiki.com/scrapers/keyify/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

marang_offices
------------------------------------------------------------------------------------------------
**Title**: marang_offices

**Web address**: https://classic.scraperwiki.com/scrapers/marang_offices/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

tebabank_atm
------------------------------------------------------------------------------------------------
**Title**: tebabank_atm

**Web address**: https://classic.scraperwiki.com/scrapers/tebabank_atm/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

tebabank_branch
------------------------------------------------------------------------------------------------
**Title**: tebabank_branch

**Web address**: https://classic.scraperwiki.com/scrapers/tebabank_branch/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

Download, save raw and parse
------------------------------------------------------------------------------------------------
**Title**: Download, save raw and parse

**Web address**: https://classic.scraperwiki.com/scrapers/dsp/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

Check whether the FTC RN site is up
------------------------------------------------------------------------------------------------
**Title**: Check whether the FTC RN site is up

**Web address**: https://classic.scraperwiki.com/scrapers/check_whether_the_ftc_rn_site_is_up/

**Date**: October 2013

**Description**: 
        
            	This scraper checks whether the FTC RN site is back up. Read more on the ScraperWiki Google Group.
        
        

Fernsehserien Sendetermine
------------------------------------------------------------------------------------------------
**Title**: Fernsehserien Sendetermine

**Web address**: https://classic.scraperwiki.com/scrapers/fernsehserien_sendetermine_1/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

RNA
------------------------------------------------------------------------------------------------
**Title**: RNA

**Web address**: https://classic.scraperwiki.com/views/rna_2/

**Date**: October 2013

**Description**: 
        
            <script src="https://views.scraperwiki.com/run/rna_2/">
        
        

Bell County Court Dockets Example
------------------------------------------------------------------------------------------------
**Title**: Bell County Court Dockets Example

**Web address**: https://classic.scraperwiki.com/scrapers/bell_county_court_dockets_example/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

Ruby meetup lighting talks
------------------------------------------------------------------------------------------------
**Title**: Ruby meetup lighting talks

**Web address**: https://classic.scraperwiki.com/views/ruby_meetup_lighting_talks/

**Date**: October 2013

**Description**: 
        
            This view has no description. 
                Sign in to add one.
            
        
        

swversion
------------------------------------------------------------------------------------------------
**Title**: swversion

**Web address**: https://classic.scraperwiki.com/scrapers/swversion/

**Date**: October 2013

**Description**: 
        
            	Basic versioning for ScraperWiki database tables.

	How to usesave([],{"foo":bar},'baz')swversion("baz")

	This moves baz to baz_[seconds since epoch] and saves baz_[seconds since epoch] as baz_current in swvariables.
        
        

Driver's Seat
------------------------------------------------------------------------------------------------
**Title**: Driver's Seat

**Web address**: https://classic.scraperwiki.com/scrapers/tractorseat/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

AJAX Tractor tests
------------------------------------------------------------------------------------------------
**Title**: AJAX Tractor tests

**Web address**: https://classic.scraperwiki.com/scrapers/ajax_tractor_tests/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

AJAX Tractor
------------------------------------------------------------------------------------------------
**Title**: AJAX Tractor

**Web address**: https://classic.scraperwiki.com/views/ajax_tractor/

**Date**: October 2013

**Description**: 
        
            This view has no description. 
                Sign in to add one.
            
        
        

Brisbane Current Weather
------------------------------------------------------------------------------------------------
**Title**: Brisbane Current Weather

**Web address**: https://classic.scraperwiki.com/scrapers/brisbane_current_weather_1/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

Brisbane Current Weather
------------------------------------------------------------------------------------------------
**Title**: Brisbane Current Weather

**Web address**: https://classic.scraperwiki.com/scrapers/brisbane_current_weather3/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

Brisbane Current Weather
------------------------------------------------------------------------------------------------
**Title**: Brisbane Current Weather

**Web address**: https://classic.scraperwiki.com/scrapers/brisbane_current_weather2/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

Brisbane Current Weather
------------------------------------------------------------------------------------------------
**Title**: Brisbane Current Weather

**Web address**: https://classic.scraperwiki.com/scrapers/brisbane_current_weather/

**Date**: October 2013

**Description**: 
        
            	Brisbane Current Weather working Thanks to Thomas
        
        

National Bank of Rwanda microfinance institutions
------------------------------------------------------------------------------------------------
**Title**: National Bank of Rwanda microfinance institutions

**Web address**: https://classic.scraperwiki.com/scrapers/national_bank_of_rwanda_microfinance_institutions/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

dbget.php
------------------------------------------------------------------------------------------------
**Title**: dbget.php

**Web address**: https://classic.scraperwiki.com/scrapers/dbgetphp/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

dbget.rb
------------------------------------------------------------------------------------------------
**Title**: dbget.rb

**Web address**: https://classic.scraperwiki.com/scrapers/dbgetrb/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

htmltable2matrix.rb
------------------------------------------------------------------------------------------------
**Title**: htmltable2matrix.rb

**Web address**: https://classic.scraperwiki.com/scrapers/htmltable2matrixrb/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

Eventbrite
------------------------------------------------------------------------------------------------
**Title**: Eventbrite

**Web address**: https://classic.scraperwiki.com/scrapers/eventbrite/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

ScraperWiki Events Eventbrite Guestlists
------------------------------------------------------------------------------------------------
**Title**: ScraperWiki Events Eventbrite Guestlists

**Web address**: https://classic.scraperwiki.com/scrapers/scraperwiki_events_eventbrite_guestlists/

**Date**: October 2013

**Description**: 
        
            This table might be helpful.

@select date(`first_scraped`,'unixepoch') as `Date signed up`, `Who am I`,`Twitter Handle` from `ny` ORDER BY `Who am I` DESC,`first_scraped`;@

I think the next step is to "automatically update a Twitter list":https://dev.twitter.com/docs/api/1/post/lists/members/create_all.
        
        

Active ScraperWikians
------------------------------------------------------------------------------------------------
**Title**: Active ScraperWikians

**Web address**: https://classic.scraperwiki.com/views/active_scraperwikians/

**Date**: October 2013

**Description**: 
        
            This view has no description. 
                Sign in to add one.
            
        
        

Running scripts in datastore queries
------------------------------------------------------------------------------------------------
**Title**: Running scripts in datastore queries

**Web address**: https://classic.scraperwiki.com/scrapers/running_scripts_in_datastore_queries/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

ScraperWikian Tweets
------------------------------------------------------------------------------------------------
**Title**: ScraperWikian Tweets

**Web address**: https://classic.scraperwiki.com/views/scraperwikian_tweets/

**Date**: October 2013

**Description**: 
        
            This view has no description. 
                Sign in to add one.
            
        
        

Test sql
------------------------------------------------------------------------------------------------
**Title**: Test sql

**Web address**: https://classic.scraperwiki.com/scrapers/test_sql/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

Ealing Tree Preservation Orders Map
------------------------------------------------------------------------------------------------
**Title**: Ealing Tree Preservation Orders Map

**Web address**: https://classic.scraperwiki.com/views/ealing_tree_preservation_orders_map/

**Date**: October 2013

**Description**: 
        
            This view has no description. 
                Sign in to add one.
            
        
        

ScraperWiki user profiles
------------------------------------------------------------------------------------------------
**Title**: ScraperWiki user profiles

**Web address**: https://classic.scraperwiki.com/scrapers/scraperwiki_user_profiles/

**Date**: October 2013

**Description**: 
        
            	To do
	
		Number of elements in set( all scraper title word tokens ) for each user in order to see how varied their scrapers are
	
        
        

HTML tags available in the scraper description
------------------------------------------------------------------------------------------------
**Title**: HTML tags available in the scraper description

**Web address**: https://classic.scraperwiki.com/scrapers/html_tags_available_in_the_scraper_description/

**Date**: October 2013

**Description**: 
        
            I can put script and iframe tags in this box.
<script>alert('hi');

Script disabled - thanks for the proof of concept Tom. Has been fixed. Z
        
        

htmltable2matrix
------------------------------------------------------------------------------------------------
**Title**: htmltable2matrix

**Web address**: https://classic.scraperwiki.com/scrapers/htmltable2matrix/

**Date**: October 2013

**Description**: 
        
            	Convert an html table (represented lxml tree where the current node is the table tag) into a list of lists, handling colspan attributes.
        
        

BetterCoach: Schedules
------------------------------------------------------------------------------------------------
**Title**: BetterCoach: Schedules

**Web address**: https://classic.scraperwiki.com/scrapers/bettercoach_schedules/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

Chainsaw Example
------------------------------------------------------------------------------------------------
**Title**: Chainsaw Example

**Web address**: https://classic.scraperwiki.com/scrapers/chainsaw_example/

**Date**: October 2013

**Description**: 
        
            	Example of how to use Chainsaw
        
        

ScraperWiki script directory hacks
------------------------------------------------------------------------------------------------
**Title**: ScraperWiki script directory hacks

**Web address**: https://classic.scraperwiki.com/scrapers/scraperwiki_script_directory_hacks/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

YouTube view counts
------------------------------------------------------------------------------------------------
**Title**: YouTube view counts

**Web address**: https://classic.scraperwiki.com/scrapers/youtube_view_counts/

**Date**: October 2013

**Description**: 
        
            	Motivation

	Some viewers of Rap News X expressed concerns that YouTube was not counting views of the video in order to prevent it from going viral.

	Comments

	rylenenger2213 said "The counter has been stuck at 108,403 for multiple days. Someone doesn't want this video to get labelled as 'viral'. Why? See for yourself! SHARE THIS VIDEO. No oppression, no suppression! Like this comment so it stays at the top so other people can be warned about the views being blocked!"

	gnu11111 said "Strange how the view counter seems stuck at 108.403 for two days now. Even I watched the video like 10 times since then."

	How to use

	Edit the VIDEOS dictionary at the top of this scraper, adding an entry for each video you would like scraped. Edit mine rather than just forking it so that we can more easily aggregate data and update the scraper.
        
        

dbget.py example
------------------------------------------------------------------------------------------------
**Title**: dbget.py example

**Web address**: https://classic.scraperwiki.com/scrapers/dbgetpy_example/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

dbget.py
------------------------------------------------------------------------------------------------
**Title**: dbget.py

**Web address**: https://classic.scraperwiki.com/scrapers/dbgetpy/

**Date**: October 2013

**Description**: 
        
            	Save URLs directly to the datastore. You can use this in your own scrapers!

	from scraperwiki.utils import swimport

	dbget=swimport('dbgetpy')
        
        

Coach USA Routes
------------------------------------------------------------------------------------------------
**Title**: Coach USA Routes

**Web address**: https://classic.scraperwiki.com/scrapers/coach_usa_routes/

**Date**: October 2013

**Description**: 
        
            	The canonical source is heregit://gitorious.org/tlevine/better_coach.git
        
        

How are scrapers cached?
------------------------------------------------------------------------------------------------
**Title**: How are scrapers cached?

**Web address**: https://classic.scraperwiki.com/scrapers/how_are_scrapers_cached/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

Broadband Coverage Birmingham
------------------------------------------------------------------------------------------------
**Title**: Broadband Coverage Birmingham

**Web address**: https://classic.scraperwiki.com/scrapers/broadband_coverage_birmingham/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

Mashed potatoes
------------------------------------------------------------------------------------------------
**Title**: Mashed potatoes

**Web address**: https://classic.scraperwiki.com/scrapers/mashed_potatoes/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

Chainsaw
------------------------------------------------------------------------------------------------
**Title**: Chainsaw

**Web address**: https://classic.scraperwiki.com/scrapers/chainsaw/

**Date**: October 2013

**Description**: 
        
            	Shortcuts for scrapers. The code is also on GitHub.
        
        

BIS school absenses
------------------------------------------------------------------------------------------------
**Title**: BIS school absenses

**Web address**: https://classic.scraperwiki.com/scrapers/bis_school_absenses/

**Date**: October 2013

**Description**: 
        
            	
		The Excel file
		Elana's question
	
        
        

@TahrirSupplies Map
------------------------------------------------------------------------------------------------
**Title**: @TahrirSupplies Map

**Web address**: https://classic.scraperwiki.com/views/tahrirsupplies_map_2/

**Date**: October 2013

**Description**: 
        
            This view has no description. 
                Sign in to add one.
            
        
        

Coach USA Schedules
------------------------------------------------------------------------------------------------
**Title**: Coach USA Schedules

**Web address**: https://classic.scraperwiki.com/scrapers/better_shortline_bus/

**Date**: October 2013

**Description**: 
        
            	Issues

	
		From-stops are often marked as to-stops.
		Routes with two tables parse wrongly.
	
        
        

Cell values of zero
------------------------------------------------------------------------------------------------
**Title**: Cell values of zero

**Web address**: https://classic.scraperwiki.com/scrapers/cell_values_of_zero/

**Date**: October 2013

**Description**: 
        
            	The scraper demonstrates that cell values of zero in the table preview were not shown. The issue has been fixed.
        
        

Movie theatres v2
------------------------------------------------------------------------------------------------
**Title**: Movie theatres v2

**Web address**: https://classic.scraperwiki.com/scrapers/movie_theatres2/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

faa-tailnums
------------------------------------------------------------------------------------------------
**Title**: faa-tailnums

**Web address**: https://classic.scraperwiki.com/scrapers/faa-tailnums/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

United States Zip Codes
------------------------------------------------------------------------------------------------
**Title**: United States Zip Codes

**Web address**: https://classic.scraperwiki.com/scrapers/us_zip_codes/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

New York law scraper status
------------------------------------------------------------------------------------------------
**Title**: New York law scraper status

**Web address**: https://classic.scraperwiki.com/views/new_york_law_scraper_status/

**Date**: October 2013

**Description**: 
        
            This view has no description. 
                Sign in to add one.
            
        
        

New York Law
------------------------------------------------------------------------------------------------
**Title**: New York Law

**Web address**: https://classic.scraperwiki.com/scrapers/new_york_law/

**Date**: October 2013

**Description**: 
        
            	Here is New York State law. A complete scrape has about 50,000 records and takes about 80 scraper runs.

	Speed

	The website goes down startlingly frequently. Since the scraper had been set to sleep for a couple seconds between each page load, I'm starting to think that something other than the scraper is bringing the site down. The server runs zVWS.

	Schema

	Tables `dir_0`, `dir_1`, `dir_2`, `dir_3` and `dir_4` correspond to links between directory levels on the website, where `dir_0` the link to the Laws Menu and `dir_3` is links to article sections.

	You can join one table row with a table row from the parent directory table on the js and parentjs columns, respectively. js happens to be the Javascript code to call a particular page.

	date_scraped is the date at which that row was scraped, in POSIX time. Use this to see how the law changes.

	Useful queries

	The span of time over which a particular complete scrape occurred.select date_scraped from dir_0 order by date_scraped desc limit 2;
        
        

Senegal financial agreements
------------------------------------------------------------------------------------------------
**Title**: Senegal financial agreements

**Web address**: https://classic.scraperwiki.com/scrapers/senegal_financial_agreements/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

Benin Financial information
------------------------------------------------------------------------------------------------
**Title**: Benin Financial information

**Web address**: https://classic.scraperwiki.com/scrapers/benin_financial_information/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

Occupy Tweets Network Graph
------------------------------------------------------------------------------------------------
**Title**: Occupy Tweets Network Graph

**Web address**: https://classic.scraperwiki.com/views/occupy_tweets_network_graph/

**Date**: October 2013

**Description**: 
        
            This view has no description. 
                Sign in to add one.
            
        
        

USDA NOP Certified Operators
------------------------------------------------------------------------------------------------
**Title**: USDA NOP Certified Operators

**Web address**: https://classic.scraperwiki.com/scrapers/usda_nop_certified_operators/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

CCOF Directory
------------------------------------------------------------------------------------------------
**Title**: CCOF Directory

**Web address**: https://classic.scraperwiki.com/scrapers/ccof_directory/

**Date**: October 2013

**Description**: 
        
            	What I scrapeI scrape "all of CCOF's organic farmers, processors, handlers, packers, and retailers, as well as CCOF certified organic crops, livestock, processed products, and organic business services" from the CCOF's directory.

	SchemaThe table of main interest is the `directory`, which shows the scraped data.

	`scrape_times` stores each of the times at which the scraper was run and can be joined on directory by scrape_id. The scraper uses this table to resume incomplete scrapings, mainly in case of errors.

	`completions` links the ids that correspond to a complete scrape of the directory; each row in this table lists a full directory scrape and lists the corresponding scrape_ids.

	`nonstandard_pairs` lists key-value "pairs" on the web pages that were not exactly two items long. "pair" elements are delimited by pipes. This might help with debugging.

	To doIn order to model the directory over time more intuitively and efficiently, it would be nice to store the completion_id in the directory table and do away with the scrape_completions table. To do this, we store completion_id while scraping and increment completion_id upon completion. We can store the current completion_id with save_var.
        
        

ScraperWiki Metadata Versioning
------------------------------------------------------------------------------------------------
**Title**: ScraperWiki Metadata Versioning

**Web address**: https://classic.scraperwiki.com/scrapers/scraperwiki_metadata_versioning/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

ScraperWiki Scraper URLs
------------------------------------------------------------------------------------------------
**Title**: ScraperWiki Scraper URLs

**Web address**: https://classic.scraperwiki.com/scrapers/scraperwiki_scraper_urls/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

ScraperWiki featured scrapers
------------------------------------------------------------------------------------------------
**Title**: ScraperWiki featured scrapers

**Web address**: https://classic.scraperwiki.com/scrapers/scraperwiki_featured_scrapers/

**Date**: October 2013

**Description**: 
        
            	It saves featured scrapers, the time at which they are featured and the current metadata at the time of their feature. It saves the URLs of the scrapers but parses only the home page of ScraperWiki to gather metadata.
        
        

Movie theatres
------------------------------------------------------------------------------------------------
**Title**: Movie theatres

**Web address**: https://classic.scraperwiki.com/scrapers/movie_theatres/

**Date**: October 2013

**Description**: 
        
            	Use this one instead.

	I scrape theatre information from Cinema Treasures. I conduct a search by zip code and extract the information presented in the search. The `zipcode` column is the zip code searched rather than the zipcode in which the theater is situated.

	Among the information is the url of the theatre's main information page. The url is saved, but the corresponding page is not scraped.
        
        

french_companies_informations
------------------------------------------------------------------------------------------------
**Title**: french_companies_informations

**Web address**: https://classic.scraperwiki.com/scrapers/french_companies_informations/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

SPINE LCI DATA
------------------------------------------------------------------------------------------------
**Title**: SPINE LCI DATA

**Web address**: https://classic.scraperwiki.com/scrapers/spine_lci_data/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

New Mexico State Audits: Opinion changes
------------------------------------------------------------------------------------------------
**Title**: New Mexico State Audits: Opinion changes

**Web address**: https://classic.scraperwiki.com/views/new_mexico_state_audits_opinion_changes/

**Date**: October 2013

**Description**: 
        
            	Identify changes in audit opinion status.
        
        

New Mexico State Audits: Parse
------------------------------------------------------------------------------------------------
**Title**: New Mexico State Audits: Parse

**Web address**: https://classic.scraperwiki.com/scrapers/new_mexico_state_audits_parse/

**Date**: October 2013

**Description**: 
        
            	Download audit data from pages from the New Mexico Office of the State Auditor's website.
        
        

New Mexico State Audits: Search
------------------------------------------------------------------------------------------------
**Title**: New Mexico State Audits: Search

**Web address**: https://classic.scraperwiki.com/scrapers/new_mexico_state_audits/

**Date**: October 2013

**Description**: 
        
            	Get URLs for audit data pages from the New Mexico Office of the State Auditor's website
        
        

Alafia Network: Parse
------------------------------------------------------------------------------------------------
**Title**: Alafia Network: Parse

**Web address**: https://classic.scraperwiki.com/scrapers/alafia_network_parse/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

Alafia Network
------------------------------------------------------------------------------------------------
**Title**: Alafia Network

**Web address**: https://classic.scraperwiki.com/scrapers/alafia_network/

**Date**: October 2013

**Description**: 
        
            	Association Professionnelle des Systemes Financiers Decentralises du Benin
        
        

Uruguay
------------------------------------------------------------------------------------------------
**Title**: Uruguay

**Web address**: https://classic.scraperwiki.com/scrapers/uruguay/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

country-code
------------------------------------------------------------------------------------------------
**Title**: country-code

**Web address**: https://classic.scraperwiki.com/scrapers/country-code/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

PostSecret
------------------------------------------------------------------------------------------------
**Title**: PostSecret

**Web address**: https://classic.scraperwiki.com/views/postsecret_1/

**Date**: October 2013

**Description**: 
        
            	Display a random PostSecret postcard
        
        

Postsecret
------------------------------------------------------------------------------------------------
**Title**: Postsecret

**Web address**: https://classic.scraperwiki.com/scrapers/postsecret/

**Date**: October 2013

**Description**: 
        
            	Scrape the PostSecret blog, saving postcard images, urls and date of publication.
        
        

Canada Elections: Contributer table
------------------------------------------------------------------------------------------------
**Title**: Canada Elections: Contributer table

**Web address**: https://classic.scraperwiki.com/scrapers/canada_elections_contributer_table/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

Canada Elections: Contributer popup
------------------------------------------------------------------------------------------------
**Title**: Canada Elections: Contributer popup

**Web address**: https://classic.scraperwiki.com/scrapers/canada_elections_contributer_popup/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

Canada Elections: Contributers (mechanize)
------------------------------------------------------------------------------------------------
**Title**: Canada Elections: Contributers (mechanize)

**Web address**: https://classic.scraperwiki.com/scrapers/canada_elections_contributers_mechanize/

**Date**: October 2013

**Description**: 
        
            	Don't use this; it doesn't work, and this one does.
        
        

Mozambique Financial Institutions
------------------------------------------------------------------------------------------------
**Title**: Mozambique Financial Institutions

**Web address**: https://classic.scraperwiki.com/scrapers/bancomoc/

**Date**: October 2013

**Description**: 
        
            	Mozambique's official database of financial institutions
        
        

RIFIDEC
------------------------------------------------------------------------------------------------
**Title**: RIFIDEC

**Web address**: https://classic.scraperwiki.com/scrapers/rifidec/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

BurundiMicrofinanceInstitutes
------------------------------------------------------------------------------------------------
**Title**: BurundiMicrofinanceInstitutes

**Web address**: https://classic.scraperwiki.com/scrapers/burundimicrofinanceinstitutes/

**Date**: October 2013

**Description**: 
        
            	Scraping the list of microfinance institutes in Burundi, as part of the Mix Market project in the Data Without Borders October 2011 New York Data Dive. See http://wiki.datawithoutborders.cc/index.php?title=Project:Current_events:NYC_DD:MIX for more details.
        
        

THacker - Emendas Parlamentares
------------------------------------------------------------------------------------------------
**Title**: THacker - Emendas Parlamentares

**Web address**: https://classic.scraperwiki.com/scrapers/thacker_-_emendas_parlamentares/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

Annual trend in gender ratio of pilgrams to Santiago
------------------------------------------------------------------------------------------------
**Title**: Annual trend in gender ratio of pilgrams to Santiago

**Web address**: https://classic.scraperwiki.com/views/annual_trend_in_gender_ratio_of_pilgrams_to_sant_1/

**Date**: October 2013

**Description**: 
        
            	Proportionately fewer women make the pilgrimage in January.

	Code
        
        

italy_air_pollution
------------------------------------------------------------------------------------------------
**Title**: italy_air_pollution

**Web address**: https://classic.scraperwiki.com/scrapers/italy_air_pollution/

**Date**: October 2013

**Description**: 
        
            	This code created this csv (1.4 gb uncompressed) based on the information at BRACE.

	BRACE shows 36 air pollutants' concentrations at the city level in Italy from 2002 to 09. The scraped dataset includes only the 15 pollutants measured in cities in Lombardy in those years. Not all pollutants were measured each year, so this dataset only includes 87 pollutant-years of measurements.

	In order to get it into a ScraperWiki SQLite3 database, I could import the enormous csv or rewrite the whole thing in Python, Ruby or PHP.

	I'll format the data more conveniently (within ScraperWiki or not) if you tell me how you want it.
        
        

klimaschutzschulenatlas
------------------------------------------------------------------------------------------------
**Title**: klimaschutzschulenatlas

**Web address**: https://classic.scraperwiki.com/scrapers/klimaschutzschulenatlas/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

Thai stock exchange news feed
------------------------------------------------------------------------------------------------
**Title**: Thai stock exchange news feed

**Web address**: https://classic.scraperwiki.com/views/thai_stock_exchange_news_feed/

**Date**: October 2013

**Description**: 
        
            This view has no description. 
                Sign in to add one.
            
        
        

Drupal project usage
------------------------------------------------------------------------------------------------
**Title**: Drupal project usage

**Web address**: https://classic.scraperwiki.com/scrapers/drupal_project_usage/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

Pilgrims
------------------------------------------------------------------------------------------------
**Title**: Pilgrims

**Web address**: https://classic.scraperwiki.com/scrapers/pilgrams/

**Date**: October 2013

**Description**: 
        
            	Information on pilgrims to Santiago is scraped from the charts on the Peregrination to Santiago website.

	Data are stored in separate tables corresponding to each chart (sex, medium, age, motivation, origin within Spain and origin outside of Spain). The month information is stored in a separate table (months) and can be joined with the others on id.

	The data for the current month are updated more frequently than monthly, so this could be updated to scrape that and thus de-aggregate the data.
        
        

uk lottery details
------------------------------------------------------------------------------------------------
**Title**: uk lottery details

**Web address**: https://classic.scraperwiki.com/scrapers/uk_lottery_details/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

Al Jazeera Headlines
------------------------------------------------------------------------------------------------
**Title**: Al Jazeera Headlines

**Web address**: https://classic.scraperwiki.com/scrapers/al_jazeera_headlines/

**Date**: October 2013

**Description**: 
        
            	Scrapes the headlines from the AL Jazeera English homepage
        
        

FT Twitter Followers
------------------------------------------------------------------------------------------------
**Title**: FT Twitter Followers

**Web address**: https://classic.scraperwiki.com/scrapers/ft_twitter_followers/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

Global Ships DB
------------------------------------------------------------------------------------------------
**Title**: Global Ships DB

**Web address**: https://classic.scraperwiki.com/scrapers/global_ships_db/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

Liverpool Council spending 500
------------------------------------------------------------------------------------------------
**Title**: Liverpool Council spending 500

**Web address**: https://classic.scraperwiki.com/scrapers/liverpool_council_spending_500/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

CP Prima news
------------------------------------------------------------------------------------------------
**Title**: CP Prima news

**Web address**: https://classic.scraperwiki.com/scrapers/cp_prima_news/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

Doctors in Ireland
------------------------------------------------------------------------------------------------
**Title**: Doctors in Ireland

**Web address**: https://classic.scraperwiki.com/scrapers/doctors_in_ireland_1/

**Date**: October 2013

**Description**: 
        
            	This scraper scrapes The Irish Medical Council's Find A Doctor WebApp to get the details about medical practitioners in Ireland

	I'm a novice programmer and it's my first attempt at a scraper - feel free to chip in. I'm planning on adding data from hospitals and universities in Ireland too, so the place of work and universities can be added. 

	The main goal is an RDF datasource for healthcare in Ireland
        
        

Commercial property in Edinburgh
------------------------------------------------------------------------------------------------
**Title**: Commercial property in Edinburgh

**Web address**: https://classic.scraperwiki.com/scrapers/commercial_property_in_edinburgh/

**Date**: October 2013

**Description**: 
        
            	README

	This was originally crawling proprietor data as well, i commented that out because at one request per record, that was causing a lot of CPU load on ScraperWiki 

	There is also a daily rate limit at SAA. Not apparent what it is, must be pretty high considering. 

	In the end i went through postcode outcodes, adding the first number of the incode for the really big ones. I'll look at the property data first and then more selectively scrape the ownership info. 

	I meant to add this fascinating link describing outcode areas: http://en.wikipedia.org/wiki/EH_postcode_area
        
        

All Party Groups
------------------------------------------------------------------------------------------------
**Title**: All Party Groups

**Web address**: https://classic.scraperwiki.com/scrapers/all_party_groups_1/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

PHP jumps when pasting bug
------------------------------------------------------------------------------------------------
**Title**: PHP jumps when pasting bug

**Web address**: https://classic.scraperwiki.com/scrapers/php-jumps-when-pasting-bug/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

Kforge cave data
------------------------------------------------------------------------------------------------
**Title**: Kforge cave data

**Web address**: https://classic.scraperwiki.com/scrapers/kforge-cave-data/

**Date**: October 2013

**Description**: 
        
            This scraper has no description. 
                Sign in to add one.
            
        
        

cs4740_3
------------------------------------------------------------------------------------------------
**Title**: cs4740_3

**Web address**: https://gitorious.org/tlevine/cs4740_3

**Date**: December 2012

**Description**: 

install
------------------------------------------------------------------------------------------------
**Title**: install

**Web address**: https://gitorious.org/tlevine/install

**Date**: July 2012

**Description**: Things I do when I install Ubuntu

hardware_steno
------------------------------------------------------------------------------------------------
**Title**: hardware_steno

**Web address**: https://gitorious.org/tlevine/hardware_steno

**Date**: May 2012

**Description**: 

lorena
------------------------------------------------------------------------------------------------
**Title**: lorena

**Web address**: https://gitorious.org/tlevine/lorena

**Date**: April 2012

**Description**: 

iposition
------------------------------------------------------------------------------------------------
**Title**: iposition

**Web address**: https://gitorious.org/tlevine/iposition

**Date**: February 2012

**Description**: 

nyc-bills-cleaning
------------------------------------------------------------------------------------------------
**Title**: nyc-bills-cleaning

**Web address**: https://gitorious.org/tlevine/nyc-bills-cleaning

**Date**: February 2012

**Description**: #jdnyc

better_coach
------------------------------------------------------------------------------------------------
**Title**: better_coach

**Web address**: https://gitorious.org/tlevine/better_coach

**Date**: January 2012

**Description**: Better coach bus schedules

hardware_steno-dictionaries
------------------------------------------------------------------------------------------------
**Title**: hardware_steno-dictionaries

**Web address**: https://gitorious.org/tlevine/hardware_steno-dictionaries

**Date**: December 2011

**Description**: My steno dictionaries

hardware_steno-lib
------------------------------------------------------------------------------------------------
**Title**: hardware_steno-lib

**Web address**: https://gitorious.org/tlevine/hardware_steno-lib

**Date**: December 2011

**Description**: Libraries for the hardware steno project

alongatag
------------------------------------------------------------------------------------------------
**Title**: alongatag

**Web address**: https://gitorious.org/tlevine/alongatag

**Date**: December 2011

**Description**: 

fc_newchapters
------------------------------------------------------------------------------------------------
**Title**: fc_newchapters

**Web address**: https://gitorious.org/tlevine/fc_newchapters

**Date**: November 2011

**Description**: Create new chapters

cross_the_cliff
------------------------------------------------------------------------------------------------
**Title**: cross_the_cliff

**Web address**: https://gitorious.org/tlevine/cross_the_cliff

**Date**: November 2011

**Description**: 

nyclu
------------------------------------------------------------------------------------------------
**Title**: nyclu

**Web address**: https://gitorious.org/tlevine/nyclu

**Date**: November 2011

**Description**: 

value_exchange
------------------------------------------------------------------------------------------------
**Title**: value_exchange

**Web address**: https://gitorious.org/tlevine/value_exchange

**Date**: November 2011

**Description**: 

historical_weather_download
------------------------------------------------------------------------------------------------
**Title**: historical_weather_download

**Web address**: https://gitorious.org/tlevine/historical_weather_download

**Date**: November 2011

**Description**: Weather Underground seems to be the only place to get historical weather data easily.

This script downloads historical weather data from Weather Underground. Most of the code is from [http://casoilresource.lawr.ucdavis.edu/drupal/node/991](Dylan). 

It provides a scrape function that takes the station id and, optionally, a date or date range. range.

bibliography
------------------------------------------------------------------------------------------------
**Title**: bibliography

**Web address**: https://gitorious.org/tlevine/bibliography

**Date**: November 2011

**Description**: Bibliographies for everything I do

casual_computing
------------------------------------------------------------------------------------------------
**Title**: casual_computing

**Web address**: https://gitorious.org/tlevine/casual_computing

**Date**: November 2011

**Description**: 

sw_boomarklet
------------------------------------------------------------------------------------------------
**Title**: sw_boomarklet

**Web address**: https://gitorious.org/tlevine/sw_boomarklet

**Date**: November 2011

**Description**: 

scraperwikify
------------------------------------------------------------------------------------------------
**Title**: scraperwikify

**Web address**: https://gitorious.org/tlevine/scraperwikify

**Date**: November 2011

**Description**: Drop-in replacements and wrappers some scraperwiki functions

fb_friends_online
------------------------------------------------------------------------------------------------
**Title**: fb_friends_online

**Web address**: https://gitorious.org/tlevine/fb_friends_online

**Date**: November 2011

**Description**: 

toilet_consultants_website
------------------------------------------------------------------------------------------------
**Title**: toilet_consultants_website

**Web address**: https://gitorious.org/tlevine/toilet_consultants_website

**Date**: October 2011

**Description**: 

keyboard_orientation
------------------------------------------------------------------------------------------------
**Title**: keyboard_orientation

**Web address**: https://gitorious.org/tlevine/keyboard_orientation

**Date**: October 2011

**Description**: 

new_mexico_audits
------------------------------------------------------------------------------------------------
**Title**: new_mexico_audits

**Web address**: https://gitorious.org/tlevine/new_mexico_audits

**Date**: October 2011

**Description**: 

postsecret_scraper
------------------------------------------------------------------------------------------------
**Title**: postsecret_scraper

**Web address**: https://gitorious.org/tlevine/postsecret_scraper

**Date**: October 2011

**Description**: 

noaa_post_project
------------------------------------------------------------------------------------------------
**Title**: noaa_post_project

**Web address**: https://gitorious.org/tlevine/noaa_post_project

**Date**: October 2011

**Description**: 

pilgrams_scraper
------------------------------------------------------------------------------------------------
**Title**: pilgrams_scraper

**Web address**: https://gitorious.org/tlevine/pilgrams_scraper

**Date**: October 2011

**Description**: 

air_pollutants
------------------------------------------------------------------------------------------------
**Title**: air_pollutants

**Web address**: https://gitorious.org/tlevine/air_pollutants

**Date**: October 2011

**Description**: 

theatres
------------------------------------------------------------------------------------------------
**Title**: theatres

**Web address**: https://gitorious.org/tlevine/theatres

**Date**: October 2011

**Description**: scrape theatre contact information

ose_microfunding
------------------------------------------------------------------------------------------------
**Title**: ose_microfunding

**Web address**: https://gitorious.org/tlevine/ose_microfunding

**Date**: September 2011

**Description**: 

status_jukebox
------------------------------------------------------------------------------------------------
**Title**: status_jukebox

**Web address**: https://gitorious.org/tlevine/status_jukebox

**Date**: September 2011

**Description**: Donate your twitter status to charity.

ptv2fcp
------------------------------------------------------------------------------------------------
**Title**: ptv2fcp

**Web address**: https://gitorious.org/tlevine/ptv2fcp

**Date**: September 2011

**Description**: Convert Pitivi save files to Final Cut XML.

my_blog
------------------------------------------------------------------------------------------------
**Title**: my_blog

**Web address**: https://gitorious.org/tlevine/my_blog

**Date**: September 2011

**Description**: My blog, written in Hyde

r-help
------------------------------------------------------------------------------------------------
**Title**: r-help

**Web address**: https://gitorious.org/tlevine/r-help

**Date**: September 2011

**Description**: Snippits in response to r-help posts

jsntp
------------------------------------------------------------------------------------------------
**Title**: jsntp

**Web address**: https://gitorious.org/tlevine/jsntp

**Date**: September 2011

**Description**: In-browser time synchronization similar to the Network Time Protocol

popcorn-qualitative
------------------------------------------------------------------------------------------------
**Title**: popcorn-qualitative

**Web address**: https://gitorious.org/tlevine/popcorn-qualitative

**Date**: September 2011

**Description**: 

statistican
------------------------------------------------------------------------------------------------
**Title**: statistican

**Web address**: https://gitorious.org/tlevine/statistican

**Date**: September 2011

**Description**: Quantitative high-class crime game

risley_purity_test
------------------------------------------------------------------------------------------------
**Title**: risley_purity_test

**Web address**: https://gitorious.org/tlevine/risley_purity_test

**Date**: September 2011

**Description**: 

jsntp-website
------------------------------------------------------------------------------------------------
**Title**: jsntp-website

**Web address**: https://gitorious.org/tlevine/jsntp-website

**Date**: September 2011

**Description**: Website for [jsntp](https://gitorious.org/tlevine/jsntp)

jsat
------------------------------------------------------------------------------------------------
**Title**: jsat

**Web address**: https://gitorious.org/tlevine/jsat

**Date**: September 2011

**Description**: Schedule things in Javascript

unkommittee
------------------------------------------------------------------------------------------------
**Title**: unkommittee

**Web address**: https://gitorious.org/tlevine/unkommittee

**Date**: September 2011

**Description**: Nonlinear discussion cotemporal Kommittee

dea4700
------------------------------------------------------------------------------------------------
**Title**: dea4700

**Web address**: https://gitorious.org/tlevine/dea4700

**Date**: August 2011

**Description**: Applied Ergonomic Methods

blog_thomaslevine
------------------------------------------------------------------------------------------------
**Title**: blog_thomaslevine

**Web address**: https://gitorious.org/tlevine/blog_thomaslevine

**Date**: August 2011

**Description**: 

ose_team_culturing
------------------------------------------------------------------------------------------------
**Title**: ose_team_culturing

**Web address**: https://gitorious.org/tlevine/ose_team_culturing

**Date**: August 2011

**Description**: 

tagr
------------------------------------------------------------------------------------------------
**Title**: tagr

**Web address**: https://gitorious.org/tlevine/tagr

**Date**: June 2011

**Description**: Find microblog posts related to R but not tagged. Then reply and tag them.

wc_interface
------------------------------------------------------------------------------------------------
**Title**: wc_interface

**Web address**: https://gitorious.org/tlevine/wc_interface

**Date**: June 2011

**Description**: 

burst_and_rename
------------------------------------------------------------------------------------------------
**Title**: burst_and_rename

**Web address**: https://gitorious.org/tlevine/burst_and_rename

**Date**: June 2011

**Description**: 

bias
------------------------------------------------------------------------------------------------
**Title**: bias

**Web address**: https://gitorious.org/tlevine/bias

**Date**: May 2011

**Description**: 

btry6030
------------------------------------------------------------------------------------------------
**Title**: btry6030

**Web address**: https://gitorious.org/tlevine/btry6030

**Date**: May 2011

**Description**: 

cs4740_4
------------------------------------------------------------------------------------------------
**Title**: cs4740_4

**Web address**: https://gitorious.org/tlevine/cs4740_4

**Date**: May 2011

**Description**: 

dea4100
------------------------------------------------------------------------------------------------
**Title**: dea4100

**Web address**: https://gitorious.org/tlevine/dea4100

**Date**: May 2011

**Description**: 

rcprs_senior_expo
------------------------------------------------------------------------------------------------
**Title**: rcprs_senior_expo

**Web address**: https://gitorious.org/tlevine/rcprs_senior_expo

**Date**: May 2011

**Description**: 

middlenames
------------------------------------------------------------------------------------------------
**Title**: middlenames

**Web address**: https://gitorious.org/tlevine/middlenames

**Date**: April 2011

**Description**: What proportion of people has middle names?

cs4740_critiques
------------------------------------------------------------------------------------------------
**Title**: cs4740_critiques

**Web address**: https://gitorious.org/tlevine/cs4740_critiques

**Date**: April 2011

**Description**: 

mannlib_exhibit
------------------------------------------------------------------------------------------------
**Title**: mannlib_exhibit

**Web address**: https://gitorious.org/tlevine/mannlib_exhibit

**Date**: April 2011

**Description**: 

funscripts
------------------------------------------------------------------------------------------------
**Title**: funscripts

**Web address**: https://gitorious.org/tlevine/funscripts

**Date**: April 2011

**Description**: 

cs4740_2
------------------------------------------------------------------------------------------------
**Title**: cs4740_2

**Web address**: https://gitorious.org/tlevine/cs4740_2

**Date**: April 2011

**Description**: 

arkeo2201_test2
------------------------------------------------------------------------------------------------
**Title**: arkeo2201_test2

**Web address**: https://gitorious.org/tlevine/arkeo2201_test2

**Date**: April 2011

**Description**: My study materials

bee3299
------------------------------------------------------------------------------------------------
**Title**: bee3299

**Web address**: https://gitorious.org/tlevine/bee3299

**Date**: March 2011

**Description**: 

cs4740_1
------------------------------------------------------------------------------------------------
**Title**: cs4740_1

**Web address**: https://gitorious.org/tlevine/cs4740_1

**Date**: March 2011

**Description**: 

dea2500
------------------------------------------------------------------------------------------------
**Title**: dea2500

**Web address**: https://gitorious.org/tlevine/dea2500

**Date**: February 2011

**Description**: Environment and social behavior

ticketchecker
------------------------------------------------------------------------------------------------
**Title**: ticketchecker

**Web address**: https://gitorious.org/tlevine/ticketchecker

**Date**: February 2011

**Description**: 

cornell_store_booklist_scraper
------------------------------------------------------------------------------------------------
**Title**: cornell_store_booklist_scraper

**Web address**: https://gitorious.org/tlevine/cornell_store_booklist_scraper

**Date**: January 2011

**Description**: 

cornell_library_checkout
------------------------------------------------------------------------------------------------
**Title**: cornell_library_checkout

**Web address**: https://gitorious.org/tlevine/cornell_library_checkout

**Date**: January 2011

**Description**: Scripts to scrape books from the Cornell Library website and automatically send requests.

unishare
------------------------------------------------------------------------------------------------
**Title**: unishare

**Web address**: https://gitorious.org/tlevine/unishare

**Date**: January 2011

**Description**: 

monkeys
------------------------------------------------------------------------------------------------
**Title**: monkeys

**Web address**: https://gitorious.org/tlevine/monkeys

**Date**: January 2011

**Description**: 

12daysplot
------------------------------------------------------------------------------------------------
**Title**: 12daysplot

**Web address**: https://gitorious.org/tlevine/12daysplot

**Date**: January 2011

**Description**: Plots of the gifts my true love gave to me on the twelve days of Christmas

hip_symmetry
------------------------------------------------------------------------------------------------
**Title**: hip_symmetry

**Web address**: https://gitorious.org/tlevine/hip_symmetry

**Date**: January 2011

**Description**: Are hips symmetrically placed on wheeled mobility devices, and what factors influence that symmetry?

Final project for BTRY 4100

noisedose
------------------------------------------------------------------------------------------------
**Title**: noisedose

**Web address**: https://gitorious.org/tlevine/noisedose

**Date**: January 2011

**Description**: 

dump
------------------------------------------------------------------------------------------------
**Title**: dump

**Web address**: https://gitorious.org/tlevine/dump

**Date**: December 2010

**Description**: 

dea4550
------------------------------------------------------------------------------------------------
**Title**: dea4550

**Web address**: https://gitorious.org/tlevine/dea4550

**Date**: December 2010

**Description**: Research methods

mrave2010
------------------------------------------------------------------------------------------------
**Title**: mrave2010

**Web address**: https://gitorious.org/tlevine/mrave2010

**Date**: November 2010

**Description**: Masquerave 2010

cmcm2010
------------------------------------------------------------------------------------------------
**Title**: cmcm2010

**Web address**: https://gitorious.org/tlevine/cmcm2010

**Date**: November 2010

**Description**: Paper for the Cornell Mathematical Contest in Modelling, November 12-16, 2010

dea6760
------------------------------------------------------------------------------------------------
**Title**: dea6760

**Web address**: https://gitorious.org/tlevine/dea6760

**Date**: May 2010

**Description**: 

maze-robot
------------------------------------------------------------------------------------------------
**Title**: maze-robot

**Web address**: https://gitorious.org/tlevine/maze-robot

**Date**: April 2010

**Description**: Robot that teaches people how to program

The platform is currently a maze

website
------------------------------------------------------------------------------------------------
**Title**: website

**Web address**: https://gitorious.org/tlevine/website

**Date**: April 2010

**Description**: thomaslevine.com

dea2030
------------------------------------------------------------------------------------------------
**Title**: dea2030

**Web address**: https://gitorious.org/tlevine/dea2030

**Date**: April 2010

**Description**: DEA 2030 Digital Communications. I didn't take it, but I did some assignments.

The spiral timeline inspired by a DEA 2030 assignment and bar and pie charts

print2web
------------------------------------------------------------------------------------------------
**Title**: print2web

**Web address**: https://gitorious.org/tlevine/print2web

**Date**: April 2010

**Description**: I don't have a printer. This system uses a cgi script and pdftk to compile into one pdf things that I've printed to pdf so that I can print them all at once in computer labs.

comm2010
------------------------------------------------------------------------------------------------
**Title**: comm2010

**Web address**: https://gitorious.org/tlevine/comm2010

**Date**: April 2010

**Description**: 

courtyard-rave
------------------------------------------------------------------------------------------------
**Title**: courtyard-rave

**Web address**: https://gitorious.org/tlevine/courtyard-rave

**Date**: April 2010

**Description**: 

btry4100
------------------------------------------------------------------------------------------------
**Title**: btry4100

**Web address**: https://gitorious.org/tlevine/btry4100

**Date**: March 2010

**Description**: Techniques of Multivariate Analysis

predictgrades
------------------------------------------------------------------------------------------------
**Title**: predictgrades

**Web address**: https://gitorious.org/tlevine/predictgrades

**Date**: March 2010

**Description**: 

slopebench
------------------------------------------------------------------------------------------------
**Title**: slopebench

**Web address**: https://gitorious.org/tlevine/slopebench

**Date**: March 2010

**Description**: I wonder whether the image has as much meaning when you don't see the words. Based on questionnaire responses, it seems that it does.

shsgradpictures
------------------------------------------------------------------------------------------------
**Title**: shsgradpictures

**Web address**: https://gitorious.org/tlevine/shsgradpictures

**Date**: March 2010

**Description**: Download pictures

moseyposters
------------------------------------------------------------------------------------------------
**Title**: moseyposters

**Web address**: https://gitorious.org/tlevine/moseyposters

**Date**: March 2010

**Description**: 

regression
------------------------------------------------------------------------------------------------
**Title**: regression

**Web address**: https://gitorious.org/tlevine/regression

**Date**: March 2010

**Description**: Calculate ordinary least squares regression geometrically

This would have been very useful to me when I was learning ordinary least squares regression.

randomperson
------------------------------------------------------------------------------------------------
**Title**: randomperson

**Web address**: https://gitorious.org/tlevine/randomperson

**Date**: March 2010

**Description**: Software for selecting a random student from the Cornell electronic directory and announcing it automatically

This can easily be extended to other schools and organizations

elmiracityhall
------------------------------------------------------------------------------------------------
**Title**: elmiracityhall

**Web address**: https://gitorious.org/tlevine/elmiracityhall

**Date**: March 2010

**Description**: Programming document of Elmira, New York's City Hall

oldhugemess
------------------------------------------------------------------------------------------------
**Title**: oldhugemess

**Web address**: https://gitorious.org/tlevine/oldhugemess

**Date**: December 2009

**Description**: 

chainsaw
------------------------------------------------------------------------------------------------
**Title**: chainsaw

**Web address**: https://gitorious.org/tlevine/chainsaw

**Date**: Unknown

**Description**: Web scraping tools

stsci3100
------------------------------------------------------------------------------------------------
**Title**: stsci3100

**Web address**: https://gitorious.org/tlevine/stsci3100

**Date**: Unknown

**Description**: Statistical sampling

web
------------------------------------------------------------------------------------------------
**Title**: web

**Web address**: https://gitorious.org/tlevine/web

**Date**: Unknown

**Description**: 

Simple Webcam
------------------------------------------------------------------------------------------------
**Title**: Simple Webcam

**Web address**: https://chrome.google.com/webstore/detail/simple-webcam/cejgmnpegppdhkmmgmdobfelcdgfhkmo?hl=en

**Date**: October 2012

**Description**: Simple Webcam is a webcam app for Chrome that just works. Open the app, click one button, then save your picture. That's it.

.medial_temporal_lobe
------------------------------------------------------------------------------------------------
**Title**: .medial_temporal_lobe

**Web address**: https://api.github.com/repos/tlevine/.medial_temporal_lobe

**Date**: October 2013

**Description**: 

.prophyl-teh-awesum
------------------------------------------------------------------------------------------------
**Title**: .prophyl-teh-awesum

**Web address**: https://api.github.com/repos/tlevine/.prophyl-teh-awesum

**Date**: September 2013

**Description**: prophyl-teh-awesum
===
I am a minimal shell `.profile` framework.

prophyl-teh-awesome stores aliases and small helpers that work across shells.
It doesn't provide themes; if you want those, try
[bash-it](https://github.com/revans/bash-it) or
[oh-my-zsh](https://github.com/robbyrussell/oh-my-zsh).

## Install

Install dependencies.

* uuidgen
* python
* 

Run this.

    git clone git://github.com/tlevine/.prophyl-teh-awesum ~/.prophyl-teh-awesum
    ~/.prophyl-teh-awesum/install
    . ~/.profile

## Customizing

Installing prophyl-teh-awesum links your `~/.profile` to
`~/.prophyl-teh-awesum/profile`. This profile script sources
all of the non-hidden files in `~/.prophyl-teh-awesum/source`
in alphabetical order.

Add scripts by putting them in this directory, disable them
by adding a dot to the front of the file name, and make
libraries by adding zeroes to the front of the file name so
they can be accessed from other files.

## Tests

Tests are run in another user account with urchin. Directions might be here eventually.

## Features
Features are documented here, with each section named after its filename.
When features are spread across multiple files, the files are named
`[feature name]-[component name]`, and the feature name is used as the
section heading here. For disabled features, the leading dot is not
used in the name.

### git
A bunch of aliases are provided.

### history
Every time a new r or sh terminal is opened, it is given a likely-unique
history file. Hacks achieve similar results for sqlite and python.

A daily crontab entry adds the history files to a git repository, commits
them and pushes them.

The `history-{sh,sqlite,r}` aliases display commonly-used commands.

### http
Start a web server in the current directory by running the command `http [port]`.
It tries using a variety of web servers in case you don't have one of them.

### ls
Learn `ls` flags! This defines an `l` command that randomly choose English
`ls` flag combinations.

    $ l -h
    usage: l -[dh] [file] ...
    -d  directories (ls -d)
    -h  this help

### scp
This reduces confusion when you forget to type a colon.
It wraps the `scp` command and warns you when you try
running it without a colon. An example:

    $ scp README.md tlevine@chainsaw.thomaslevine.com
    usage: scp [-12346BCpqrv] [-c cipher] [-F ssh_config] [-i identity_file]
               [-l limit] [-o ssh_option] [-P port] [-S program]
               [[user@]host1:]file1 ... [[user@]host2:]file2
     
    warning: The scp command that you ran has no colon, so you probably don't
           want to run it. Use "env scp" if you really meant to do that.

### tmux
Tmux commands are long, so this file provides some aliases.

    alias tmuxl='tmux list-sessions'
    alias tmuxa='tmux attach'

### urlencoding
Encode and decode url strings.

* `urlencode [string]`: Encode the string as a url.
* `urldecode [url]`: Decode the url.


## To do
This section documents unimplemented features in the spirit of
documentation-driven development.

### history
Storage of the history file is designed to make it easy to share history across
computers. Files are named `~/.history/<language>-<datetime>-<uuid>`. This way,
different sessions will not manipulate the same file, so merging histories is easy.

### history-ipython
Ipython history is stored as a sqlite3 database.


2008.1A
------------------------------------------------------------------------------------------------
**Title**: 2008.1A

**Web address**: https://api.github.com/repos/tlevine/2008.1A

**Date**: April 2013

**Description**: Google Code Jam 2013
======

http://code.google.com/codejam/contests.html


201204-homepage-experiment
------------------------------------------------------------------------------------------------
**Title**: 201204-homepage-experiment

**Web address**: https://api.github.com/repos/tlevine/201204-homepage-experiment

**Date**: May 2012

**Description**: An experiment was conducted to see how homepage layout affected whether
people would arrive at the /request_data page. The design is documented
in [a Google Spreadsheet](https://docs.google.com/a/scraperwiki.com/spreadsheet/ccc?key=0AmgaEqd-YKjXdGNENndyR1BScFMtdG45OEJyQXZaR2c)

I ran post-hoc pairwise Fisher's Exact Test on the six obvious comparisons.

    Unadjusted p-values

    within.businessy      within.notbusinessy          within.roundone 
                0.67                     0.71                     0.00 
     within.roundtwo businessy.v.notbusinessy      roundone.v.roundtwo 
                0.00                     0.00                     0.56 

    Bonferroni-adjusted p-values

    within.businessy      within.notbusinessy          within.roundone 
                4.03                     4.24                     0.00 
     within.roundtwo businessy.v.notbusinessy      roundone.v.roundtwo 
                0.00                     0.00                     3.34 

The adjusted p-values show significance for the differences between
businessy and non-businessy, and the unadjusted p-values show no
significant differences within businessyness conditions; it seems that
the businessyness rather an the minimalness explains the high score
for the minimal layout and that the minimal layout performed
unremarkably differently from the original blobby layout.


aaron-swartz
------------------------------------------------------------------------------------------------
**Title**: aaron-swartz

**Web address**: https://api.github.com/repos/tlevine/aaron-swartz

**Date**: January 2013

**Description**: Obituaries

* [Internet Archive](http://blog.archive.org/2013/01/12/aaron-swartz-hero-of-the-open-world-rip/)
* [BoingBoing](http://boingboing.net/2013/01/12/rip-aaron-swartz.html)
* [Lessig](http://lessig.tumblr.com/post/40347463044/prosecutor-as-bully)
* [Quinn Norton](http://www.quinnnorton.com/said/?p=641)
* [Quinn Norton, again](http://www.quinnnorton.com/said/?p=644)
* [MIT](http://tech.mit.edu/V132/N61/swartz.html)
* [New York Times](http://www.nytimes.com/2013/01/13/technology/aaron-swartz-internet-activist-dies-at-26.html?hp&_r=1&)

Tweets

* https://twitter.com/doingitwrong/status/290134967823003648
* https://twitter.com/benfinoradin/status/290143768215171072
* https://twitter.com/fredbenenson/status/290161119128068096
* https://twitter.com/tcarmody/status/290132878996680704
* https://twitter.com/waxpancake/status/290127963784101890
* https://twitter.com/tcarmody/status/290127769675890688
* https://twitter.com/justgrimes/status/290126959227326464
* https://twitter.com/tjl/status/290165347590475776
* https://twitter.com/mattblaze/status/290135923163791360
* https://twitter.com/zephoria/status/290157904462483456
* https://twitter.com/paulsmith/status/290153316145582080
* https://twitter.com/jcstearns/status/290178703676301314

Email

* http://lists.w3.org/Archives/Public/www-tag/2013Jan/0017.html
* http://lists.freeculture.org/pipermail/discuss/2013-January/007109.html


alameda-houses
------------------------------------------------------------------------------------------------
**Title**: alameda-houses

**Web address**: https://api.github.com/repos/tlevine/alameda-houses

**Date**: July 2013

**Description**: It looks like this is a list with parcel identifiers.
https://data.acgov.org/Government/Alameda-County-Parcel-Boundaries/2m43-xsic?
http://www.acgov.org/government/geospatial.htm

> Unpaid taxes are just that: a record of an outstanding amount. I used this
> web page to look up property tax history on parcels:
> http://www.acgov.org/propertytax/
> 
> Also you can browse through this list of links to get an idea of other
> kinds of useful info/documentation/history on properties.  It would be nice
> to have a way to consolidate this info into one lookup.
> http://www.homesnotjailssf.org/wb/pages/whatwebsitesarehelpfulforresearchinginformationaboutvacantbuildings.php
> 
> Finally, a great indicator of neglect on a property is if there have been
> blight notices posted, or blight related liens on a property. If you could
> generate lists of blighted properties, we could use the address info to do
> recon and discover promising properties


appgen-ops
------------------------------------------------------------------------------------------------
**Title**: appgen-ops

**Web address**: https://api.github.com/repos/tlevine/appgen-ops

**Date**: April 2013

**Description**: AppGen DevOps
======

## Hosting
AppGen generates random NYC Big Apps. All apps are served from one Sinatra
instance, with a seed specifying a particular app. In order to hide that these
are all the same apps, we are proxying them through various cloudsourced web
services. Specifically, we're using

* Heroku ([API documentation](https://api-docs.heroku.com))
* NearlyFreeSpeech ([API documentation](https://members.nearlyfreespeech.net/wiki/API/Introduction))
* Google AppEngine
  * [AppEngine site](https://appengine.google.com)
  * [Bad documentation](https://developers.google.com/appengine/docs/adminconsole/index)
  * [Tutorial](http://www.labnol.org/internet/setup-proxy-server/12890/)
* Nodejitsu [API documentation](https://www.nodejitsu.com/documentation/api/)

Here are some [other ideas](http://www.quora.com/Is-there-anything-like-Heroku-I-can-use-for-a-PHP-site).

### How to
Deploy a random app proxy to Heroku.

    ./heroku.sh

## Facebook accounts
You need a facebook account to submit something on CollabFinder. You can submit
multiple apps, so the AppGen team could just submit all of the apps, but the
city might catch on. Instead, we're going to tell other people to submit the
apps for us. In case that doesn't work, we can also consider buying Facebook
accounts. Here are some vendors.

* https://buyaccs.com/en/
* http://amazingdevelopers.com/idsforsale/facebook-pva/
* http://www.dataentryassistant.com/order-services/social-media-accounts/facebook-accounts.html

## Proxy server

* [php-simple-proxy](http://benalman.com/projects/php-simple-proxy/) ([code](https://raw.github.com/cowboy/php-simple-proxy/master/ba-simple-proxy.php))


april-25-stats
------------------------------------------------------------------------------------------------
**Title**: april-25-stats

**Web address**: https://api.github.com/repos/tlevine/april-25-stats

**Date**: April 2013

**Description**: April 25 Statistics Talk Statistics
===
Two statistics talks are competing in New York on April 25, 2013.

* [Data in the Big City](http://www.meetup.com/DataKind-NYC/events/112727792/)
    ([DataKind](http://www.meetup.com/DataKind-NYC/))
* [Bigvis: visualising 100,000,000 observations in R with Hadley Wickham](http://www.meetup.com/nyhackr/events/112271042/)
    ([New York Open Statistical Programming Meetup](http://www.meetup.com/nyhackr/))

I'm downloading the RSVPs to see who signs up for which one or both.

    # Write crontab
    echo "*/30 * * * * cd '$PWD' && ./download.sh"

Each time the file changes, a new datestamped file is saved.


april-25-stats-downloads
------------------------------------------------------------------------------------------------
**Title**: april-25-stats-downloads

**Web address**: https://api.github.com/repos/tlevine/april-25-stats-downloads

**Date**: April 2013

**Description**: 

arabic-tweets
------------------------------------------------------------------------------------------------
**Title**: arabic-tweets

**Web address**: https://api.github.com/repos/tlevine/arabic-tweets

**Date**: March 2013

**Description**: Arabic Tweets Data
====
Arabic Tweets from the Qatar Institute are prepared in a few convenient formats
for use at the DataKind datadive. The data are plyed in a few different ways
and are converted into a few different formats for fast loading into different
programs. A server build script is also created so that we can deploy a few
servers and load the data into the respective programs before the datadive
starts.

## Formats
Data are available as a 25GB CSV file on S3. Talk to Tom if you want that one.
You can also get random samples of that file of varying sizes.

* [20,000 rows](http://arabictweets.s3-website-us-west-2.amazonaws.com/tweets-sample-20000.csv)
* [100,000 rows](http://arabictweets.s3-website-us-west-2.amazonaws.com/tweets-sample-100000.csv)
* [500,000 rows](http://arabictweets.s3-website-us-west-2.amazonaws.com/tweets-sample-500000.csv)

It's also in a MySQL database, and we have code for loading them into R and
Python (Pandas).

## Converting data
Here are instructions for running the scripts that clean up the spreadsheets
slightly, load them into MySQL and then take them out in other formats

### Cleaning up spreadsheets
Download the files to the `tweets` directory, and gunzip them; they'll now be
named `tweets/tweets_ar_1?[0-9].txt`. We stored them in S3, so `s3cmd` was
helpful.

Load each source spreadsheet into R, clean it up, then export it as another
spreadsheet, this time a csv.

    Rscript clean.r

### Into the database
Add the database to your `~/.my.cnf`, then set up the schema.

    mysql tweets < schema.sql

Add the users while you're at it.

    mysql tweets < users.sql

Then load the data.

    ./import.sh

Once that finishes, everything will be in the `tweets.tweets` table.

### Out of the database
The `python` and `r` directories have boilerplate code for accessing the data
from the MySQL server.

The `subset` directory has scripts that produce small spreadsheets that are
subsets and other slices of the data.

## Infrastructure
We have some files that are suitable for analysis on a laptop, but we've also
loaded the full dataset into a server in Virginia and have set up some other
servers in Virginia from which you can analyse the dataset. 

All of the systems are AWS EC2 or RDS instances. I've been using the ones with
15GB of RAM ([m2.xlarge](http://aws.amazon.com/ec2/instance-types/) and
db.m2.xlarge, respectively), which is enough to fit most of the data in RAM.
The m2.2xlarge should be enough to fit the whole thing in RAM, and we can
easily use different instances if you need more power.

### Database
Data are in a canonical MySQL database. You can access it with
[this .my.cnf](.my.cnf) or just with this command:

    mysql -usuperhero -pJ4j5yq6P6c4 -h tweets2.carpklcd5jnh.us-east-1.rds.amazonaws.com

This account only has `SELECT` permissions on the `tweets` table; you need not
worry that you will accidentally alter the data if you use this account.

If lots of people start hitting this, we'll spin up more databases with the same data.

You'll probably want to alter the database somewhat, so we'll make database
users with more privileges.

### iPython Notebook
We have iPython Notebooks running in Virginia on
[NotebookCloud](https://notebookcloud.appspot.com) images.

### RStudio
We're using the [Bioconductor AMI](http://bioconductor.org/help/bioconductor-cloud-ami/).
Check the "Outputs" tab for the box's address.
Make sure to use the version with SSH access so you can add these extra things:

    # As root
    apt-get update
    apt-get install libmysqlclient-dev s3cmd
    adduser [username]

    # As the user
    cd
    git clone git://github.com/tlevine/arabic-tweets.git
    ln -s arabic-tweets/.my.cnf .
    Rscript -e "install.packages('RMySQL')"

Also, note that most of the storage is mounted on `/mnt`.


### Useful snippets
Wait for the csv to be ready, then load them.

    while true; do if test $(ls tweets/*.csv | wc -l) -eq 12; then sleep 1h && ./import.sh; break; else sleep 10; fi; don

Transfer things to and from S3.

    s3cmd put tweets_ar_1.txt.csv s3://arabictweets
    s3cmd get s3://arabictweets/tweets_ar_1.txt.csv

I got these warnings.

    $ mysqlimport tweets --local --default-character-set=utf8 --fields-enclosed-by=\" --fields-terminated-by=, --lines-terminated-by=\\n --columns=username,userid,id,date,text tweets/tweets.csv
    tweets.tweets: Records: 55477934  Deleted: 0  Skipped: 54916  Warnings: 8477983

It's on 8.5 million out of 55 million records. In order to see the warnings,
I need to [run `SHOW WARNINGS` from the same session](http://serverfault.com/questions/96401/how-does-one-list-warnings-from-the-mysqlimport-utility),
which you can't do with `mysqlimport`. So maybe I'll try running this inside
a session.

The range of dates is November 20, 2011 to March 16, 2013.

    mysql> select min(date), max(date) from tweets;
    +---------------------+---------------------+
    | min(date)           | max(date)           |
    +---------------------+---------------------+
    | 2011-11-20 17:14:40 | 2013-03-16 12:42:58 |
    +---------------------+---------------------+
    1 row in set (0.00 sec)



audio-cable-power-controller
------------------------------------------------------------------------------------------------
**Title**: audio-cable-power-controller

**Web address**: https://api.github.com/repos/tlevine/audio-cable-power-controller

**Date**: August 2012

**Description**: Monitor audio cable signal to change speaker power
================

I want a thingy that listens to an audio cable and turns a power plug on and
off depending on whether music is playing. It's a box with four ports:

* Female 3.5 mm audio
* Male 3.5 mm audio
* Female power plug
* Male power plug

When sound is coming through the audio cable, power goes through; when sound
isn't coming through, power doesn't go through. This may be more easily
to implement if we approximate this as follows: When sound is comes through,
power gets turned on; when sound hasn't come through for a minute, power gets
turned off.


avloppsguiden-engelska
------------------------------------------------------------------------------------------------
**Title**: avloppsguiden-engelska

**Web address**: https://api.github.com/repos/tlevine/avloppsguiden-engelska

**Date**: August 2012

**Description**: 

Plain text pages are stored in the `page_sources` table.

Binary files are stored in the `binary_files` table and
referenced in the `nontext_pages` column.


avloppsguiden-husagare
------------------------------------------------------------------------------------------------
**Title**: avloppsguiden-husagare

**Web address**: https://api.github.com/repos/tlevine/avloppsguiden-husagare

**Date**: August 2012

**Description**: 

avloppsguiden-husagare-english
------------------------------------------------------------------------------------------------
**Title**: avloppsguiden-husagare-english

**Web address**: https://api.github.com/repos/tlevine/avloppsguiden-husagare-english

**Date**: January 2013

**Description**: # Avloppsguiden pAY= Engelska

Redigera dokumenten i det hA$?r git repository och gAY=
<s>[hit](http://sewerguide.thomaslevine.com)</s>
fAPr att visa webbplatsen.

Skript A$?r i `bin`.

    bin/map.sh bin/tabs.sh

## Prioriteringar

### ?

* [/avloppsteknik.html](http://sewerguide.thomaslevine.com/avloppsteknik.html)
* [/v%C3%A4lj-teknik-.html](http://sewerguide.thomaslevine.com/v%C3%A4lj-teknik-.html)
* [/wc-och-infiltration.html](http://sewerguide.thomaslevine.com/wc-och-infiltration.html)
* [/urinsorterande-wc-och-markb%C3%A4dd.html](http://sewerguide.thomaslevine.com/urinsorterande-wc-och-markb%C3%A4dd.html)
* [/sluten-tank-och-infiltration-av-bdt-.html](http://sewerguide.thomaslevine.com/sluten-tank-och-infiltration-av-bdt-.html)
* [/urinsorterande-torrtoalett-och-kompaktfilter.html](http://sewerguide.thomaslevine.com/urinsorterande-torrtoalett-och-kompaktfilter.html)
* [/wc-med-kemisk-f%C3%A4llning-och-markb%C3%A4dd.html](http://sewerguide.thomaslevine.com/wc-med-kemisk-f%C3%A4llning-och-markb%C3%A4dd.html)
* [/wc-med-minireningsverk-och-efterbehandling.html](http://sewerguide.thomaslevine.com/wc-med-minireningsverk-och-efterbehandling.html)
* [/wc-med--markb%C3%A4dd-och-fosforf%C3%A4lla.html](http://sewerguide.thomaslevine.com/wc-med--markb%C3%A4dd-och-fosforf%C3%A4lla.html)

### Toilet types

* [/toaletter.html](http://sewerguide.thomaslevine.com/toaletter.html)
* [/urinsorterande-torrtoalett.html](http://sewerguide.thomaslevine.com/urinsorterande-torrtoalett.html)
* [/multrum-och-mulltoaletter.html](http://sewerguide.thomaslevine.com/multrum-och-mulltoaletter.html)
* [/utedass-(latrin](http://sewerguide.thomaslevine.com/utedass-(latrin).html).html)
* [/f%C3%B6rbr%C3%A4nningstoalett.html](http://sewerguide.thomaslevine.com/f%C3%B6rbr%C3%A4nningstoalett.html)
* [/%C3%B6vriga-torrtoaletter-.html](http://sewerguide.thomaslevine.com/%C3%B6vriga-torrtoaletter-.html)
* [/vanlig-wc.html](http://sewerguide.thomaslevine.com/vanlig-wc.html)
* [/extremt-sn%C3%A5lspolande-toalett.html](http://sewerguide.thomaslevine.com/extremt-sn%C3%A5lspolande-toalett.html)
* [/vakuumtoalett.html](http://sewerguide.thomaslevine.com/vakuumtoalett.html)
* [/urinsorterande-vattentoalett.html](http://sewerguide.thomaslevine.com/urinsorterande-vattentoalett.html)
* [/ledningar.html](http://sewerguide.thomaslevine.com/ledningar.html)
* [/slamavskiljare.html](http://sewerguide.thomaslevine.com/slamavskiljare.html)

### Treatment options

* [/behandling.html](http://sewerguide.thomaslevine.com/behandling.html)
* [/infiltration.html](http://sewerguide.thomaslevine.com/infiltration.html)
* [/markb%C3%A4dd.html](http://sewerguide.thomaslevine.com/markb%C3%A4dd.html)
* [/prefabricerat-filter.html](http://sewerguide.thomaslevine.com/prefabricerat-filter.html)
* [/sprayfilter.html](http://sewerguide.thomaslevine.com/sprayfilter.html)
* [/kemisk-f%C3%A4llning.html](http://sewerguide.thomaslevine.com/kemisk-f%C3%A4llning.html)
* [/fosforfilter.html](http://sewerguide.thomaslevine.com/fosforfilter.html)
* [/stort-fosforfilter.html](http://sewerguide.thomaslevine.com/stort-fosforfilter.html)
* [/minireningsverk.html](http://sewerguide.thomaslevine.com/minireningsverk.html)
* [/sluten-tank.html](http://sewerguide.thomaslevine.com/sluten-tank.html)

### Finishing

* [/biofilterdike.html](http://sewerguide.thomaslevine.com/biofilterdike.html)
* [/resorptionsdike.html](http://sewerguide.thomaslevine.com/resorptionsdike.html)
* [/%C3%B6versilning.html](http://sewerguide.thomaslevine.com/%C3%B6versilning.html)
* [/v%C3%A5tmarkdamm.html](http://sewerguide.thomaslevine.com/v%C3%A5tmarkdamm.html)
* [/bevattning.html](http://sewerguide.thomaslevine.com/bevattning.html)
* [/rotzonsanl%C3%A4ggning.html](http://sewerguide.thomaslevine.com/rotzonsanl%C3%A4ggning.html)
* [/infiltration.html](http://sewerguide.thomaslevine.com/infiltration.html)
* [/markb%C3%A4dd.html](http://sewerguide.thomaslevine.com/markb%C3%A4dd.html)

### Disposal onsite

* [/ta-hand-om-urin-latrin-och-slam-p%C3%A5-din-egen-tomt.html](http://sewerguide.thomaslevine.com/ta-hand-om-urin-latrin-och-slam-p%C3%A5-din-egen-tomt.html)


axis-ticks
------------------------------------------------------------------------------------------------
**Title**: axis-ticks

**Web address**: https://api.github.com/repos/tlevine/axis-ticks

**Date**: March 2013

**Description**: Where should ticks be for a given plot?
====

## How to use
Clone this repository, then run this.

    ghc tick.hs
    ./tick [minimum data value] [maximum data value] [desired tick count]

You can also use the Python version, but it only supports zero as the minimum
data value.

    ./tick.py [max data value] [desired tick count] 

For example, to choose about eight ticks for data whose maximum is 234234.

    ./tick.py 234234 8
    [0.0, 50000.0, 100000.0, 150000.0, 200000.0, 250000.0]

Or if it's 0.8080841

    python2 tick.py 0.8080841 9
    [0.0, 0.1, 0.2, 0.30000000000000004, 0.4, 0.5, 0.6000000000000001, 0.7000000000000001, 0.8]

Here are some more examples.

    for max in $(seq 0 100); do echo -n $max:\  && python2 ./tick.py $max 5; done
    0: [0]
    1: [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]
    2: [0.0, 0.5, 1.0, 1.5, 2.0]
    3: [0.0, 1.0, 2.0, 3.0]
    4: [0.0, 1.0, 2.0, 3.0, 4.0]
    5: [0.0, 1.0, 2.0, 3.0, 4.0, 5.0]
    6: [0.0, 2.0, 4.0, 6.0]
    7: [0.0, 2.0, 4.0, 6.0, 8.0]
    8: [0.0, 2.0, 4.0, 6.0, 8.0]
    9: [0.0, 2.0, 4.0, 6.0, 8.0]
    10: [0.0, 2.0, 4.0, 6.0, 8.0, 10.0]
    11: [0.0, 2.0, 4.0, 6.0, 8.0, 10.0]
    12: [0.0, 5.0, 10.0, 15.0]
    13: [0.0, 5.0, 10.0, 15.0]
    14: [0.0, 5.0, 10.0, 15.0]
    15: [0.0, 5.0, 10.0, 15.0]
    16: [0.0, 5.0, 10.0, 15.0, 20.0]
    17: [0.0, 5.0, 10.0, 15.0, 20.0]
    18: [0.0, 5.0, 10.0, 15.0, 20.0]
    19: [0.0, 5.0, 10.0, 15.0, 20.0]
    20: [0.0, 5.0, 10.0, 15.0, 20.0]
    21: [0.0, 5.0, 10.0, 15.0, 20.0]
    22: [0.0, 5.0, 10.0, 15.0, 20.0]
    23: [0.0, 5.0, 10.0, 15.0, 20.0]
    24: [0.0, 5.0, 10.0, 15.0, 20.0]
    25: [0.0, 5.0, 10.0, 15.0, 20.0, 25.0]
    26: [0.0, 5.0, 10.0, 15.0, 20.0, 25.0]
    27: [0.0, 5.0, 10.0, 15.0, 20.0, 25.0]
    28: [0.0, 5.0, 10.0, 15.0, 20.0, 25.0]
    29: [0.0, 5.0, 10.0, 15.0, 20.0, 25.0]
    30: [0.0, 10.0, 20.0, 30.0]
    31: [0.0, 10.0, 20.0, 30.0]
    32: [0.0, 10.0, 20.0, 30.0]
    33: [0.0, 10.0, 20.0, 30.0]
    34: [0.0, 10.0, 20.0, 30.0]
    35: [0.0, 10.0, 20.0, 30.0]
    36: [0.0, 10.0, 20.0, 30.0]
    37: [0.0, 10.0, 20.0, 30.0]
    38: [0.0, 10.0, 20.0, 30.0]
    39: [0.0, 10.0, 20.0, 30.0]
    40: [0.0, 10.0, 20.0, 30.0, 40.0]
    41: [0.0, 10.0, 20.0, 30.0, 40.0]
    42: [0.0, 10.0, 20.0, 30.0, 40.0]
    43: [0.0, 10.0, 20.0, 30.0, 40.0]
    44: [0.0, 10.0, 20.0, 30.0, 40.0]
    45: [0.0, 10.0, 20.0, 30.0, 40.0]
    46: [0.0, 10.0, 20.0, 30.0, 40.0]
    47: [0.0, 10.0, 20.0, 30.0, 40.0]
    48: [0.0, 10.0, 20.0, 30.0, 40.0]
    49: [0.0, 10.0, 20.0, 30.0, 40.0]
    50: [0.0, 10.0, 20.0, 30.0, 40.0, 50.0]
    51: [0.0, 10.0, 20.0, 30.0, 40.0, 50.0]
    52: [0.0, 10.0, 20.0, 30.0, 40.0, 50.0]
    53: [0.0, 10.0, 20.0, 30.0, 40.0, 50.0]
    54: [0.0, 10.0, 20.0, 30.0, 40.0, 50.0]
    55: [0.0, 10.0, 20.0, 30.0, 40.0, 50.0]
    56: [0.0, 10.0, 20.0, 30.0, 40.0, 50.0]
    57: [0.0, 10.0, 20.0, 30.0, 40.0, 50.0]
    58: [0.0, 10.0, 20.0, 30.0, 40.0, 50.0]
    59: [0.0, 10.0, 20.0, 30.0, 40.0, 50.0]
    60: [0.0, 20.0, 40.0, 60.0]
    61: [0.0, 20.0, 40.0, 60.0]
    62: [0.0, 20.0, 40.0, 60.0]
    63: [0.0, 20.0, 40.0, 60.0]
    64: [0.0, 20.0, 40.0, 60.0]
    65: [0.0, 20.0, 40.0, 60.0]
    66: [0.0, 20.0, 40.0, 60.0]
    67: [0.0, 20.0, 40.0, 60.0]
    68: [0.0, 20.0, 40.0, 60.0]
    69: [0.0, 20.0, 40.0, 60.0]
    70: [0.0, 20.0, 40.0, 60.0, 80.0]
    71: [0.0, 20.0, 40.0, 60.0, 80.0]
    72: [0.0, 20.0, 40.0, 60.0, 80.0]
    73: [0.0, 20.0, 40.0, 60.0, 80.0]
    74: [0.0, 20.0, 40.0, 60.0, 80.0]
    75: [0.0, 20.0, 40.0, 60.0, 80.0]
    76: [0.0, 20.0, 40.0, 60.0, 80.0]
    77: [0.0, 20.0, 40.0, 60.0, 80.0]
    78: [0.0, 20.0, 40.0, 60.0, 80.0]
    79: [0.0, 20.0, 40.0, 60.0, 80.0]
    80: [0.0, 20.0, 40.0, 60.0, 80.0]
    81: [0.0, 20.0, 40.0, 60.0, 80.0]
    82: [0.0, 20.0, 40.0, 60.0, 80.0]
    83: [0.0, 20.0, 40.0, 60.0, 80.0]
    84: [0.0, 20.0, 40.0, 60.0, 80.0]
    85: [0.0, 20.0, 40.0, 60.0, 80.0]
    86: [0.0, 20.0, 40.0, 60.0, 80.0]
    87: [0.0, 20.0, 40.0, 60.0, 80.0]
    88: [0.0, 20.0, 40.0, 60.0, 80.0]
    89: [0.0, 20.0, 40.0, 60.0, 80.0]
    90: [0.0, 20.0, 40.0, 60.0, 80.0]
    91: [0.0, 20.0, 40.0, 60.0, 80.0]
    92: [0.0, 20.0, 40.0, 60.0, 80.0]
    93: [0.0, 20.0, 40.0, 60.0, 80.0]
    94: [0.0, 20.0, 40.0, 60.0, 80.0]
    95: [0.0, 20.0, 40.0, 60.0, 80.0]
    96: [0.0, 20.0, 40.0, 60.0, 80.0]
    97: [0.0, 20.0, 40.0, 60.0, 80.0]
    98: [0.0, 20.0, 40.0, 60.0, 80.0]
    99: [0.0, 20.0, 40.0, 60.0, 80.0]
    100: [0.0, 20.0, 40.0, 60.0, 80.0, 100.0]

## Brainstorm

Let's start with just knowledge of the maximum value.

### Number bases
Why not just use the maximum value? People are used to special number bases.

* **1s** 1, 2, 3, 4, 5
* **2s** 2, 4, 6, 8, 10
* **5s** 5, 10, 15, 20
* **10s** 10, 20, 30, 40

Hmm these are interesting because 1s are like 10s, just with different
numbers of zeroes, and **2s** * **5s** = **10s**

### How many ticks per plot
Only so many ticks fit on a plot. Maybe 10?

### Maximum value
How can we handle when the maximum value not is convenient? Let's group that
into

1. Add a tick above the maximum value.
2. Don't add a tick above the maximum value.

How far from the maximum value is acceptable? Maybe one tenth of the plot?
One fifth? Maybe it's different depending on whether it's up or down?

### Can we limit the number bases?
The max number dictates the order of magnitude of the ticks, but how do we
choose whether to use **1s**, **2s** or **5s**. Maybe it's fine to always
use **1s**. Hmm what are the extreme situations.

If the max number is 120, counting by **1s** is fine up to 120. But it's
not fine if the max number is 180 or 200. What should we do here? So we
can't limit number bases to just **1s**.

## Idea
First, express the maximum value in scientific notation. Work only with the
part that's not an exponent of ten; add that part back in at the end.

Second, propose a few tick ranges based on the number bases. Choose the one
for which the max tick is close to the maximum data value and for which the
number of tick marks is close to the desired number..

Finally, add the order of magnitude back.

## More ideas
R seems to do this pretty well. Actually, it seems quite similar to mine.
But the algorithm is somewhat mysterious; I wonder how it works.
`plot` calls something equivalent `axTicks`.

	axTicks
	function (side, axp = NULL, usr = NULL, log = NULL, nintLog = NULL) 
	{
		if (!(side <- as.integer(side)) %in% 1L:4L) 
		    stop("'side' must be in {1:4}")
		is.x <- side%%2 == 1
		XY <- function(ch) paste0(if (is.x) 
		    "x"
		else "y", ch)
		if (is.null(axp)) 
		    axp <- par(XY("axp"))
		else if (!is.numeric(axp) || length(axp) != 3) 
		    stop("invalid 'axp'")
		if (is.null(log)) 
		    log <- par(XY("log"))
		else if (!is.logical(log) || any(is.na(log))) 
		    stop("invalid 'log'")
		if (log && axp[3L] > 0) {
		    if (!any((iC <- as.integer(axp[3L])) == 1L:3L)) 
		        stop("invalid positive 'axp[3]'")
		    if (is.null(usr)) 
		        usr <- par("usr")[if (is.x) 
		            1:2
		        else 3:4]
		    else if (!is.numeric(usr) || length(usr) != 2) 
		        stop("invalid 'usr'")
		    if (is.null(nintLog)) 
		        nintLog <- par("lab")[2L - is.x]
		    if (is.finite(nintLog)) {
		        axisTicks(usr, log = log, axp = axp, nint = nintLog)
		    }
		    else {
		        if (needSort <- is.unsorted(usr)) {
		            usr <- usr[2:1]
		            axp <- axp[2:1]
		        }
		        else axp <- axp[1:2]
		        ii <- round(log10(axp))
		        x10 <- 10^((ii[1L] - (iC >= 2L)):ii[2L])
		        r <- switch(iC, x10, c(outer(c(1, 5), x10))[-1L], 
		            c(outer(c(1, 2, 5), x10))[-1L])
		        if (needSort) 
		            r <- rev(r)
		        r[usr[1L] <= log10(r) & log10(r) <= usr[2L]]
		    }
		}
		else {
		    seq.int(axp[1L], axp[2L], length.out = 1L + abs(axp[3L]))
		}
	}
	<bytecode: 0x1699bb0>
	<environment: namespace:graphics>

which calls `axisTicks`

	axisTicks
	function (usr, log, axp = NULL, nint = 5) 
	{
		if (is.null(axp)) 
		    axp <- unlist(.axisPars(usr, log = log, nintLog = nint), 
		        use.names = FALSE)
		.Call(R_CreateAtVector, axp, if (log) 10^usr else usr, nint, 
		    log)
	}
	<bytecode: 0x1f11c20>
	<environment: namespace:grDevices>

which calls `R_CreateAtVector` (in C) and `.axisPars`. `.axisPars` calls
`R_GAxisPars` (in C).

	.axisPars
	function (usr, log = FALSE, nintLog = 5) 
	{
		.Call(R_GAxisPars, usr, log, nintLog)
	}
	<bytecode: 0x1f14620>
	<environment: namespace:grDevices>

Both of these C functions are in
[`axis_scales.c`](https://svn.r-project.org/R/trunk/src/library/grDevices/src/axis_scales.c).

    /*
     *  R : A Computer Language for Statistical Data Analysis
     *  Copyright (C) 2004-11   The R Core Team.
     *
     *  This program is free software; you can redistribute it and/or modify
     *  it under the terms of the GNU General Public License as published by
     *  the Free Software Foundation; either version 2 of the License, or
     *  (at your option) any later version.
     *
     *  This program is distributed in the hope that it will be useful,
     *  but WITHOUT ANY WARRANTY; without even the implied warranty of
     *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
     *  GNU General Public License for more details.
     *
     *  You should have received a copy of the GNU General Public License
     *  along with this program; if not, a copy is available at
     *  http://www.r-project.org/Licenses/
     */

    #ifdef HAVE_CONFIG_H
    #include <config.h>
    #endif
    
    #include <R.h>
    #include <Rinternals.h>
    #include <R_ext/GraphicsEngine.h>
     
    #include "grDevices.h"

    SEXP R_CreateAtVector(SEXP axp, SEXP usr, SEXP nint, SEXP is_log)
    {
        int nint_v = asInteger(nint);
        Rboolean logflag = asLogical(is_log);

        axp = coerceVector(axp, REALSXP);
        usr = coerceVector(usr, REALSXP);
        if(LENGTH(axp) != 3) error(_("'%s' must be numeric of length %d"), "axp", 3);
        if(LENGTH(usr) != 2) error(_("'%s' must be numeric of length %d"), "usr", 2);

        return CreateAtVector(REAL(axp), REAL(usr), nint_v, logflag);
        // -> ../../../main/plot.c
    }

    SEXP R_GAxisPars(SEXP usr, SEXP is_log, SEXP nintLog)
    {
        Rboolean logflag = asLogical(is_log);
        int n = asInteger(nintLog);// will be changed on output ..
        double min, max;
        const char *nms[] = {"axp", "n", ""};
        SEXP axp, ans;

        usr = coerceVector(usr, REALSXP);
        if(LENGTH(usr) != 2) error(_("'%s' must be numeric of length %d"), "usr", 2);
        min = REAL(usr)[0];
        max = REAL(usr)[1];

        GAxisPars(&min, &max, &n, logflag, 0);// axis = 0 :<==> do not warn.. [TODO!]
        // -> ../../../main/graphics.c

        PROTECT(ans = mkNamed(VECSXP, nms));
        SET_VECTOR_ELT(ans, 0, (axp = allocVector(REALSXP, 2)));// protected
        SET_VECTOR_ELT(ans, 1, ScalarInteger(n));
        REAL(axp)[0] = min;
        REAL(axp)[1] = max;

        UNPROTECT(1);
        return ans;
    }

The plotting code is intertwined with the tick decision code, so reading this
is a bit unpleasant. This section of `axTicks` looks like the interesting bit.

    if (needSort <- is.unsorted(usr)) {
        usr <- usr[2:1]
        axp <- axp[2:1]
    }
    else axp <- axp[1:2]
    ii <- round(log10(axp))
    x10 <- 10^((ii[1L] - (iC >= 2L)):ii[2L])
    r <- switch(iC, x10, c(outer(c(1, 5), x10))[-1L], 
        c(outer(c(1, 2, 5), x10))[-1L])
    if (needSort) 
        r <- rev(r)
    r[usr[1L] <= log10(r) & log10(r) <= usr[2L]]

From the `axTicks` documentation,

		 axp: numeric vector of length three, defaulting to apar("xaxp")a
		      or apar("yaxp")a depending on the asidea argument
		      (apar("xaxp")a if asidea is 1 or 3, apar("yaxp")a if side is
		      2 or 4).

		 usr: numeric vector of length two giving user coordinate limits,
		      defaulting to the relevant portion of apar("usr")a
		      (apar("usr")[1:2]a or apar("usr")[3:4]a for asidea in (1,3)
		      or (2,4) respectively).

		 log: logical indicating if log coordinates are active; defaults to
		      apar("xlog")a or apar("ylog")a depending on asidea.

	 nintLog: (only used when aloga is true): approximate (lower bound for
		      the) number of tick intervals; defaults to apar("lab")[j]a
		      where aja is 1 or 2 depending on asidea.  Set this to aInfa
		      if you want the same behavior as in earlier R versions (than
		      2.14.x).

I dunno how `axp` and `usr` interact, but it looks like the function cares more
about `axp`; here's what the `?par` help says.

     axaxpa A vector of the form ac(x1, x2, n)a giving the coordinates
          of the extreme tick marks and the number of intervals between
          tick-marks when apar("xlog")a is false.  Otherwise, when
          _log_ coordinates are active, the three values have a
          different meaning: For a small range, ana is _negative_, and
          the ticks are as in the linear case, otherwise, ana is in
          a1:3a, specifying a case number, and ax1a and ax2a are the
          lowest and highest power of 10 inside the user coordinates,
          a10 ^ par("usr")[1:2]a. (The a"usr"a coordinates are
          log10-transformed here!)

          n=1 will produce tick marks at 10^j for integer j,

          n=2 gives marks k 10^j with k in {1,5},

          n=3 gives marks k 10^j with k in {1,2,5}.

          See aaxTicks()a for a pure R implementation of this.

          This parameter is reset when a user coordinate system is set
          up, for example by starting a new page or by calling
          aplot.windowa or setting apar("usr")a: ana is taken from
          apar("lab")a.  It affects the default behaviour of subsequent
          calls to aaxisa for sides 1 or 3.

That sounds like same algorithm as the one I happened to write.

I don't know where this `iC` comes from, but I think this is the important bit
of the code: First, make sure `axp` is ordered properly. Then separate the order
of magnitude.

    ii <- round(log10(axp))
    x10 <- 10^((ii[1L] - (iC >= 2L)):ii[2L])

It looks like that `iC` variable is the `n` that is referenced in the `par`
documentation. If it's 1, we get the ticks at 10^j (`x10`), if it's 2, we
get the version for k in {1,5}, and if it's 3, we get it for k in {1, 2, 5}.

    r <- switch(iC, x10, c(outer(c(1, 5), x10))[-1L], 
        c(outer(c(1, 2, 5), x10))[-1L])

And then `r` is subset somehow.

    r[usr[1L] <= log10(r) & log10(r) <= usr[2L]]

And then I went to sleep.


banana
------------------------------------------------------------------------------------------------
**Title**: banana

**Web address**: https://api.github.com/repos/tlevine/banana

**Date**: January 2013

**Description**: 

barelywebgit
------------------------------------------------------------------------------------------------
**Title**: barelywebgit

**Web address**: https://api.github.com/repos/tlevine/barelywebgit

**Date**: June 2012

**Description**: BarelyWebGit
===========

BarelyWebGit statically generates a barely-web-site that inside of a
[NearlyFreeSpeech.net](http://nearlyfreespeech.net) (NFSN) site. Well you
could host the site somewhere else, but the default configuration uses
NFSN's directory structure, and you'd probably be able
to use something fancier if you were hosting somewhere.

## How to

First, set up the public version.

```#sh
# Download
git clone git@github.com:tlevine/barelywebgit.git
cd barelywebgit

# Install dependencies
./barelywebgit deps

# Create a bare repository on the server.
accountname_sitename=tom_thomaslevine-git # <-- Change this for your site.
git remote add nfsn $accountname_sitename@ssh.phx.nearlyfreespeech.net:~/barelywebgit.git
cmd="mkdir $repository_name; cd $repository_name; git init; echo  -e '[receive]\ndenyCurrentBranch = ignore' >> .git/config"
ssh $accountname_sitename@ssh.phx.nearlyfreespeech.net:~/barelywebgit.git $cmd

# Switch to the deploy branch
git checkout deploy
git push -u origin deploy
```

Edit your configuration inside of deploy. If you want to change BarelyWebGit,
make a branch from master so that it's easy to contribute upstream; master is
set up to ignore the configuration files.

Now we can configure a password.

```#sh
echo Dunno how this will work yet
```

Or not. Then we push our private repositories to `/home/private`, which is
also `$HOME`.

```#sh
cd some/other/repository_name
./barelywebgit init origin $accountname_sitename@ssh.phx.nearlyfreespeech.net
git push -u origin master
```

For now, repository names can't have spaces, equal-signs or pipes (`/[ =|]/`).

### More how-to

In case you're curious, `barelywebgit init` just runs this

```#sh
repository_name="`basename \`pwd\``"
git remote add origin "$accountname_sitename@ssh.phx.nearlyfreespeech.net:~/$repository_name.git"
ssh "$accountname_sitename@ssh.phx.nearlyfreespeech.net:~/barelywebgit.git git init --bare $repository_name.git"
```

If you want to change the directory structure, probably because you're not
using NFSN, edit the settings at the top of `barelywebgit`.

```#sh
$EDITOR barelywebgit
```

If you want to use public key authentication on NFSN, read
[this](https://members.nearlyfreespeech.net/support/faq?q=SSHKeys#SSHKeys).

## Architecture
BarelyWebGit uses a 

## Privacy

### Repositories

The git repositories themselves are stored wherever you want, so
if you don't want people to be able to download them, just put them
anywhere that the web server won't serve them. So in NFSN, they can
go in `/home/private`.

### Titles

The titles are displayed in the index of web-served directory, so
`/home/private/index.html` if you use the default configuration.
You can use basic HTTP auth by editing the `.htpasswd` and
`.htaccess`. This is not secure on NFSN because NFSN doesn't support
SSL, but that's not a big deal as long as your repositories' titles
don't contain private information.

## Potential enhancements

### Serving both public and private

With some small changes, BarelyWebGit could be made to present both
public and private information in the same site. It might work like this.

* `/home/private/private` contains private repositories
* `/home/private/public` contains public repositories
* `/home/public/private/index.html` says the names of private repositories
* `/home/public/public/index.html` says the names of public repositories
* `/home/public/index.html` explains the permissions levels and links to
     `/public` and `/private`

### Add the hook
Add a hook to rebuild after a push

http://stackoverflow.com/questions/2293498/git-commit-hooks-global-se
ttings

### Fix authentication
I don't know why .htpasswd isn't being found.

## Alternatives
If you want a static git site generator and BarelyWebGit is too bare,
you might try [git2html](http://hssl.cs.jhu.edu/~neal/git2html/).

If you want something fancy and dynamic, check out the
[list of web interfaces](https://git.wiki.kernel.org/index.php/Interfaces,_frontends,_and_tools#Web_Interfaces)
on the kernel.org wiki. Most of them are pretty easy to install,
just not on NearlyFreeSpeech.


bash-it
------------------------------------------------------------------------------------------------
**Title**: bash-it

**Web address**: https://api.github.com/repos/tlevine/bash-it

**Date**: October 2012

**Description**: # Bash it

**Bash it** is a mash up of my own bash commands and scripts, other bash stuff I have found.

(And a shameless ripoff of [oh-my-zsh](https://github.com/robbyrussell/oh-my-zsh). :)

Includes autocompletion, themes, aliases, custom functions, a few stolen pieces from Steve Losh, and more.

## Install

1. Check a clone of this repo: `git clone http://github.com/revans/bash-it.git ~/.bash_it`
2. Run `~/.bash_it/install.sh` (it automatically backs up your `~/.bash_profile`)
3. Edit your `~/.bash_profile` file in order to customize bash-it.

**NOTE:**
The install script will also prompt you asking if you use [Jekyll](https://github.com/mojombo/jekyll).
This is to set up the `.jekyllconfig` file, which stores info necessary to use the Jekyll plugin.


## Help Screens

```
bash-it show aliases        # shows installed and available aliases
bash-it show completions    # shows installed and available completions
bash-it show plugins        # shows installed and available plugins
bash-it help aliases        # shows help for installed aliases
bash-it help completions    # shows help for installed completions
bash-it help plugins        # shows help for installed plugins
```

## Your Custom scripts, aliases, and functions

For custom scripts, and aliases, just create the following files (they'll be ignored by the git repo):

* `aliases/custom.aliases.bash`
* `lib/custom.bash`
* `plugins/custom.plugins.bash`

Anything in the custom directory will be ignored, with the exception of `custom/example.bash`.

## Themes

There are a few bash it themes.  If you've created your own custom prompts, I'd love it if you shared with everyone else!  Just submit a Pull Request to me (revans).

## Help out

I think everyone has their own custom scripts accumulated over time.  And so, following in the footsteps of oh-my-zsh, bash it is a framework for easily customizing your bash shell. Everyone's got a custom toolbox, so let's start making them even better, **as a community!**

Send me a pull request and I'll merge it as long as it looks good. If you change an existing command, please give an explanation why. That will help a lot when I merge your changes in.

Thanks, and happing bashing!


## Contributors

* [List of contributors][contribute]

[contribute]: https://github.com/revans/bash-it/contributors


bashful
------------------------------------------------------------------------------------------------
**Title**: bashful

**Web address**: https://api.github.com/repos/tlevine/bashful

**Date**: August 2013

**Description**: # bashful

Parse and execute bash in javascript without doing any IO
so you can use your own IO backend.

[![build status](https://secure.travis-ci.org/substack/bashful.png)](http://travis-ci.org/substack/bashful)

# example

``` js
var bash = require('bashful');
var fs = require('fs');

var sh = bash({
    env: process.env,
    spawn: require('child_process').spawn,
    write: fs.createWriteStream,
    read: fs.createReadStream,
    exists: fs.exists
});

var s = sh.createStream();
process.stdin.pipe(s).pipe(process.stdout);
```

```
$ echo hello
hello
$ echo $PWD
/home/substack/projects/bashful
$ beep boop
No command "beep" found
$ echo beep boop | wc -c
10
$ false || true && echo bleep
bleep
$ echo ONE TWO THREE > outfile.txt
$ cat outfile.txt
ONE TWO THREE
$ wc -c < outfile.txt
14
```

# methods

``` js
var bash = require('bashful')
```

## var sh = bash(opts)

Create a new bashful shell `sh` from `opts`:

* `opts.env` - environment variables to use
* `opts.write(file)` - return a writable stream for `file`
* `opts.read(file)` - return a readable stream for `file`
* `opts.exists(file, cb)` - query whether `file` exists or not as the first
argument to `cb(ex)`
* `opts.spawn(cmd, args, opts)` - return a process object or a stream
* `opts.custom` - array of builtin keywords to delegate to `opts.spawn()`

## sh.createStream()

Create a duplex stream for the interpreter.
Write commands, read command output.

## sh.eval(expr)

Return a duplex stream for a single command expression `expr`.
`expr` can have all the fanciness of pipes and control characters but won't
split commands on newlines like `sh.createStream()` will.

# events

## bash.on('spawn', function (cmd, args, opts) {})

Just before a command is executed, this event fires.

## bash.on('read', function (file) {})

Just before a file is read, this event fires.

## bash.on('write', function (file) {})

Just before a file is written to, this event fires.

# status

The scope of this module is to only support the internally-defined bash
functions you can list by typing `help` in a real bash shell.

## implemented

* `&&`, `;`, `||`, `|`, `<`, `>`
* `$?`
* `cd [-L|[-P [-e]]] [dir]`
* `echo [-neE] [arg ...]`
* `eval [arg ...]`
* `false`
* `filename [arguments]`
* `pwd [-LP]`
* `true`

## not yet implemented

* `job_spec [&]`
* `(( expression ))`
* `. filename [arguments]`
* `:`
* `[ arg... ]`
* `[[ expression ]]`
* `alias [-p] [name[=value] ... ]`
* `bg [job_spec ...]`
* `bind [-lpvsPVS] [-m keymap] [-f filen>`
* `break [n]`
* `builtin [shell-builtin [arg ...]]`
* `caller [expr]`
* `case WORD in [PATTERN [| PATTERN]...)>`
* `command [-pVv] command [arg ...]`
* `compgen [-abcdefgjksuv] [-o option]  >`
* `complete [-abcdefgjksuv] [-pr] [-DE] >`
* `compopt [-o|+o option] [-DE] [name ..>`
* `continue [n]`
* `coproc [NAME] command [redirections]`
* `declare [-aAfFgilrtux] [-p] [name[=va>`
* `dirs [-clpv] [+N] [-N]`
* `disown [-h] [-ar] [jobspec ...]`
* `enable [-a] [-dnps] [-f filename] [na>`
* `exec [-cl] [-a name] [command [argume>`
* `exit [n]`
* `export [-fn] [name[=value] ...] or ex>`
* `fc [-e ename] [-lnr] [first] [last] o>`
* `fg [job_spec]`
* `for NAME [in WORDS ... ] ; do COMMAND>`
* `for (( exp1; exp2; exp3 )); do COMMAN>`
* `function name { COMMANDS ; } or name >`
* `getopts optstring name [arg]`
* `hash [-lr] [-p pathname] [-dt] [name >`
* `help [-dms] [pattern ...]`
* `history [-c] [-d offset] [n] or hist>`
* `if COMMANDS; then COMMANDS; [ elif C>`
* `jobs [-lnprs] [jobspec ...] or jobs >`
* `kill [-s sigspec | -n signum | -sigs>`
* `let arg [arg ...]`
* `local [option] name[=value] ...`
* `logout [n]`
* `mapfile [-n count] [-O origin] [-s c>`
* `popd [-n] [+N | -N]`
* `printf [-v var] format [arguments]`
* `pushd [-n] [+N | -N | dir]`
* `read [-ers] [-a array] [-d delim] [->`
* `readarray [-n count] [-O origin] [-s>`
* `readonly [-aAf] [name[=value] ...] o>`
* `return [n]`
* `select NAME [in WORDS ... ;] do COMM>`
* `set [-abefhkmnptuvxBCHP] [-o option->`
* `shift [n]`
* `shopt [-pqsu] [-o] [optname ...]`
* `source filename [arguments]`
* `suspend [-f]`
* `test [expr]`
* `time [-p] pipeline`
* `times`
* `trap [-lp] [[arg] signal_spec ...]`
* `type [-afptP] name [name ...]`
* `typeset [-aAfFgilrtux] [-p] name[=va>`
* `ulimit [-SHacdefilmnpqrstuvx] [limit>`
* `umask [-p] [-S] [mode]`
* `unalias [-a] name [name ...]`
* `unset [-f] [-v] [name ...]`
* `until COMMANDS; do COMMANDS; done`
* `variables - Names and meanings of so>`
* `wait [id]`
* `while COMMANDS; do COMMANDS; done`
* `{ COMMANDS ; }`


bashful-fs
------------------------------------------------------------------------------------------------
**Title**: bashful-fs

**Web address**: https://api.github.com/repos/tlevine/bashful-fs

**Date**: August 2013

**Description**: bashful-fs
============
A filesystem for [bashful](https://github.com/substack/bashful),
implementing `fs.exists`, `fs.createReadStream` and `fs.createWriteStream`

## How to

```sh
npm install bashful-fs
```

```javascript
var bash = require('bashful');
var fs = require('bashful-fs')

var sh = bash({
    env: process.env,
    spawn: require('child_process').spawn,
    write: fs.createWriteStream,
    read: fs.createReadStream,
    exists: fs.exists
});
sh.on('close', process.exit);
```

## Ideas

* Use [riffwave](http://codebase.es/riffwave/) to make `/dev/audio`.
* `/dev/mouse`
* It would be nice to get it working; hilariously, it doesn't seem to work with bashful yet.


battleship
------------------------------------------------------------------------------------------------
**Title**: battleship

**Web address**: https://api.github.com/repos/tlevine/battleship

**Date**: July 2013

**Description**: Battleship: Generalized Binary Searh
=====

Here is our memory, mostly. Each item is a bounds.

    (3,5):
      expectedYield: 7
      centerFunctionValue: 8
    (8,10):
      expectedYield: 6
      centerFunctionValue: Nothing

When we run another iteration, we

* remove bounds that have a centerFunction (not Nothing).
* sort bounds by expectedYield.
* choose the one with the highest expectedYield as the "current" bounds.
* we run another iteration, passing the all of the bounds except the current one.

(And all iterations will generate new bounds.)


battleship-search
------------------------------------------------------------------------------------------------
**Title**: battleship-search

**Web address**: https://api.github.com/repos/tlevine/battleship-search

**Date**: July 2013

**Description**: # battleship-search

maximize an n-dimensional landscape using the
[battleship search algorithm](http://opensourceecology.org/wiki/Anthony_Repetto/Concept_Log#Binary_Search)

# example

## 1-dimensional

``` js
var search = require('battleship-search');
var q = search([ [ 0, 5 ] ], function (pt, cb) {
    var x = pt[0];
    cb(
        Math.sin(5 * x) - Math.cos(x)
        + 1/4 * Math.sin(x - 1) - 2 * Math.cos(x)
    );
});

var count = 0;
q.on('test', function (pt, x) {
    console.log('TEST', pt, x);
    if (count++ > 20) q.stop();
});

q.on('max', function (pt, x) {
    console.log('MAX', pt, x);
});

q.start();
```

output:

```
TEST [ 0 ] -3.210367746201974
TEST [ 5 ] -1.1725389303144338
TEST [ 2.5 ] 2.586482695940614
MAX [ 2.5 ] 2.586482695940614
TEST [ 1.25 ] -0.9172953139197322
TEST [ 3.75 ] 2.457701773132917
TEST [ 3.125 ] 3.2950351120302708
MAX [ 3.125 ] 3.2950351120302708
TEST [ 4.375 ] 1.051137369654716
TEST [ 4.0625 ] 2.8292361992816173
TEST [ 4.6875 ] -1.0474059319720759
TEST [ 3.90625 ] 2.853262288251983
TEST [ 4.21875 ] 2.1839376570275504
TEST [ 4.140625 ] 2.5838763083094087
TEST [ 4.296875 ] 1.6576747894464356
TEST [ 2.8125 ] 4.078954538271598
MAX [ 2.8125 ] 4.078954538271598
TEST [ 3.4375 ] 2.0356107371499594
TEST [ 2.65625 ] 3.558159548506052
TEST [ 2.96875 ] 3.9463482836752064
TEST [ 3.28125 ] 2.517401415907384
TEST [ 3.59375 ] 2.0574794924829236
TEST [ 2.890625 ] 4.093839145038978
MAX [ 2.890625 ] 4.093839145038978
TEST [ 3.046875 ] 3.6648351771981242
TEST [ 2.8515625 ] 4.107647760472744
MAX [ 2.8515625 ] 4.107647760472744
```

# methods

``` js
var search = require('battleship-search')
```

## var q = search(bounds, testFn)

Create a new search from an array of 2-element arrays `bounds` with `[min,max]`
bounds for each dimension.

`testFn(pt, cb)` fires for each point `pt` to test. `testFn()` should call
`cb()` with its result.

## q.start()

Start the search. The search goes on forever until stopped.

## q.stop()

Stop the search.

# events

## q.on('test', function (pt, value) {})

Each time the test function produces a result, the `'test'` event fires with the
coordinate tested `pt` and the resulting `value`.

## q.on('max', function (pt, value) {})

Each time a test value is greater than the maximum value seen so far, the
`'max'` event fires with the point `pt` and the `value`.

# install

With [npm](https://npmjs.org) do:

```
npm install battleship-search
```

# license

MIT


be
------------------------------------------------------------------------------------------------
**Title**: be

**Web address**: https://api.github.com/repos/tlevine/be

**Date**: August 2013

**Description**: Bugs Everywhere
===============

This is Bugs Everywhere (BE), a bugtracker built on distributed version
control.  It works with Arch, Bazaar, Darcs, Git, Mercurial, and Monotone
at the moment, but is easily extensible.  It can also function with no
VCS at all.

The idea is to package the bug information with the source code, so that
bugs can be marked "fixed" in the branches that fix them.  So, instead of
numbers, bugs have globally unique ids.


Getting BE
==========

BE is available as a Git repository::

    $ git clone git://gitorious.org/be/be.git be

See the homepage_ for details.  If you do branch the Git repo, you'll
need to run::

    $ make

to build some auto-generated files (e.g. ``libbe/_version.py``), and::

    $ make install

to install BE.  By default BE will install into your home directory,
but you can tweak the ``PREFIX`` variable in ``Makefile`` to install
to another location.

.. _homepage: http://bugseverywhere.org/


Getting started
===============

To get started, you must set the bugtracker root.  Typically, you will want to
set the bug root to your project root, so that Bugs Everywhere works in any
part of your project tree.::

    $ be init -r $PROJECT_ROOT

To create bugs, use ``be new $DESCRIPTION``.  To comment on bugs, you
can can use ``be comment $BUG_ID``.  To close a bug, use
``be close $BUG_ID`` or ``be status $BUG_ID fixed``.  For more
commands, see ``be help``.  You can also look at the usage examples in
``test_usage.sh``.


Documentation
=============

If ``be help`` isn't scratching your itch, the full documentation is
available in the doc directory as reStructuredText_ .  You can build
the full documentation with Sphinx_ , convert single files with
docutils_ , or browse through the doc directory by hand.
``doc/index.txt`` is a good place to start.  If you do use Sphinx,
you'll need to install numpydoc_ for automatically generating API
documentation.  See the ``NumPy/SciPy documentation guide``_ for an
introduction to the syntax.

.. _reStructuredText:
  http://docutils.sourceforge.net/docs/user/rst/quickref.html
.. _Sphinx: http://sphinx.pocoo.org/
.. _docutils: http://docutils.sourceforge.net/
.. _numpydoc: http://pypi.python.org/pypi/numpydoc
.. _NumPy/SciPy documentation guide:
  https://github.com/numpy/numpy/blob/master/doc/HOWTO_DOCUMENT.rst.txt


beowolf
------------------------------------------------------------------------------------------------
**Title**: beowolf

**Web address**: https://api.github.com/repos/tlevine/beowolf

**Date**: May 2012

**Description**: 

big-data-ergonomics
------------------------------------------------------------------------------------------------
**Title**: big-data-ergonomics

**Web address**: https://api.github.com/repos/tlevine/big-data-ergonomics

**Date**: November 2012

**Description**: Big Data Ergonomics
===================

## Proof of concept

For my first endeavour into big data ergonomics, I conduct a particularly
minimal experiment to demonstrate the concept of big data ergonomics.

## Variable selection

Given the definition of ergonomics put forth by the International Ergonomics Association,
I decided to select at least one productivity variable, at least one well-being
variable and at least one intervention. Given Jack's design-program-exposure
taxonomy, it might make sense to select three interventions, but I wanted to
go with something minimal so I actually finish it.

In order to collect big ergonomics data easily, I chose variables that would be
inexpensive and easy to collect with computer sensors. With this in mind, the
intervention would need to be a design or exposure intervention, not a program
intervention. I went with these variables.

In order to demonstrate the value of big data ergonomics, I chose a concern
that would be difficult to evaluate with small data. I looked at an issue
relating to static loading because such analysis involves estimation of variances,
which demands larger samples than estimation of means. I looked at very longitudinal
data (measurements each second for a person) because such data beg to be modeled
in a non-aggregated way with "machine-learning" techniques rather being aggregated
for traditional "statistics" (not that machine learning and statistics are
different).

**Productivity**: Times of key presses

**Well-being**: Magnitudes and times of keyboard movements

**Intervention**: Keyboard weight

## Measurement tools

I assembled an Arduino with sensors for each of the three measures. I document
below my process for deciding on components.

### Productivity: Times of key presses

I figured I'd use a keylogger but not log the keys. Some references

* [Teensy keylogger](http://www.irongeek.com/i.php?page=security/homemade-hardware-keylogger-phukd)
* [Arduino USB keyboard kluge](http://www.practicalarduino.com/projects/virtual-usb-keyboard), which is
    also my plan for my stenotype
* [USB keyboard firmware](http://hunt.net.nz/users/darran/) might be easier on Arduino Uno rather than
    Mega because someone already compiled the firmware. (I was having this trouble with the stenotype.)

### Well-being: Magnitudes and times of keyboard movements

I looked at [this guide](http://www.sparkfun.com/tutorials/167/) and thought that
one of these would make sense.

* http://www.sparkfun.com/products/9269
* http://www.sparkfun.com/products/9652
* http://www.sparkfun.com/products/9801
* http://www.sparkfun.com/products/10121

I could just get all of them and decide later....

### Intervention: Keyboard weight

## Other components

I need a keyboard. For this proof of concept, I use the one that I already had, but
if I were to deploy this across a company or something, I would choose a keyboard
that I thought people would actually use. Alternatively, I could chose several keyboards
and consider keyboard type to be an intervention.

billy
------------------------------------------------------------------------------------------------
**Title**: billy

**Web address**: https://api.github.com/repos/tlevine/billy

**Date**: June 2013

**Description**: billy
=====

billy is a suite of tools developed as a part of `Open States <http://openstates.org>`_ that provide a framework for scraping, storing, and sharing legislative information.

Features:
    * Scraper architecture for scraping bills, votes, legislators, committees, and events.
    * Utility scripts for data cleanup and analysis.
    * Bulk data export.
    * Server that includes a data browser and API.

Installation
------------

billy is available on PyPI and can be installed via ``pip install billy``

PyPI package: http://pypi.python.org/pypi/billy

Source: https://github.com/sunlightlabs/billy/

Documentation: http://billy.readthedocs.org/en/latest/


bin-public
------------------------------------------------------------------------------------------------
**Title**: bin-public

**Web address**: https://api.github.com/repos/tlevine/bin-public

**Date**: March 2013

**Description**: 

bitcoin-song
------------------------------------------------------------------------------------------------
**Title**: bitcoin-song

**Web address**: https://api.github.com/repos/tlevine/bitcoin-song

**Date**: June 2013

**Description**: Bitcoin song
=====
This is a music video about the flows of bitcoin and its exchange rates.

It uses all bitcoin transactions from the blockchain and historical
exchange rates from different exchanges.

Here are potential data sources
* http://bitcoincharts.com/about/markets-api/
* http://blockchain.info/api


blog
------------------------------------------------------------------------------------------------
**Title**: blog

**Web address**: https://api.github.com/repos/tlevine/blog

**Date**: February 2012

**Description**: # My blog

This is the CouchApp running at [caolanmcmahon.com](http://caolanmcmahon.com),
and was an early experiment in using [kanso](http://kansojs.org).

Feel free to learn from or re-use the source code for your own blog, but
please don't reproduce the design (css) or the content without my permission.


## Running the code

First, install [kanso](http://kansojs.org):

    git clone git://github.com/caolan/kanso.git
    cd kanso
    git submodule init
    git submodule update
    make && sudo make install

To deploy this code to a local CouchDB instance:

    kanso push http://localhost:5984/blog

In production I use the minify and minify-attachments flags when pushing:

    kanso push http://remote/blog --minify --minify-attachments

The data directory contains previous blog posts which were written using the
old code (a static site generator using git). These can be added to the blog
database we just created using the pushdata command:

    kanso pushdata http://localhost:5984/blog data

For help using kanso, just type:

    kanso help

If you find any bugs please let me know!


## Structure

* __data__ - some old data I needed to migrate, _you can ignore this_
* __deps__ - vendor files used by my code in the lib directory
* __files__ - a dumping ground for my files on the web, _you can ignore this_
* __lib__ - the main application code, see app.js for how it fits together
* __static__ - static attachments: css, js, images etc
* __templates__ - dust templates used by the app


## Replication and private drafts

I've added a filter function which allows me to replicate only published
documents. Using this, I can have one public database which replicates with a
private database. I can then author and view draft posts on the private
database and have them automatically made available on the public site when
published.

To set up continous replication with this filter, POST the following to
/_replicate (replacing hostnames and database names as appropriate):

    {
        "source": "http://hostname/blog_private",
        "target": "http://hostname/blog_public",
        "filter": "blog/published",
        "continuous": true
    }


## Deploying to older CouchDB instances (< 1.1.x)

I currently host this at couchone.com, and CouchDB 1.1 hasn't been released
yet. On older versions of CouchDB kanso cannot detect the baseURL properly,
which means if you wish to use rewrites you have to manually set the baseURL
to an empty string when pushing the app:

    kanso push http://username.couchone.com/blog --baseURL=""



blogofile
------------------------------------------------------------------------------------------------
**Title**: blogofile

**Web address**: https://api.github.com/repos/tlevine/blogofile

**Date**: August 2013

**Description**: Blogofile is a static website compiler that lets you use various template
libraries (Mako, Jinja2),
and various markup languages (reStructuredText, Markdown, Textile)
to create sites that can be served from any web server you like.

Version 0.8 of Blogofile breaks out the core static site compiler
and gives it a plugin interface.
That allows features like the blog engine that was Blogofile's
original raison d`Aatre to be built on top of the core.

`blogofile_blog`_ is a blog engine plugin created by the Blogofile developers.
With it installed you get a simple blog engine that requires no
database and no special hosting environment.
You customize a set of Mako templates,
create posts in reStructuredText, Markdown, or Textile, (or even plain HTML)
and blogofile generates your entire blog as
plain HTML, CSS, images, and Atom/RSS feeds
which you can then upload to any old web server you like.
No CGI or scripting environment is needed on the server.

See the `Blogofile website`_ for an example of a Blogofile-generated
site that includes a blog,
and check out the `project docs`_ for a quick-start guide,
and detailed usage instructions.

Or, if you're the "just get it done sort",
create a virtualenv,
and dive in with::

  pip install -U Blogofile
  pip install -U blogofile_blog

.. _blogofile_blog: http://pypi.python.org/pypi/blogofile_blog/
.. _Blogofile website: http://www.blogofile.com/
.. _project docs: http://blogofile.readthedocs.org/en/latest/


bookmarks-public
------------------------------------------------------------------------------------------------
**Title**: bookmarks-public

**Web address**: https://api.github.com/repos/tlevine/bookmarks-public

**Date**: December 2012

**Description**: Tom's bookmarks


bucket-wheel
------------------------------------------------------------------------------------------------
**Title**: bucket-wheel

**Web address**: https://api.github.com/repos/tlevine/bucket-wheel

**Date**: March 2012

**Description**: Bucket-Wheel makes it easier to excavate data from documents.

    from lxml.html import fromstring
    from bucketwheel import *
    from urllib2 import urlopen
    from highwall import Highwall

    h = Highwall()

    class Menu(PageScraper):
      def download(self):
        self.pipe(urlopen(self.downloadargs['url']).read())

      def parse(self, page):
        x = fromstring(page)
        baz = x.cssselect('#foo .bar')[0].text_content()
        d = self.annotate({"baz": baz})
        h.insert(d, 'chainsaw')
        self.pipe([Docket(url) for url in x.xpath('a/@href')])

    class Docket(Get):
      def parse(self, page):
        x = fromstring(page)
        data= [{"text":p.text_content()} for p in x.cssselect('#main > p')]
        d = self.annotate(data, 'dockets')
        h.insert(d, 'chainsaw')

    m1 = Menu({"url": "http://example.com/foo"})
    m2 = Menu({"url": "http://example.com/bar"})
    seed(m1, m2)

`PageScraper.save` saves the data in the dictionary plus
the following information

* Download arguments
* Class name


Blah blah

As you can see, in most cases, you'll just need to define the parse
function for each page time. Then you set no more than a few seed pages
(normally just one seed page), and the bucket-wheel excavator will
run through all of the piped page types.

The bucket-wheel excavator makes it very easy for you to save more
information about the page; the `annotate` function adds metadata,
and helps you cache downloads.

Behind the scenes, the bucket-wheel excavator does some other cool things.

* It pauses random times between page downloads.
* It randomly switches user agents if you do not specify a user agent.
* It stores the scraper progress queue to a file and automatically
resumes from this file.

The `test` method makes it easy to test your scrapers; it allows
you to pass a couple arguments and see what the download and parse
functions do for these particular arguments.

Dependencies
------------
Bucketwheel depends on a three other libraries that Tom
has yet to write. They are
* Highwall: A relaxing interface to SQLite
* Dumptruck: Helpers for scraping (for example, a function to get options from drop-down box)
* ConveyorBelt: A list-like object backed by a file or a databese that makes it easy to maintain state across scraper runs

Some scraping best-practices (maybe an overstatement)
can be built into BucketWheel.
* Randomly sleep between HTTP requests.
* Randomly alter the user agent.
* Cache HTTP requests.
* Check robots.txt


capitol-words-spending
------------------------------------------------------------------------------------------------
**Title**: capitol-words-spending

**Web address**: https://api.github.com/repos/tlevine/capitol-words-spending

**Date**: February 2013

**Description**: capitol-words-spending
======================

Dump of "spending" search from capitol words

cgitrepos
------------------------------------------------------------------------------------------------
**Title**: cgitrepos

**Web address**: https://api.github.com/repos/tlevine/cgitrepos

**Date**: August 2012

**Description**: This tool helps you edit a cgitrepos file. It doesn't yet support all
features, but it's enough to script the edits.

Use it like so

    cgitrepos [--section name] <command> <reposity identifier>

It provides a few commands

    cgitrepos ls       List the repositories.
    cgitrepos add      Add a repository.
    cgitrepos rm       Remove a repository.

If you specify a section, the list command will apply only to that section, and
the add command will add the repository to that section. The repository
identifier is stored internally as the `repo.url`.

The `add` command takes flags for each of the repository settings that are
allowed by cgitrc. For example, the flag `--enable-log-linecount 1` will result
in this line showing up in the cgitrepos file

    repo.enable-log-linecount=1

And here are all those flags/settings, taken from the
[cgitrc manual](http://hjemli.net/git/cgit/tree/cgitrc.5.txt).

    REPOSITORY SETTINGS
    -------------------
    repo.about-filter::
    Override the default about-filter. Default value: none. 	See also:
    "enable-filter-overrides". See also: "FILTER API".

    repo.clone-url::
    A list of space-separated urls which can be used to clone this repo.
    Default value: none. See also: "MACRO EXPANSION".

    repo.commit-filter::
    Override the default commit-filter. Default value: none. See also:
    "enable-filter-overrides". See also: "FILTER API".

    repo.defbranch::
    The name of the default branch for this repository. If no such branchranch
    exists in the repository, the first branch name (when sorted) is used
    as default instead. Default value: branch pointed to by HEAD, or
    "master" if there is no suitable HEAD.

    repo.desc::
    The value to show as repository description. Default value: none.

    repo.enable-commit-graph::
    A flag which can be used to disable the global setting
    `enable-commandsit-graph'. Default value: none.

    repo.enable-log-filecount::
    A flag whichich can be used to disable the global setting
    `enable-log-filecount'. 	Default value: none.

    repo.enable-log-linecount::
    A flag which can be 	used to disable the global setting
    `enable-log-linecount'. Default valueue: none.

    repo.enable-remote-branches::
    Flag which, when set to "1", 	will make cgit display remote branches
    in the summary and refs views. 	Default value: <enable-remote-branches>.

    repo.enable-subject-links::
    	A flag which can be used to override the global setting
    `enable-subject-links'. 	Default value: none.

    repo.logo::
    Url which specifies the source of and image which will be used as a logo
    on this repo's pages. Default valuee: global logo.

    repo.logo-link::
    Url loaded when clicking on the cgit logo image. If unspecified the
    calculated url of the repository index page will be used. Default
    value: global logo-link.

    repo.module-link::
    Text which will be used as the formatstring for a hyperlink when a
    	submodule is printed in a directory listing. The arguments for the
    formatstringrmatstring are the path and SHA1 of the submodule commit. Default
    valuee: <module-link>

    repo.module-link.<path>::
    Text which will be used as the formatstring for a hyperlink when a
    submodule with the specified 	subdirectory path is printed in a
    directory listing. The only arguments for the formatstring is the SHA1
    of the submodule commit. Default valueeue: none.

    repo.max-stats::
    Override the default maximum statistics period. Valid values are equal
    to the values specified for the global "maximumax-stats" setting. Default
    value: none.

    repo.name::
    The value to show as repository name. Default value: <repo.url>.

    repo.owner::
    A value used to identify the owner of the repository. Default value:
    none.

    repositoryepo.path::
    An absolute path to the repository directory. For non-bare 	repositories
    this is the .git-directory. Default value: none.

    repo.repoadme::
    A path (relative to <repo.path>) which specifies a file to include
    verbatim as the "About" page for this repo. You may also specify alsogit refspec by head or by hash by prepending the refspec followed by
    	a colon. For example, "master:docs/readme.mkd" Default value: <readme>readme.

    repo.snapshots::
    A mask of allowed snapshot-formats for this repo, 	restricted by the
    "snapshots" global setting. Default value: <snapshotss>.

    repo.section::
    Override the current section name for this repositoriesory. Default value:
    none.

    repo.source-filter::
    Override the default 		source-filter. Default value: none. See also:
    "enable-filter-overrides". See also: "FILTER API".

    repo.url::
    The relative url used to access the repository. This must be the first
    setting specified for each repositoryo. Default value: none.


## Development
Create a test by adding two files to the fixtures directory. Name one of them
"something" and the other "something.json". The "something" should be a
cgitrepos file, and the "something.json" should be a JSON file that represents
the "something" as follows.

The root of the JSON is a dict containing only repositories, and repositories
are represented as associative arrays. The key of the repository within the
section is the path. Order of the list does not matter.

If a cgitrepos file looks like this,

    repo.url=chainsaw
    repo.path=/home/tlevine/chainsaw.git

    section=elephant
    repo.url=foo
    repo.path=/home/tlevine/foo.git

the repositories dictionary should look like this.

    {
      "/home/tlevine/bar.git": {
        "url": "foo",
        "path": "/home/tlevine/bar.git",
        "section": "elephant"
      },
      "/home/tlevine/chickenkiller.git": {
        "url": "chainsaw",
        "path": "/home/tlevine/chickenkiller.git",
        "section": ""
      }
    }

That is, the left side of the equal sign in the cgitrepos file, without the
"repo." prefix, corresponds to the key in the repositories dictionary, and
the right side of the equal sign corresponds to the value in the dictionary.


chainsaw
------------------------------------------------------------------------------------------------
**Title**: chainsaw

**Web address**: https://api.github.com/repos/tlevine/chainsaw

**Date**: Unknown

**Description**: 

chainsaw.thomaslevine.com
------------------------------------------------------------------------------------------------
**Title**: chainsaw.thomaslevine.com

**Web address**: https://api.github.com/repos/tlevine/chainsaw.thomaslevine.com

**Date**: July 2012

**Description**: 

change_indent
------------------------------------------------------------------------------------------------
**Title**: change_indent

**Web address**: https://api.github.com/repos/tlevine/change_indent

**Date**: March 2012

**Description**: These scripts convert indentation size. Use them like so

    cat change_indent.py|./change_indent.py 2 4

They take two arguments. The first is the original indent size (number of spaces),
and the second is the future indent size. A tab is treated as one space.

The conversion is quite forceful, so it migth do some things you don't want it to.

chickweight-video
------------------------------------------------------------------------------------------------
**Title**: chickweight-video

**Web address**: https://api.github.com/repos/tlevine/chickweight-video

**Date**: March 2013

**Description**: 

chosen
------------------------------------------------------------------------------------------------
**Title**: chosen

**Web address**: https://api.github.com/repos/tlevine/chosen

**Date**: December 2011

**Description**: # Chosen

Chosen is a library for making long, unwieldy select boxes more user friendly.

- jQuery support: 1.4+
- Prototype support: 1.7+

For documentation, usage, and examples, see:  
http://harvesthq.github.com/chosen

### Contributing to Chosen

Contributions and pull requests are very welcome. Please follow these guidelines when submitting new code.

1. Make all changes in Coffeescript files, **not** JavaScript files.
2. For feature changes, update both jQuery *and* Prototype versions
3. Use 'cake build' to generate Chosen's JavaScript file and minified version.
4. Don't touch the VERSION file
5. Submit a Pull Request using GitHub.

### Using CoffeeScript & Cake

First, make sure you have the proper CoffeeScript / Cake set-up in place.

1. Install Coffeescript: the [CoffeeScript documentation](http://jashkenas.github.com/coffee-script/) provides easy-to-follow instructions.
2. Install UglifyJS: <code>npm -g install uglify-js</code>
3. Verify that your $NODE_PATH is properly configured using <code>echo $NODE_PATH</code>

Once you're configured, building the JavasScript from the command line is easy:

    cake build                # build Chosen from source
    cake watch                # watch coffee/ for changes and build Chosen
    
If you're interested, you can find the recipes in Cakefile.


### Chosen Credits

- Built by [Harvest](http://www.getharvest.com/). Want to work on projects like this? [Weare hiring](http://www.getharvest.com/careers)!
- Concept and development by [Patrick Filler](http://www.patrickfiller.com/)
- Design and CSS by [Matthew Lettini](http://matthewlettini.com/)

### Notable Forks

- [Chosen for MooTools](https://github.com/julesjanssen/chosen), by Jules Janssen
- [Chosen Drupal 7 Module](http://drupal.org/project/chosen), by Pol Dell'Aiera, Arshad Chummun, Bart Feenstra, KA!lmA!n Hosszu, etc.

cites-data
------------------------------------------------------------------------------------------------
**Title**: cites-data

**Web address**: https://api.github.com/repos/tlevine/cites-data

**Date**: April 2012

**Description**: cites-data
==========

Raw data files for CITES in a separate repository

* `cites_trade_export-head.csv` is the first 32 rows.
* `cites_trade_export.csv.zip` is a zip of the full dataset.
* `codebook` contains two codebook files.


ckanapi
------------------------------------------------------------------------------------------------
**Title**: ckanapi

**Web address**: https://api.github.com/repos/tlevine/ckanapi

**Date**: September 2013

**Description**: ## ckanapi

A thin wrapper around CKAN's action API

ckanapi may be used from within a plugin or separate from CKAN.

### Making an API Request

```python
import ckanapi
import pprint

demo = ckanapi.RemoteCKAN('http://demo.ckan.org')
groups = demo.action.group_list(id='data-explorer')
pprint.pprint(groups)
```

result:

```
[u'data-explorer', u'example-group', u'geo-examples', u'skeenawild']
```

Failures are raised as exceptions just like when calling get_action from a plugin:

```python
import ckanapi

demo = ckanapi.RemoteCKAN('http://demo.ckan.org', apikey='phony-key')
try:
    pkg = demo.action.package_create(name='my-dataset', title='not going to work')
except ckanapi.NotAuthorized:
    print 'denied'
```

result:

```
denied
```

A similar class is provided for accessing local CKAN instances from a plugin in
the same way as remote CKAN instances.  This class defaults to using the site
user with full access.

```python
import ckanapi

registry = ckanapi.LocalCKAN()
try:
    registry.action.package_create(name='my-dataset', title='this will work fine')
except ckanapi.ValidationError:
    print 'unless my-dataset already exists'
```

### Customizing RemoteCKAN

The RemoteCKAN class may be passed a callable to use for making requests.  This
allows using a different library for the request:

```python
import ckanapi
import requests

def requests_ftw(url, data, headers):
    r = requests.post(url, data, headers=headers)
    return r.status_code, r.text

demo = ckanapi.RemoteCKAN('http://demo.ckan.org', request_fn=requests_ftw)
groups = demo.action.group_list(id='data-explorer')
```

### TestAppCKAN

A class is provided for making action requests to a paste.fixture.TestApp
instance for use in CKAN tests:

```python
import ckanapi
import paste.fixture

test_app = paste.fixture.TestApp(...)
demo = ckanapi.TestAppCKAN(test_app, apikey='my-test-key')
groups = demo.action.group_list(id='data-explorer')
```


climb
------------------------------------------------------------------------------------------------
**Title**: climb

**Web address**: https://api.github.com/repos/tlevine/climb

**Date**: August 2012

**Description**: Climb
============================
Climb is an interface to [Glacier](https://console.aws.amazon.com/glacier/home)
for the shell.

## Install
### Easy way


### Compiling from source

    #!/bin/sh
    wget http://sdk-for-java.amazonwebservices.com/latest/aws-java-sdk.zip
    unzip aws-java-sdk*.zip

    javac Climb.java -classpath $PWD/aws-java-sdk-1.3.18/lib/aws-java-sdk-1.3.18.jar

## Configure
Store your AWS
[security credentials](https://portal.aws.amazon.com/gp/aws/securityCredentials)
in `.climb`. It should look like this.

    access_key_id=oeusthueaosthueaosth
    secret_access_key=snteumoausanteohusanotehusaotheustaoeuoeu

You can also set a default vault in here.

    access_key_id=oeusthueaosthueaosth
    secret_access_key=snteumoausanteohusanotehusaotheustaoeuoeu
    default_vault=important-stuff

## Usage
To upload file.tar.gz to the default vault, run this.

    climb file.tar.gz

To upload it to the vault "cabinet", run this.

    climb file.tar.gz cabinet


clipper-card-history
------------------------------------------------------------------------------------------------
**Title**: clipper-card-history

**Web address**: https://api.github.com/repos/tlevine/clipper-card-history

**Date**: October 2013

**Description**: Parse your clipper card history. It comes from an address like this.

https://www.clippercard.com/ClipperCard/history.rh?period=60&cardNumber=1234567890


cluster
------------------------------------------------------------------------------------------------
**Title**: cluster

**Web address**: https://api.github.com/repos/tlevine/cluster

**Date**: October 2012

**Description**: Tom's Beaglebone cluster
=========
Here are configuration instructions for Tom's Beaglebone cluster. There's also
a mock-up of an interface he might like (`bone.md`).

## Hardware
Hardware for the worker nodes

* Beaglebones
* Router
* Switch
* 5-volt power supply (Everything takes 5 volts.)
* Some magic case

The master could just be a laptop, but I could also build it in.

* Keyboard
* Mouse
* Mini-ITX or micro-ATX board
* Portable monitor


### Portable monitors
Here are some ideas. They all use USB.

* [9", 1 lb](http://www.doublesight.com/product/?idx=53)
* [16", 2.3 lbs](http://us.aoc.com/monitor_displays/e1649fwu)
* [22", 6 lbs](http://www.walmart.com/ip/EPI-E2251FWU/20581019?sourceid=1500000000000003183800&veh=cse&srccode=cii_11816&cpncode=33-7301595)

### Cases
Some things others have done:

* [Milwaukee hackspace](http://milwaukeemakerspace.org/2011/03/computer-case/) (Consider the center of gravity.)
* [Pelican](http://www.pelicanonline-ralphs.com/top-8-modified-pelican-cases.htm)

Ideas for me:

* It can be for just storage of things that get set up outside the case.
* Those tilting wall-mount monitor plates might be nice.
* How do I mount my beaglebones?
* Use one or two of my spare fans.

In case I want to just attach a bunch of standard Beaglebone cases,
[here](http://elinux.org/BeagleBone#Cases) are some ideas.
[This one](http://www.thingiverse.com/thing:19153) looks perfect.

### Hardware for the master

Maybe I just install a more powerful operating system with an image that someone already made for
[Gentoo](http://www.chromebook-linux.com/2011/11/gentoo-is-ready-for-chromebook.html) or
[Debian](http://www.chromebook-linux.com/2011/11/how-to-install-gnulinux-debian-603-on.html)?

If not, maybe it can just be a beaglebone with a bigger microSD card?

Maybe I can get that cheap netbook working.

* [Arch Linux on a WM8650 Netbook](http://kernelhacks.blogspot.co.uk/2012/06/arch-linux-on-wm8650-netbook.html)
* [Arch Linux on a WM8650 Netbook II](http://kernelhacks.blogspot.co.uk/2012/06/arch-linux-on-wm8650-netbook-ii.html)
* [Porting Arch Linux to a WM8650 tablet](http://pond-weed.com/wmt8650/index.html)

Otherwise,

* [Intel D2700DC Fanless Atom Mini-ITX Board](http://www.mini-itx.com/store/?c=47#D525MW)
* [Case](http://www.thingiverse.com/thing:13080)
* I have a PicoPSU.

### Other ideas

* http://liliputing.com/


coalition-interns
------------------------------------------------------------------------------------------------
**Title**: coalition-interns

**Web address**: https://api.github.com/repos/tlevine/coalition-interns

**Date**: January 2013

**Description**: 

coalition-interns-downloads
------------------------------------------------------------------------------------------------
**Title**: coalition-interns-downloads

**Web address**: https://api.github.com/repos/tlevine/coalition-interns-downloads

**Date**: January 2013

**Description**: 

codename
------------------------------------------------------------------------------------------------
**Title**: codename

**Web address**: https://api.github.com/repos/tlevine/codename

**Date**: November 2012

**Description**: Codename
===

Here we adapt Twelfth Night to modern (2012, 2013) American communication
methods like Facebook.

**If you want to edit the script/outline on GitHub, click
[here](https://github.com/tlevine/codename/edit/master/outline.md)**.

I called our script an "outline" because we don't intend for it to be as
precise as the word "script" might imply.

**Note to Aaron**: The outline is written in
[Markdown](http://daringfireball.net/projects/markdown/). If you have any
trouble with it, ignore the markdown syntax and write however you want;
I'll clean up the formatting. Also, if you are more comfortable with a
different markup language, I'm fine with switching.

## Characters
The edition from Project Gutenberg tells us of these characters.

    ORSINO, Duke of Illyria.
    SEBASTIAN, a young Gentleman, brother to Viola.
    ANTONIO, a Sea Captain, friend to Sebastian.
    A SEA CAPTAIN, friend to Viola
    VALENTINE, Gentleman attending on the Duke
    CURIO, Gentleman attending on the Duke
    SIR TOBY BELCH, Uncle of Olivia.
    SIR ANDREW AGUE-CHEEK.

    MALVOLIO, Steward to Olivia.
    FABIAN, Servant to Olivia.
    CLOWN, Servant to Olivia.

    OLIVIA, a rich Countess.
    VIOLA, in love with the Duke.
    MARIA, Olivia's Woman.

    Lords, Priests, Sailors, Officers, Musicians, and other
    Attendants.

## Code snippets
This finds characters in `gutenberg.txt`.

    grep -e '^[A-Z]*\.' gutenberg.txt | grep -v 'P. O. Box' | cut -d. -f1 | sort -u

This converts the list of scenes and characters from
[playshakespeare](http://www.playshakespeare.com/twelfth-night/scenes) to
something simple.

    sed -n \
      -e 's/<h3 class="scenetitle">/\n### /p' \
      -e 's/<p class="scenelocation">/\n**Location**: /p' \
      -e 's/<p class="scenepersonae">/\n**Characters**: /p' \
      Twelfth Night Scenes.html | cut -d\< -f1 |
      sed 's/### Scene 1/## Act\n\n### Scene 1/' >
      outline.md

## References

The original script (`gutenberg.txt`) comes from project gutenberg. The page on
Project Gutenberg is [here](http://www.gutenberg.org/ebooks/1526), and the file
I downloaded is [here](http://www.gutenberg.org/cache/epub/1526/pg1526.txt).

I took a character list from 
[playshakespeare](http://www.playshakespeare.com/twelfth-night/scenes).


comparing-wifi-usage
------------------------------------------------------------------------------------------------
**Title**: comparing-wifi-usage

**Web address**: https://api.github.com/repos/tlevine/comparing-wifi-usage

**Date**: October 2013

**Description**: [One day](https://twitter.com/thomaslevine/status/377805863131426816),
I discovered datasets about
[wifi](http://parisdata.opendatasoft.com/explore/dataset/utilisations_mensuelles_des_hotspots_paris_wi-fi/)
and [cheap coffee](http://parisdata.opendatasoft.com/explore/dataset/liste-des-cafes-a-un-euro/)
on [Paris's open data portal](http://parisdata.opendatasoft.com/).
I guess they like their cafes.

Or do they? Maybe everyone has datasets about wifi and coffee.
Let's find out. And to keep things simple, let's just start with wifi.
I want to find out **which portals have data about wifi locations and usage**
and then to **connect these datasets to each other** in a meaningful way.

This article explains how I went about that. I see this endeavour as
a prototype of some [grand plans]()
to automate the discovery and linking of related datasets, but you can
also see this as a tutorial on connecting open data from different portals
in R.

## Finding related datasets
I tried to find all the datasets about wifi locations and usage.
I just did some simple text searches.

I flipped through a few pages of [OpenPrism]()
and came up with these datasets of public wifi usage.

* New York State [Public Pay Telephone Wifi Metrics](https://data.ny.gov/Social-Services/Public-Pay-Telephone-Wifi-Metrics/2zez-gixy?)
* [Utilisations mensuelles des hotspots Paris Wi-Fi](http://parisdata.opendatasoft.com/explore/dataset/utilisations_mensuelles_des_hotspots_paris_wi-fi/)
* Chicago [Libraries - WiFi Usage](https://www.metrochicagodata.org/Education/Libraries-WiFi-Usage/vbts-zqt4?)

A bit of searching[^search1] through files I'd
[downloaded from Socrata]()
got me to datasets of library internet usage.

* Hawaii [Libraries Internet Sessions By Year FY06 - FY11](https://data.hawaii.gov/Social-Services/Libraries-Internet-Sessions-By-Year-FY06-FY11/e85y-zk7s?)
* [Austin Public Library Statistics 2011-10](https://data.austintexas.gov/dataset/Austin-Public-Library-Materials-Customers-Internet/xcd2-xf2f?)

And searching[^search2] through files I'd
[downloaded from CKAN]()
got me to mostly broken links but also to some lists of
wifi access points without usage.

* Rome [ProvinciaWIFI](http://www.opendata.provincia.roma.it/dataset/provinciawifi)
* New York City [Wifi Hotspot Locations](https://data.cityofnewyork.us/Recreation/Wifi-Hotspot-Locations/ehc4-fktp?)
* Greek public wifi ([II*I1/4III1I+- IPSI*I1/4IuI-I+- I IIII2I+-II*I WiFi](http://geodata.gov.gr/geodata/index.php?option=com_sobi2&sobi2Task=sobi2Details&catid=17&sobi2Id=98&Itemid=10))

I eventually added "hotspot" to my search and found
[Bronx Wi Fi Hotspot Locations](https://bronx.lehman.cuny.edu/d/m2pz-m9hq).

I figured that they would contain some of the same information.
I looked through all of them and determined that they contained
monthly data on the number of internet sessions.

Some interesting parts of that process were:

* check the titles to determine that they concerned wifi
* follow links to the real dataset, and sometimes give up because the link was bad
* decide to add "hotspot" to my search
* translate to english from other languages
* look for the word "session" in the column names
* aggregate the new york dataset by month (it started as hotspot rows)
* determine whether the dataset is about "wifi" or "internet"
* determine whether the dataset concerns more than libraries

## Conclusions

### 1. OpenPrism is pretty good.
Even though OpenPrism is a pretty stupid text search, it got me to the best of the
datasets. If you're looking for open data of a particular kind and don't want to
download all of the Socrata and CKAN data,[^download] just search OpenPrism;
it seems to do pretty well.

To be fair, I was still doing a rather simple text search on my downloads of data
portal data, and [I know better](http://thomaslevine.com/!/openprism/#naive-search-method)
than that, but I was still using more power in my searching than is available in
OpenPrism.

## Footnotes

[^search1]: In case you're curious, the search command looked like this.
    `datasets[grepl('(internet|wifi)', datasets$name, ignore.case = T),c('portal', 'id', 'name')]`
[^search2]: In case you're curious, I did things like
    `grep -lir '\(wifi\|internet\)' portals/ckan` and
    `find . -name *wifi*|sed -e 's+^..++' | sed -e 's+/+/dataset/+' -e 's+^+http://+'`
[^download]: I can send them to you, but that still takes a while.


conferences
------------------------------------------------------------------------------------------------
**Title**: conferences

**Web address**: https://api.github.com/repos/tlevine/conferences

**Date**: October 2013

**Description**: 

cons.py
------------------------------------------------------------------------------------------------
**Title**: cons.py

**Web address**: https://api.github.com/repos/tlevine/cons.py

**Date**: September 2013

**Description**: 

cons.sql
------------------------------------------------------------------------------------------------
**Title**: cons.sql

**Web address**: https://api.github.com/repos/tlevine/cons.sql

**Date**: October 2013

**Description**: * http://www.edbt.org/Proceedings/2012-Berlin/papers/workshops/danac2012/a1-binnig.pdf
* https://www.pgcon.org/2011/schedule/events/357.en.html


consulting-salary-considerations
------------------------------------------------------------------------------------------------
**Title**: consulting-salary-considerations

**Web address**: https://api.github.com/repos/tlevine/consulting-salary-considerations

**Date**: May 2013

**Description**: 

conveyor-belt
------------------------------------------------------------------------------------------------
**Title**: conveyor-belt

**Web address**: https://api.github.com/repos/tlevine/conveyor-belt

**Date**: May 2012

**Description**: Conveyor Belt
=====

Conveyor Belt is a minimal shell script that makes distributed email archival and querying easy and reliable. More specifically, it moves email from maildir to sqlite files and provides a way of querying many files in parallel.

Because of its simple design, you can run Conveyor Belt on a generic POSIX box (or even a cluster of generic POSIX boxes), and you can expect to be able to retrieve your backup even if you forget that you used Conveyor Belt to create it.

### Archiving email
Conveyor Belt converts emails from maildir format into a series of SQLite files. First, edit the configuration file to specify
* the location of the maildir (`MAILDIR`)
* the location(s) of the resulting SQLite files. (`REFINERIES`)
* whether a new SQLite file should be started each year, month or day (`TRUCKLOAD`)

A simple one `~/.conveyor-belt` looks like this.

    # ~/.conveyor-belt
    
    # Location of the maildir
    MAILDIR=~/mail
    
    # Save files to a local directory.
    REFINERIES=~/conveyor-belt-backup

    # Make a new file for each month
    TRUCKLOAD=month

You can get sort of fancy with the `REFINERIES` variable.

    # If you specify multiple refineries as an array, files will go to the places.
    REFINERIES=( ~/conveyor-belt-backup /mnt/external-hd/conveyor-belt-backup/ )

You must always specify a local directory, but you may also specify that
files should be sent to another server.

    # If you specify a hostname, files will be sent with scp.
    REFINERIES=( ~/conveyor-belt-backup thomaslevine.com:~/conveyor-belt-backup/%Y )

And what if you want files sent different places depending on the date?

    # If you specify date flags (see `man date`), files will be sent
    # different places depending on the date(s) of the contained emails.
    # For example, this will send SQLite files for May 2012 emails to
    # email-2012-05.thomaslevine.com:~/conveyor-belt-backup
    # Note that the date flags are independent of the TRUCKLOAD variable.
    REFINERIES=( ~/conveyor-belt-backup tlevine@email-%Y-%m.thomaslevine.com:~/conveyor-belt-backup )

Create the various `REFINERIES` directies if they don't exist.

Once that's all configured, run `conveyor-belt save` to perform a backup.
`conveyor-belt save` looks through the local directory and finds the two
most recent dates during which emails were backed up. (Maybe this should be the two most recent, at least for daily, in case the backup happens to be run right at the end of a day and some emails are late).
Conveyor belt looks for new emails starting on these dates, and proceeds
until the current date (If an email was sent after today, it is not backed up.) It adds these files to their respective databases and sends them
to the various refineries.

### File organization
The filename of a Conveyor Belt SQLite file indicates the user whose email is being backed up and the date range of the backup.

    <email address>_<date>.sqlite

It matches this regular expression

    .*_[0-9]{4}(-[0-9]{2}(-[0-9]{2})?)?.sqlite

So here are some valid filenames.

    # Emails to and from tlevine@example.com in May 2012
    tlevine@example.com_2012-05.sqlite

    # Emails to and from tlevine@example.org on May 3, 2012
    tlevine@example.org_2012-05-03.sqlite

A refinery is a directory that contains a bunch of Conveyor Belt SQLite files.

### Querying email

Each SQLite database contains a table called `e` (short for "email") that looks like this.

<table>
  <thead>
    <tr>
      <th></th><th></th><th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td></td><td></td><td></td>
    </tr>
    <tr>
      <td></td><td></td><td></td>
    </tr>
    <tr>
      <td></td><td></td><td></td>
    </tr>
  </tbody>
</table>

It has the following schema

    CREATE TABLE e (
      from TEXT NOT NULL,
      to TEXT NOT NULL, 
      cc TEXT NOT NULL, 
      bcc TEXT NOT NULL, 
      subject TEXT NOT NULL,
      ...
    )

and these indices.

    CREATE INDEX from ON e(from)
    CREATE INDEX datetime ON e(date)
    CREATE INDEX subject ON e(subject)

And you can query this like any other SQLite database.

But if you run your query through Conveyor Belt, you can query a bunch of them at once. And if you specified multiple refineries, it will balance load across them.

So you could do something like this.

    conveyor-belt 'SELECT * FROM `e`'

You probably don't want to do that though; maybe you select a subset
based on the indexed columns.

    conveyor-belt 'SELECT `subject` FROM `e` WHERE `to` LIKE "%mom%"'

Remember that the and `datetime` column are is partially expressed
in the filename; if you subset based on a datetime range, Conveyor Belt
will skip files that don't correspond to the range.

The database also has a table called `conveyor_belt_parameters`
that stores the following information.

* Email account that the emails belong to
* Conveyor Belt directory into which the database was first written
* Date backup was run, &c. (maybe)

These variables can be conveniently accessed with some special functions

    account() -- The email account
    cbdir() -- The Conveyor Belt directory

Like the `date` column, the `account()` function result is expressed in the filename; if you subset based on `account()`, Conveyor Belt knows to skip files for other accounts.

Once you've narrowed it down a bit, you might even search the non-indexed
columns.

    conveyor-belt 'SELECT `subject` FROM `e` WHERE (
      account() = "tlevine@example.com" AND
      `datetime` > "2012-01-02" AND
      `datetime` < "2012-01-07" AND
      `to` LIKE "%mom%" AND
      `body` LIKE "%velociraptors%"
    )'

### Query Output
By default, queries just return the output of the sqlite3 shell,
stacked on top of each other by database. (So `ORDER BY` commands may
yeild surprising orderings.)

You can also have the data come back as an SQLite file, a CSV file or a maildir.

    # Save the output as SQLite to emails_i_sent.sqlite
    conveyor-belt --as-sqlite emails_i_sent.sqlite \
      'SELECT * FROM `e` WHERE `from` = "tlevine@example.com"'

    # Save the output as CSV to emails_i_sent.csv
    conveyor-belt --as-csv emails_i_sent.csv \
      'SELECT * FROM `e` WHERE `from` = "tlevine@example.com"'

    # Save the output as a maildir in to emails_i_sent
    conveyor-belt --as-maildir emails_i_sent \
      'SELECT * FROM `e` WHERE `from` = "tlevine@example.com"'


cornell-gifting
------------------------------------------------------------------------------------------------
**Title**: cornell-gifting

**Web address**: https://api.github.com/repos/tlevine/cornell-gifting

**Date**: May 2012

**Description**: 

couch.thomaslevine.com
------------------------------------------------------------------------------------------------
**Title**: couch.thomaslevine.com

**Web address**: https://api.github.com/repos/tlevine/couch.thomaslevine.com

**Date**: August 2012

**Description**: couch.thomaslevine.com
======================

Install couch

    apt-get install couchdb
    
Configure it

    # Listen on all ports.
    echo 'bind_address=0.0.0.0' >> /etc/couchdb/local.ini
    
    # Redirect port 80 to 5984.
    iptables -t nat -A PREROUTING -p tcp --dport 80 -j REDIRECT --to-ports 5984

Kill [all remnants](http://serverfault.com/questions/79453/why-cant-i-access-my-couchdb-instance-externally-on-ubuntu-9-04-server)
of it to reload the configuration. Do this manually, or just reboot.

crawl-collabfinder
------------------------------------------------------------------------------------------------
**Title**: crawl-collabfinder

**Web address**: https://api.github.com/repos/tlevine/crawl-collabfinder

**Date**: April 2013

**Description**: Crawl Collabfinder
=====
I crawl CollabFinder.


crawl-collabfinder-projects
------------------------------------------------------------------------------------------------
**Title**: crawl-collabfinder-projects

**Web address**: https://api.github.com/repos/tlevine/crawl-collabfinder-projects

**Date**: April 2013

**Description**: crawl-collabfinder-downloads
============================

css-fun
------------------------------------------------------------------------------------------------
**Title**: css-fun

**Web address**: https://api.github.com/repos/tlevine/css-fun

**Date**: October 2013

**Description**: 

csv-soundsystem
------------------------------------------------------------------------------------------------
**Title**: csv-soundsystem

**Web address**: https://api.github.com/repos/tlevine/csv-soundsystem

**Date**: December 2012

**Description**: ## One-time website configuration

Make the `www.csvsoundsystem.com` bucket.

Set the bucket policy to this.

    {
        "Version": "2008-10-17",
        "Statement": [
            {
                "Sid": "PublicReadForGetBucketObjects",
                "Effect": "Allow",
                "Principal": {
                    "AWS": "*"
                },
                "Action": "s3:GetObject",
                "Resource": "arn:aws:s3:::www.csvsoundsystem.com/*"
            }
        ]
    }

Set the `Index Document` to `index.csv`.

Make an A record pointing `csvsoundsystem.com` to [108.174.51.19](http://redirect.thomaslevine.com/).

Make a CNAME record pointing `www.csvsoundsystem.com` to `csvsoundsystem.com.s3-website-us-west-2.amazonaws.com`.

## Uploading `index.csv` to S3

Upload the file.

Make `index.csv` public. (In the web console, right-click on it.)

Set the following metadata for `index.csv`.

    Content-Type: text/csv
    Content-Disposition: attachment; filename=csvsoundsystem.csv

If you haven't yet configured DNS, you can test with this address:
[http://www.csvsoundsystem.com.s3-website-us-west-2.amazonaws.com/](http://www.csvsoundsystem.com.s3-website-us-west-2.amazonaws.com/)


ctrl-cmd
------------------------------------------------------------------------------------------------
**Title**: ctrl-cmd

**Web address**: https://api.github.com/repos/tlevine/ctrl-cmd

**Date**: July 2013

**Description**: ctrl-cmd
======
`ctrl-cmd` is like the standard Windows `CMD.EXE` but with three missing features:

1. `C^c` copies
2. `C^v` pastes
3. Multi-line selections select linearly with the text rather than rectangularly
    with the screen.

It is a modified version of the `CMD.EXE` from [ReactOS](http://www.reactos.org).

## Downloading the upstream source

    ./download.sh


cu-police-daily
------------------------------------------------------------------------------------------------
**Title**: cu-police-daily

**Web address**: https://api.github.com/repos/tlevine/cu-police-daily

**Date**: April 2013

**Description**: Cornell Police Daily Crime Log
===
Inspired by [Jonathan's](https://scraperwiki.com/scrapers/cornell_police_daily_crime_log/)

The files have been downloaded to `downloads`.
One could put all of the HTML tables contained
in said files into a long table with more
standard syntax.


cu-police-daily-downloads
------------------------------------------------------------------------------------------------
**Title**: cu-police-daily-downloads

**Web address**: https://api.github.com/repos/tlevine/cu-police-daily-downloads

**Date**: April 2013

**Description**: 

cubic-error
------------------------------------------------------------------------------------------------
**Title**: cubic-error

**Web address**: https://api.github.com/repos/tlevine/cubic-error

**Date**: August 2013

**Description**: * The median minimizes the absolute error.
* The mean minimizes the squared error.
* What about the cubic error?

Here are the points that minimize higher-order errors
of a skewed distribution.

![](cubic-error.png)


cutjs
------------------------------------------------------------------------------------------------
**Title**: cutjs

**Web address**: https://api.github.com/repos/tlevine/cutjs

**Date**: August 2013

**Description**: 

data-guacamole
------------------------------------------------------------------------------------------------
**Title**: data-guacamole

**Web address**: https://api.github.com/repos/tlevine/data-guacamole

**Date**: May 2013

**Description**: Data Guacamole
=====
This is a gastronomification of 
[New York City math test scores](https://data.cityofnewyork.us/Education/Math-Test-Results-2006-2012-District-All-Students/7yig-nj52),
based on [this guacamole recipe](http://www.theyummylife.com/guacamole).

Install `R` and then `plyr` and `reshape2`.

    R -e 'install.packages(c("plyr", "reshape2"))'

Then run the recipe-generator like so.

    ./gastronomify.r

The results will go to [`data_guacamole.csv`](data_guacamole.csv).

The resulting table tells you, in order, how much of the different ingredients
to combine in the guacamole. Each ingredient represents the average test score
for a particular grade, and each recipe represents a particular year. The
ingredients and their corresponding grades are listed below.

* Ripe medium-size avocados; Haas recommended, if available (grade 3)
* Teaspoons of garlic powder (grade 4)
* Teaspoons of kosher salt (grade 5)
* Teaspoons of freshly ground black pepper (grade 6)
* Tablespoons of fresh lime juice (grade 7)
* Cups of chopped cilantro (grade 8)

In order to make all of the guacamole recipes at once, you need all of these
ingredients in total.

* 28.00 Ripe medium-size avocados; Haas recommended, if available
* 3.50 Teaspoons of garlic powder
* 3.50 Teaspoons of kosher salt
* 3.50 Teaspoons of freshly ground black pepper
* 7.00 Tablespoons of fresh lime juice
* 1.75 Cups of chopped cilantro

If that sounds like a bit too much guacamole, divide all of the quantities
by some number greater than one.


data-music-brainstorm
------------------------------------------------------------------------------------------------
**Title**: data-music-brainstorm

**Web address**: https://api.github.com/repos/tlevine/data-music-brainstorm

**Date**: August 2013

**Description**: 

Datadives
------------------------------------------------------------------------------------------------
**Title**: Datadives

**Web address**: https://api.github.com/repos/tlevine/Datadives

**Date**: May 2012

**Description**: 

datahh-json-website
------------------------------------------------------------------------------------------------
**Title**: datahh-json-website

**Web address**: https://api.github.com/repos/tlevine/datahh-json-website

**Date**: February 2013

**Description**: datahh-json-website
===================

work-in-progress personal website development

DataKind-Chicago-2012
------------------------------------------------------------------------------------------------
**Title**: DataKind-Chicago-2012

**Web address**: https://api.github.com/repos/tlevine/DataKind-Chicago-2012

**Date**: May 2012

**Description**: DataKind-Chicago-2012
=====================

So you know to use gitmodules

datakind-scy
------------------------------------------------------------------------------------------------
**Title**: datakind-scy

**Web address**: https://api.github.com/repos/tlevine/datakind-scy

**Date**: May 2012

**Description**: We're compiling a list of organizations


dblist
------------------------------------------------------------------------------------------------
**Title**: dblist

**Web address**: https://api.github.com/repos/tlevine/dblist

**Date**: May 2012

**Description**: 

deadpeoplebornonmybirthday
------------------------------------------------------------------------------------------------
**Title**: deadpeoplebornonmybirthday

**Web address**: https://api.github.com/repos/tlevine/deadpeoplebornonmybirthday

**Date**: July 2012

**Description**: deadpeoplebornonmybirthday is a website that is compiled from my
[names dataset](https://github.com/tlevine/united-states-middlenames),
which comes, in turn, from the Social Security Death Master File.

The `api` contains data, and the `www` is a pretty nanoc site.


delete-email
------------------------------------------------------------------------------------------------
**Title**: delete-email

**Web address**: https://api.github.com/repos/tlevine/delete-email

**Date**: October 2013

**Description**: Delete all of the emails in a directory in an email account.

    ./delete.py [username] [password] [directory] [[folder]]

If you do not specify folder, "INBOX" is used.


Deleware-Corporations-Scraper
------------------------------------------------------------------------------------------------
**Title**: Deleware-Corporations-Scraper

**Web address**: https://api.github.com/repos/tlevine/Deleware-Corporations-Scraper

**Date**: January 2012

**Description**: 

Deleware-Corporations-Scraper--Fixtures
------------------------------------------------------------------------------------------------
**Title**: Deleware-Corporations-Scraper--Fixtures

**Web address**: https://api.github.com/repos/tlevine/Deleware-Corporations-Scraper--Fixtures

**Date**: January 2012

**Description**: 

dep
------------------------------------------------------------------------------------------------
**Title**: dep

**Web address**: https://api.github.com/repos/tlevine/dep

**Date**: Unknown

**Description**: 

designed-congressional-record
------------------------------------------------------------------------------------------------
**Title**: designed-congressional-record

**Web address**: https://api.github.com/repos/tlevine/designed-congressional-record

**Date**: April 2013

**Description**: Designed Congressional Record
=====
We take two sections of an issue of the Congressional Record, distill them
into three sentences each, and then convert them into representations other
than prose in an attempt to make them more accessable to other people.

## Related work

* http://sunlightfoundation.com/blog/2012/05/21/grade-level-congress/


desk
------------------------------------------------------------------------------------------------
**Title**: desk

**Web address**: https://api.github.com/repos/tlevine/desk

**Date**: September 2013

**Description**: These are my Arch configuration files from May 19, 2012.

I'm also starting to use them for a laptop on November 29, 2012.

I store most things in git repositories, but here are some things that I don't
because they're big or binary. Make sure I back these up somehow.

* ekg2 logsqlite database
* Army 404 PDFs
* MongoDB contents
* SQLite files in general


dev.socrata.com
------------------------------------------------------------------------------------------------
**Title**: dev.socrata.com

**Web address**: https://api.github.com/repos/tlevine/dev.socrata.com

**Date**: September 2013

**Description**: 

dicti
------------------------------------------------------------------------------------------------
**Title**: dicti

**Web address**: https://api.github.com/repos/tlevine/dicti

**Date**: May 2012

**Description**: dicti
=======
`dicti` is a dictionary with case-insensitive keys.

It works like the normal `dict` except that key matching
is case-insensitive.

### Installing

    pip install dicti

### Creating
Import `dicti`.

    from dicti import dicti

Then instantiate `dicti` like you would a normal dict;
for example, these work.

    dict(foo = 'bar', answer = 42)
    dicti(foo = 'bar', answer = 42)
    
    dict({'foo': 'bar', 'answer': 42})
    dicti({'foo': 'bar', 'answer': 42})

### Retrieving keys
You can retrieve an item with a case-insensitive match.

    di = dicti()
    di['cAsE'] = 1
    di['case'] == di['CASE']

Methods that record keys record the original case,
just as a normal dictionary does.

    di = dicti()
    di['cAsE'] = 1
    di.keys() == ['cAsE']
    di['Case'] = 1
    di.keys() == ['Case']
    di['caSE'] == 1

Keys are still stored in their original case, however;
the original keys are presented when you request them
with methods like `dicti.keys`.


dieselfuel
------------------------------------------------------------------------------------------------
**Title**: dieselfuel

**Web address**: https://api.github.com/repos/tlevine/dieselfuel

**Date**: March 2012

**Description**: Helpers for web scrapers


dlite-jsonp
------------------------------------------------------------------------------------------------
**Title**: dlite-jsonp

**Web address**: https://api.github.com/repos/tlevine/dlite-jsonp

**Date**: August 2013

**Description**: Use JSONP in Browserify
======

```sh
npm install dlite-jsonp
```

```javascript
require('dlite-jsonp')('http://example.com?a=b&callback=%3F', function(result){
  console.log(result)
})
```


docket-parser
------------------------------------------------------------------------------------------------
**Title**: docket-parser

**Web address**: https://api.github.com/repos/tlevine/docket-parser

**Date**: January 2012

**Description**: 

doodle-videos
------------------------------------------------------------------------------------------------
**Title**: doodle-videos

**Web address**: https://api.github.com/repos/tlevine/doodle-videos

**Date**: November 2012

**Description**: 

dumptruck
------------------------------------------------------------------------------------------------
**Title**: dumptruck

**Web address**: https://api.github.com/repos/tlevine/dumptruck

**Date**: August 2012

**Description**: DumpTruck
==============
DumpTruck is a document-like interface to a SQLite database.

**But this is no longer the canonical repository**; use [this one](https://github.com/scraperwiki/dumptruck).

Quick start
----------
Install, save data and retrieve it using default settings.

### Install

    pip2 install dumptruck || pip install dumptruck

### Initialize

Open the database connection by initializing the a DumpTruck object

    dt = DumpTruck()

### Save
The simplest `insert` call looks like this.

    dt.insert({"firstname":"Thomas","lastname":"Levine"})

This saves a new row with "Thomas" in the "firstname" column and
"Levine" in the "lastname" column. It uses the table "dumptruck"
inside the database "dumptruck.db". It creates or alters the table
if it needs to.

If you insert one row, `DumpTruck.insert` returns the rowid of the row.

    dt.insert({"foo", "bar"}, "new-table") == 1

If you insert many rows, `DumpTruck.insert` returns a list of the rowids of the
new rows.

    dt.insert([{"foo", "one"}, {"foo", "two"}], "new-table") == [2, 3]

### Retrieve
Once the database contains data, you can retrieve them.

    data = dt.dump()

The data come out as a list of ordered dictionaries,
with one dictionary per row.

Slow start
-------
### Initialize

You can specify a few of keyword arguments when you initialize the DumpTruck object.
For example, if you want the database file to be `bucket-wheel-excavators.db`,
you can use this.

    dt = DumpTruck(dbname="bucket-wheel-excavators.db")

It actually takes up to three keyword arguments.

    DumpTruck(dbname='dumptruck.db', auto_commit = True, vars_table = "_dumptruckvars")

* `dbname` is the database file to save to; the default is dumptruck.db.
* `vars_table` is the name of the table to use for `DumpTruck.get_var`
and `DumpTruck.save_var`; default is `_dumptruckvars`. Set it to `None`
to disable the get_var and save_var methods.
* `auto_commit` is whether changes to the database should be automatically committed;
if it is set to `False`, changes must be committed with the `commit` method
or with the `commit` keywoard argument.

### Saving
As discussed earlier, the simplest `insert` call looks like this.

    dt.insert({"firstname": "Thomas", "lastname": "Levine"})

#### Different tables
By default, that saves to the table `dumptruck`. You can specify different table;
this saves to the table `diesel-engineers`.

    dt.insert({"firstname": "Thomas", "lastname": "Levine"}, "diesel-engineers")

#### Multiple rows
You can also pass a list of dictionaries.

    data=[
        {"firstname": "Thomas", "lastname": "Levine"},
        {"firstname": "Julian", "lastname": "Assange"}
    ]
    dt.insert(data)

#### Complex objects
You can even past nested structures; dictionaries,
sets and lists will automatically be dumped to JSON.

    data=[
        {"title":"The Elements of Typographic Style","authors":["Robert Bringhurst"]},
        {"title":"How to Read a Book","authors":["Mortimer Adler","Charles Van Doren"]}
    ]
    dt.insert(data)

Your data will be stored as JSON. When you query it, it will
come back as the original Python objects.

And if you have some crazy object that can't be JSONified,
you can use the dead-simple pickle interface.

    # This fails
    data = {"weirdthing": {range(100): None}
    dt.insert(data)

    # This works
    from DumpTruck import Pickle
    data = Pickle({"weirdthing": {range(100): None})
    dt.insert(data)

It automatically pickles and unpickles your complex object for you.

#### Names
Column names and table names automatically get quoted if you pass them without quotes,
so you can use bizarre table and column names, like `no^[hs!'e]?'sf_"&'`

#### Null values
`None` dictionary values are always equivalent to non-existance of the key.
That is, these insert commands are equivalent.

    dt = DumpTruck()
    dt.insert({ u'foo': 8, u'bar': None})
    dt.insert({ u'foo': 8})

Passing an empty dictionary creates a new row with all NULL values.

    # These all create a row with all NULL values.
    dt.insert({})
    dt.insert([{}])
    dt.insert({u'potato': None})

More precisely, they set the values to the default values via this SQL.

    INSERT INTO foo DEFAULT VALUES

Passing an empty list to `insert` inserts zero rows (rather than one);
this command does nothing.

    dt.insert([])

You can pass zero rows or empty rows to `DumpTruck.insert`, but you'll get an
error if you try passing them to `DumpTruck.create_table`.

### Retrieving

You can use normal SQL to retrieve data from the database.

    data = dt.execute('SELECT * FROM `diesel-engineers`')

The data come back as a list of dictionaries, one dictionary
per row. They are coerced to different python types depending
on their database types.

### Individual values
It's often useful to be able to quickly and easily save one metadata value.
For example, you can record which page the last run of a script managed to get up to.

    dt.save_var('last_page', 27)
    27 == dt.get_var('last_page')

It's stored in a table that you can specify when initializing DumpTruck.
If you don't specify one, it's stored in `_dumptruckvars`.

If you want to save anything other than an int, float or string type,
use json or pickle.

### Helpers
DumpTruck provides specialized wrapper around some common commands.

`DumpTruck.tables` returns a set of all of the tables in the database.

    dt.tables()

`DumpTruck.drop` drops a table.

    dt.drop("diesel-engineers")

`DumpTruck.dump` returns the entire particular table as a list of dictionaries.

    dt.dump("coal")

It's equivalent to running this:

    dt.execute('SELECT * from `coal`;')

### Creating empty tables
When working with relational databases, one typically defines a schema
before populating the database. You can use the `DumpTruck.insert` method
like this by calling it with `create_only = True`.

For example, if the table `tools` does not exist, the following call will create the table
`tools` with the columns `toolName` and `weight`, with the types `TEXT` and `INTEGER`,
respectively, but will not insert the dictionary values ("jackhammer" and 58) into the table.

    dt.create_table({"toolName":"jackhammer", "weight": 58}, "tools")

If you are concerned about the order of the tables, pass an OrderedDict.

    dt.create_table(OrderedDict([("toolName", "jackhammer"), ("weight", 58)]), "tools")

The columns will be created in the specified order.

### Indices

#### Creating
DumpTruck contains a special method for creating indices. To create an index,
first create an empty table. (See "Creating empty tables" above.)
Then, use the `DumpTruck.create_index` method.

    dt.create_index(['toolName'], 'tools')

This will create a non-unique index on the column `tool`. To create a unique
index, use the keyword argument `unique = True`.

    dt.create_index(['toolName'], 'tools', unique = True)

You can also specify multi-column indices.

    dt.create_index(['toolName', 'weight'], 'tools')

DumpTruck names these indices according to the names of the relevant table and columns.
The index created in the previous example might be named `dt__tools_toolName_weight`.

#### Other index manipulation
DumpTruck does not implement special methods for viewing or removing indices, but here
are the relevant SQLite SQL commands.

The following command lists indices on the `tools` table.

    dt.execute('PRAGMA index_list(tools)')

The following command gives more information about the index named `dt__tools_toolName_weight`.

    dt.execute('PRAGMA index_info(dt__tools_toolName_weight)')

And this one deletes the index.

    dt.execute('DROP INDEX dt__tools_toolName_weight')

For more information on indices and, particularly, the `PRAGMA` commands, check
the [SQLite documentation]().

### Delaying commits
By default, the `insert`, `get_var`, `drop` and `execute` methods automatically commit changes.
You can stop one of them from committing by passing `commit=False` to the method.
Commit manually with the `commit` method.  For example:

    dt = DumpTruck()
    dt.insert({"name":"Bagger 293","manufacturer":"TAKRAF","height":95}, commit=False)
    dt.save_var('page_number', 42, commit=False)
    dt.commit()


dumptruck-web
------------------------------------------------------------------------------------------------
**Title**: dumptruck-web

**Web address**: https://api.github.com/repos/tlevine/dumptruck-web

**Date**: February 2013

**Description**: Foo bar
==============

## Running on Nginx

### CGI
Here's an Nginx FastCGI configuration for Ubuntu based on the
[Arch Linux Wiki](https://wiki.archlinux.org/index.php/Nginx#FastCGI).

Install.

    apt-get install fcgiwrap nginx

Configure the nginx site. (Try `/etc/nginx/sites-enabled/default`.)

    location / {                                               
      fastcgi_param DOCUMENT_ROOT /var/www/dumptruck-web/;
      fastcgi_param SCRIPT_NAME dumptruck_web.py;
      fastcgi_param SCRIPT_FILENAME /var/www/dumptruck-web/dumptruck_web.py;
      fastcgi_pass unix:/var/run/fcgiwrap.socket;  
      
      # Fill in the gaps. This does not overwrite previous settings,
      # so it goes last
      include /etc/nginx/fastcgi_params;
     }

This depends on `/var/www/dumptruck_web.py` being a cgi script file that www-data
can execute.
 
If you're installing this as part of cobalt, the configuration is 

    rewrite  ^\/([^\s/]+)\/sqlite\/?$  /$1/sqlite?box=$1;

    location ~ ^\/([^\s/]+)\/sqlite\/?$ {
        fastcgi_param DOCUMENT_ROOT /var/www/dumptruck-web/;
        fastcgi_param SCRIPT_NAME dumptruck_web.py;
        fastcgi_param SCRIPT_FILENAME /var/www/dumptruck-web/dumptruck_web.py;

        # Fill in the gaps. This does not overwrite previous settings,
        # so it goes last
        include /etc/nginx/fastcgi_params;
        fastcgi_pass unix:/var/run/fcgiwrap.socket;
    }


Specify some high number of processes in `/etc/init.d/fcgiwrap` like so.

    FCGI_CHILDREN="9001"

You could also try something less extreme.

Then (re)start the daemons.

    service fcgiwrap restart
    service nginx restart

If this doesn't work, read `/etc/init.d/fcgiwrap`.

An example (simple) script would be

    #!/usr/bin/env python
    
    print '''HTTP/1.1 200
    Content-Type: text/plain
    
    Hello world
    '''

An API call looks like this,

    /jack-in-the/sqlite?q=SELECT+foo+FROM+baz

but the CGI script expects this,

    /sqlite?q=SELECT+foo+FROM+baz&box=made-of-ticky-tacky

so the Nginx needs to rewrite the URL.

### uWSGI
Here's a configuration based on the
[uWSGI quickstart](http://projects.unbit.it/uwsgi/wiki/Quickstart) 

Also see [uWSGI on nginx page](http://projects.unbit.it/uwsgi/wiki/RunOnNginx),
for reference but note that this explains a configuration that may be
unnecessarily complicated.

Install these.

    apt-get install uwsgi nginx uwsgi-plugin-{http,python}  

Run this (preferably as a daemon).

    uwsgi \
      --plugins http,python \
      --wsgi-file foobar.py \
      --socket 127.0.0.1:3031 \
      --callable application \
      --processes 20

Use some high number of processes because they block.

We'll have to adjust the api script so that it works with uWSGI;
once we do, add this to the nginx site. (Try `/etc/nginx/sites-enabled/default`.)

    location /path/to/sqlite {
        include uwsgi_params;
        uwsgi_pass 127.0.0.1:3031;
    }

Restart nginx.

    service nginx restart

Test

    curl localhost/path/to/sqlite?q=SELECT+42+FROM+sqlite_master&boxname=jack-in-the

An example (simple) script would be

    def application(env, start_response):
        start_response('200 OK', [('Content-Type','text/html')])
        return "Hello World"

## Add later
Gzip responses.

## SQLite errors
The SQLite errors are normally pretty good, so an api call with that raises a
SQLite error normally displays the error messages. This includes

* Locked databases
* Ungrammatical SQL

Some of these errors aren't great, like

* Database that the user doesn't have permission to read

We also treat some things as errors that SQLite doesn't:

* Databases that don't exist


dumptruck-website
------------------------------------------------------------------------------------------------
**Title**: dumptruck-website

**Web address**: https://api.github.com/repos/tlevine/dumptruck-website

**Date**: October 2012

**Description**: # [HTML5 Boilerplate](http://html5boilerplate.com)

HTML5 Boilerplate is a professional front-end template that helps you build fast, robust, adaptable, and future-proof websites. Spend more time developing and less time reinventing the wheel.

This project is the product of many years of iterative development and combined community knowledge. It does not impose a specific development philosophy or framework, so you're free to architect your code in the way that you want.


## Quick start

Clone the git repo - `git clone git://github.com/h5bp/html5-boilerplate.git` - or [download it](https://github.com/h5bp/html5-boilerplate/zipball/master)


## Features

* HTML5 ready. Use the new elements with confidence.
* Cross-browser compatible (Chrome, Opera, Safari, Firefox 3.6+, IE6+).
* Designed with progressive enhancement in mind.
* CSS normalizations and common bug fixes.
* IE-specific classes for easier cross-browser control.
* A default print stylesheet, performance optimized.
* Mobile browser optimizations.
* Protection against any stray `console.log` causing JavaScript errors in IE6/7.
* The latest jQuery via CDN, with a local fallback.
* A custom Modernizr build for feature detection.
* An optimized Google Analytics snippet.
* Apache server caching, compression, and other configuration defaults for Grade-A performance.
* Cross-domain Ajax and Flash.
* "Delete-key friendly." Easy to strip out parts you don't need.
* Extensive inline and accompanying documentation.


## Contributing

Anyone and everyone is welcome to [contribute](https://github.com/h5bp/html5-boilerplate/wiki/contribute). Hundreds of developers have helped make the HTML5 Boilerplate what it is today.


## Project information

* Source: http://github.com/h5bp/html5-boilerplate
* Web: http://html5boilerplate.com
* Docs: http://html5boilerplate.com/docs
* Twitter: http://twitter.com/h5bp


## License

### Major components:

* jQuery: MIT/GPL license
* Modernizr: MIT/BSD license
* Normalize.css: Public Domain

### Everything else:

The Unlicense (aka: public domain)


DWB---NYCLU-Cleaning-Data
------------------------------------------------------------------------------------------------
**Title**: DWB---NYCLU-Cleaning-Data

**Web address**: https://api.github.com/repos/tlevine/DWB---NYCLU-Cleaning-Data

**Date**: October 2011

**Description**: [This folder](https://dl-web.dropbox.com/get/datadive_nyclu/Data.zip?w=a45327a6&dl=1) contains the appended 2007-2010 data in a hopefully slightly smaller form with some of the variables left out, saved as "data.csv" or "data.RData". 

The same data set with the crime suspected, crime of arrest, and summons crime is "all.csv", "all.RData".

Additional variables that were removed can be merged back in by rownum and year from the [year]merged.csv data sets.

The key to the recoded variables is the "key" pdf.

The GenerateData R file should generate the data I have in the folder from the data available for download on the NYCPD website.

email maryclaregriffin@gmail.com with questions, comments, and/or concerns.

DWB---NYCLU-Dates
------------------------------------------------------------------------------------------------
**Title**: DWB---NYCLU-Dates

**Web address**: https://api.github.com/repos/tlevine/DWB---NYCLU-Dates

**Date**: November 2011

**Description**: I wrote some scripts for selecting columns from the data
and cutting them up into smaller files with fewer rows.
The latter will be useful if you can parellize a job across rows.

I also cleaned all of the dates. Here's how you run that.

    $ ./get_data.sh #Download the data and extract it
    $ ./select.sh #Select the date and time columns
    $ ./export_dates-auto.py #Clean the dates and times and send them to json files

The resulting dates are in `data/*-cleantimes.json`.
The "key" refers is the raw time, and the "value" is
the cleaned time. They're are also
[here](http://thomaslevine.com/sqf-dates.tar.gz).

dwb-neef
------------------------------------------------------------------------------------------------
**Title**: dwb-neef

**Web address**: https://api.github.com/repos/tlevine/dwb-neef

**Date**: March 2012

**Description**: 

earthmover
------------------------------------------------------------------------------------------------
**Title**: earthmover

**Web address**: https://api.github.com/repos/tlevine/earthmover

**Date**: March 2012

**Description**: Earthmover provides a way of writing concise, modular programs for
extracting data from messy sources like websites.
It has four components, each of which can be used independently of the others.

**Dumptruck** is a relaxing interface to SQLite. It makes common database commands more concise and Pythonic.

**DieselFuel** is an assortment of helpers for scraping. For example, it provides a function to get options from drop-down box.

**dblist** is a list-like object backed by a plain text file or a SQLite databese. BucketWheel uses it to maintain a stack across scraper runs.

**BucketWheel** abstracts the less variable aspects of scrapers, embeds scraping best practices
and encourages a modular design of scrapers.


ecohack-wetlands
------------------------------------------------------------------------------------------------
**Title**: ecohack-wetlands

**Web address**: https://api.github.com/repos/tlevine/ecohack-wetlands

**Date**: November 2012

**Description**: EcoHack 3 Wetlands Project
=======
Scott from the Gulf Restoration Network is already using the data in his
[efforts](http://healthygulf.org/our-work/wetlands/wetlands-overview)
to protect wetlands from reckless development, but he doesn't have the time or
skills that we have this weekend.

Let's do something with these data! Here are some ideas based on discussions
with Scott, but do consider other things too.

* Improve the identification of the "total acreage" figure
* Map the applications. Until we extract latitudes and longitudes, we can use
    parish instead.
* Present the data otherwise. We have the full images and text of these
    documents, and I've already pulled out information relating to laws, dates
    and people.
* Migrate data from an old version (See below.)

## Dataset files

### Spreadsheet
We have data on applications to build things on wetlands. The `wetlands.csv`
file in this repository is taken from September 9, 2012, but the data are
updated [every day](https://github.com/tlevine/wetlands). I (Tom) recommend
that we just work on this spreadsheet today. If additional data cleaning code
is written, I'll eventually move it to the automatic importer scripts.

### PDFs
If I get power back in my house, the pdf files linked in the spreadsheet will
be available where they say they are. If not, I'll find some other place to
host them.

### Mongo
A bit more data is stored in a MongoDB, but I doubt that's of interest to
anyone for such a small dataset. If power comes back on in my house, we can use
that too.

### Old version
The `v1` version was running for a few months before I started this `v2`
version, and I haven't moved the data over. The storage format is
[documented](https://github.com/tlevine/wetlands/tree/master/v1), and
some of the files are [here](http://chainsaw.thomaslevine.com/wetlands/).

Migrating these data to the new `v2` filesystem could be a nice quick project.


ecohack2012-cites
------------------------------------------------------------------------------------------------
**Title**: ecohack2012-cites

**Web address**: https://api.github.com/repos/tlevine/ecohack2012-cites

**Date**: April 2012

**Description**: Run `download.sh` to download and unzip the data from DropBox.


ekg2-logsqlite-analysis
------------------------------------------------------------------------------------------------
**Title**: ekg2-logsqlite-analysis

**Web address**: https://api.github.com/repos/tlevine/ekg2-logsqlite-analysis

**Date**: September 2012

**Description**: EKG2 logsqlite analysis
===========
Analyze status activity from an EKG2 logsqlite database.


euler
------------------------------------------------------------------------------------------------
**Title**: euler

**Web address**: https://api.github.com/repos/tlevine/euler

**Date**: March 2013

**Description**: 

euler-sql
------------------------------------------------------------------------------------------------
**Title**: euler-sql

**Web address**: https://api.github.com/repos/tlevine/euler-sql

**Date**: October 2013

**Description**: Project Euler (http://projecteuler.net/problems) in SQL


facebook-chat-analysis
------------------------------------------------------------------------------------------------
**Title**: facebook-chat-analysis

**Web address**: https://api.github.com/repos/tlevine/facebook-chat-analysis

**Date**: June 2013

**Description**: Facebook Chat
====


`./union.sh` combines all of the databases into one database, `/tmp/logs.db`.

`./analyze.r` does all of the analysis after the database has been created.

## Temporary file storage
I configure my computers such that `/tmp` is a ram disk, so I'm storing
temporary files in `/tmp`. (This makes things fast.)


facebook-friends-online
------------------------------------------------------------------------------------------------
**Title**: facebook-friends-online

**Web address**: https://api.github.com/repos/tlevine/facebook-friends-online

**Date**: September 2012

**Description**: The analysis code expects the EKG2_LOGSQLITE_DB environment variable to be set.


fallos
------------------------------------------------------------------------------------------------
**Title**: fallos

**Web address**: https://api.github.com/repos/tlevine/fallos

**Date**: September 2012

**Description**: * Fallos de Argentina
* Tom y Cesar
* Hacks/Hackers Buenos Aires Media Party Hackathon


favicon_repository
------------------------------------------------------------------------------------------------
**Title**: favicon_repository

**Web address**: https://api.github.com/repos/tlevine/favicon_repository

**Date**: April 2012

**Description**: 

fbi-occupy-report
------------------------------------------------------------------------------------------------
**Title**: fbi-occupy-report

**Web address**: https://api.github.com/repos/tlevine/fbi-occupy-report

**Date**: December 2012

**Description**: Read about the report [here](http://www.guardian.co.uk/commentisfree/2012/dec/29/fbi-coordinated-crackdown-occupy).
I got it from [DocumentCloud](http://www.documentcloud.org/documents/549516-fbi-spy-files-on-the-occupy-movement).


fcx2013
------------------------------------------------------------------------------------------------
**Title**: fcx2013

**Web address**: https://api.github.com/repos/tlevine/fcx2013

**Date**: April 2013

**Description**: 

feedformatter
------------------------------------------------------------------------------------------------
**Title**: feedformatter

**Web address**: https://api.github.com/repos/tlevine/feedformatter

**Date**: April 2012

**Description**: # USAGE

See http://code.google.com/p/feedformatter/w/list for instructions.

# REQUIREMENTS

Feedformatter requires some version of ElementTree.

There are two versions of ElementTree - the Python version and the C version.  T
he C version is faster, but feedformatter will happily work with either version.
  It chooses the fastes version available on your system.

Modern versions of Python have ElementTree in the standard library so feedformat
ter should "just work".

If you have an older version of Python, you'll need to make sure ElementTree is 
installed in your site-libs directory.  It's in PyPI so you can just use easy_in
stall.


fizzbuzz-latex
------------------------------------------------------------------------------------------------
**Title**: fizzbuzz-latex

**Web address**: https://api.github.com/repos/tlevine/fizzbuzz-latex

**Date**: March 2013

**Description**: 

flickr-aurora
------------------------------------------------------------------------------------------------
**Title**: flickr-aurora

**Web address**: https://api.github.com/repos/tlevine/flickr-aurora

**Date**: April 2013

**Description**: Photographic Validation of Satellite Forecasting Models for Extreme Weather Events
===
This script downloads images for a prototype of a website for the study of
current auroras.

    # Set your credentials
    export FLICKR_KEY=12345abcde...

    # Create schema
    sqlite3 aurora.db < schema.sql

    # Download
    ./run

    # Generate csv
    sqlite3 aurora.db -csv -header 'SELECT * FROM aurora;'

Test the parser

    nosetests2


forjar
------------------------------------------------------------------------------------------------
**Title**: forjar

**Web address**: https://api.github.com/repos/tlevine/forjar

**Date**: July 2013

**Description**: Forjar
=========

Forjar (spanish for forge) is a data generator for mocking datasets. You setup a schema, a period of time, and a model of how the data should perform over time and then run it!

## Dependancies

 - sqlalchemy

## Running

There is currently one example.  Run

    python examples/boatio.py

The default settings will run the simulation for 365 days and pump it to the sqlite database called forjer.db.

To run the simulation for 150 days into the database at sqlite:///boatio.sqlite use

    python examples/boatio.py -d 150 -e sqlite:///boatio.sqlite





formaldehyde
------------------------------------------------------------------------------------------------
**Title**: formaldehyde

**Web address**: https://api.github.com/repos/tlevine/formaldehyde

**Date**: March 2013

**Description**: 

Let's be inspired by [Where's George](http://www.wheresgeorge.com/).

* It's designed for people who aren't ordinary web users.
* Guerilla advertising
* Define your identity

We're talking to

* Trailer resellers
* People who just bought a trailer
* People after hurricane Katrina
  * Enter serial number and story
* Mobile homes in general

After Katrina, the government bought ~100,000 trailers.

For privacy: Locations are zip codes.

Do any data need to be private?

We would like to ask lots of questions of people, but that could get daunting. People may come back many times because they are interested in their health, and we can ask the questions gradually.

A diary of stories and symtoms, related to zip code weather, would collect wonderful data. It could help with the science a lot and might feel cathartic.

Attempts to remediate (like with plants)


User registrations would be nice. But not quite necessary because VINs are long.

Information to collect

* Phone number
* Email address

First website, then phone number?


freespace-favicon-sniffer
------------------------------------------------------------------------------------------------
**Title**: freespace-favicon-sniffer

**Web address**: https://api.github.com/repos/tlevine/freespace-favicon-sniffer

**Date**: June 2013

**Description**: This converts a favicon.

    convert -scale 25x25 favicon.ico led.png



frons
------------------------------------------------------------------------------------------------
**Title**: frons

**Web address**: https://api.github.com/repos/tlevine/frons

**Date**: December 2012

**Description**: You're probably interested in frons.csv. frons.db is a sqlite database.
I didn't save the code for converting.


gastronomify-groceries
------------------------------------------------------------------------------------------------
**Title**: gastronomify-groceries

**Web address**: https://api.github.com/repos/tlevine/gastronomify-groceries

**Date**: May 2013

**Description**: Gastronomify
=====

We can order food from Safeway, though their website isn't great.
Here are some notes on that.

## Foods
As a start for gastronomification, we might try fancy fruit juices
because their identification codes are less likely to change over time.
Here are some helpful URLs on that front.

* http://shop.safeway.com/superstore/searchShelf.asp?ShelfId=5_3_15&search=64
* http://shop.safeway.com/superstore/default.asp?page=d&navURL=/dNet/shelves.aspx?ID=5_3&mainURL=/dNet/IconShelves.aspx?ID=5_3
* http://shop.safeway.com/superstore/shelf.asp?shelfId=5_3_15&DeptName=Beverages&AisleName=Juice%20%26%20Nectars&ShelfName=Specialty%20Juice%20%26%20Drinks

Some obvious variables are here.

* Brand
* Fruit
* Size of bottle
* Number of bottles ordered

## Safeway API
Safeway's website isn't the most exciting, but we can work with it.


## Data source
We'll use the daily treasury data compiled by CSV Soundsystem.

## How to run

    export SAFEWAY_EMAIL=foo@datapad.io
    export SAFEWAY_PASSWORD=abcdefg
    ./gastronomify.py

I think we're gonna need to write this in Selenium or something.


gh-pages-symlink-test
------------------------------------------------------------------------------------------------
**Title**: gh-pages-symlink-test

**Web address**: https://api.github.com/repos/tlevine/gh-pages-symlink-test

**Date**: February 2013

**Description**: Do symlinks work on github-pages? If they do, these pages will be the same.

* [Target](http://tlevine.github.com/gh-pages-symlink-test/target.txt)
* [Link](http://tlevine.github.com/gh-pages-symlink-test/link.txt)


git-achievements
------------------------------------------------------------------------------------------------
**Title**: git-achievements

**Web address**: https://api.github.com/repos/tlevine/git-achievements

**Date**: October 2012

**Description**: Git-Achievements records all of the achievements you acquire while using Git.

There are over 40 achievement, most with different levels to be achieved.
After a git command is executed if a new achievement is unlocked
git-achievements will display a message on the console for your enjoyment:

********************************************************************************
                            Git Achievement Unlocked!                            

                                    Caretaker                                    
                    Added a .gitignore file to a repository.                    
********************************************************************************


GitHub Pages
------------

A log of all of your achievements is kept locally, but you can also publish
it to GitHub pages so you can share your achievements (and there is a rss
feed so people can track your achievements).

For example here is the project maintainer's achievements page: http://icefox.github.com/git-achievements

If you are viewing a forked version of git-achievements you want to replace icefox
with the github user account you want to see like so:

http://<username>.github.com/git-achievements

To push your achievements to GitHub first fork the project on GitHub,
clone *your* repository and set the following config to true:

git config --global achievement.upload "true"

When an achievement is unlocked the index.html file will be overwritten,
committed and then a 'git push origin' will be executed.


Install
-------
Add git-achievements to your path and alias git to git-achievements

For example add the following to the end of your ~/.bash_profile

export PATH="$PATH:~/git/git-achievements"
alias git="git-achievements"

You can get your first achievement by running

    git achievements --help

Running Tests
-----
Install Urchin (http://www.urchin.sh) by putting it in your PATH;
here's one way to do that.

    wget -O /usr/local/bin/urchin https://raw.github.com/scraperwiki/urchin/0c6837cfbdd0963903bf0463b05160c2aecc22ef/urchin
    chmod +x /usr/local/bin/urchin

Run the tests like so (from the root of the git repository.)

    urchin ./test


git-extras
------------------------------------------------------------------------------------------------
**Title**: git-extras

**Web address**: https://api.github.com/repos/tlevine/git-extras

**Date**: October 2012

**Description**: # Git Extras

Little git extras.

## Installation

Clone / Tarball:

```bash
$ make install
```

One-liner:

```bash
$ curl https://raw.github.com/visionmedia/git-extras/master/bin/git-extras | INSTALL=y sh
```

[MacPorts](http://www.macports.org/)

```bash
$ sudo port install git-extras
```

[Brew](github.com/mxcl/homebrew/) (buggy):

```bash
$ brew install git-extras
```

## Screencasts

  Just getting started? Check out these screencasts:
  
 - [introduction](https://vimeo.com/45506445) -- covering git-ignore, git-setup, git-changelog, git-release, git-effort and more

## Commands

 - `git extras`
 - `git squash`
 - `git summary`
 - `git effort`
 - `git changelog`
 - `git commits-since`
 - `git count`
 - `git create-branch`
 - `git delete-branch`
 - `git delete-submodule`
 - `git delete-tag`
 - `git fresh-branch`
 - `git graft`
 - `git alias`
 - `git ignore`
 - `git info`
 - `git release`
 - `git contrib`
 - `git repl`
 - `git undo`
 - `git gh-pages`
 - `git setup`
 - `git touch`
 - `git obliterate`
 - `git feature`
 - `git refactor`
 - `git bug`
 - `git promote`
 - `git local-commits`

## extras

The main `git-extras` command.

Output the current `--version`:

```bash
$ git extras
```

List available commands:

```bash
$ git extras --help
```

Update to the latest `git-extras`:

```bash
$ git extras update
```


## gh-pages

Sets up the `gh-pages` branch.  (See [GitHub Pages](http://pages.github.com/) documentation.)

## git-[feature|refactor|bug] [finish] &lt;name&gt;

Create the given feature, refactor, or bug branch `name`:

```bash
$ git feature dependencies
```

Afterwards, the same command will check it out:

```bash
$ git checkout master
$ git feature dependencies
```

When finished, we can `feature finish` to merge it into the current branch:

```bash
$ git checkout master
$ git feature finish dependencies
```

All of this works with `feature`, `bug`, or `refactor`.

## git-contrib &lt;author&gt;

Output `author`'s contributions to a project:

```bash
$ git contrib visionmedia
visionmedia (18):
  Export STATUS_CODES
  Replaced several Array.prototype.slice.call() calls with Array.prototype.unshift.call()
  Moved help msg to node-repl
  Added multiple arg support for sys.puts(), print(), etc.
  Fix stack output on socket error
  ...
```

## git-summary

Outputs a repo summary:

```bash
$ git summary

project  : git-extras
repo age : 10 months ago
commits  : 163
active   : 60 days
files    : 93
authors  :
   97	Tj Holowaychuk          59.5%
   37	Jonhnny Weslley         22.7%
	8	Kenneth Reitz           4.9%
	5	Aggelos Orfanakos       3.1%
	3	Jonathan "Duke" Leto    1.8%
	2	Gert Van Gool           1.2%
	2	Domenico Rotiroti       1.2%
	2	Devin Withers           1.2%
	2	TJ Holowaychuk          1.2%
	1	Nick Campbell           0.6%
	1	Alex McHale             0.6%
	1	Jason Young             0.6%
	1	Jens K. Mueller         0.6%
	1	Guillermo Rauch         0.6%
```

This command can also take a *commitish*, and will print a summary for commits in 
the commmitish range:

```bash
$ git summary v42..
```

## git-effort [file ....]

  Displays "effort" statistics, currently just the number of commits per file, showing highlighting where the most activity is. The "active days" column is the total number of days which contributed modifications to this file.

```
node (master): git effort --above 15 {src,lib}/*
```

  ![git effort](http://f.cl.ly/items/0b0w0S2K1d100e2T1a0D/Screen%20Shot%202012-02-08%20at%206.43.34%20PM.png)

  If you wish to ignore files with commits `<=` a value you may use `--above`:
  
```
$ git effort --above 5
```

## git-repl

GIT read-eval-print-loop:

```bash
$ git repl

git> ls-files
History.md
Makefile
Readme.md
bin/git-changelog
bin/git-count
bin/git-delete-branch
bin/git-delete-tag
bin/git-ignore
bin/git-release

git> quit
```

  By default `git ls-files` is used, however you may pass one or more files to `git-effort(1)`, for example:

```
$ git effort bin/* lib/*
```

## git-commits-since [date]

List commits since `date` (defaults to "last week"):

```bash
$ git commits-since
... changes since last week
TJ Holowaychuk - Fixed readme
TJ Holowaychuk - Added git-repl
TJ Holowaychuk - Added git-delete-tag
TJ Holowaychuk - Added git-delete-branch

$ git commits-since yesterday
... changes since yesterday
TJ Holowaychuk - Fixed readme
```

## git-count

Output commit count:

```bash
$ git count

total 1844
```

Output detailed commit count:

```bash
$ git count --all

visionmedia (1285)
Tj Holowaychuk (430)
Aaron Heckmann (48)
csausdev (34)
ciaranj (26)
Guillermo Rauch (6)
Brian McKinney (2)
Nick Poulden (2)
Benny Wong (2)
Justin Lilly (1)
isaacs (1)
Adam Sanderson (1)
Viktor Kelemen (1)
Gregory Ritter (1)
Greg Ritter (1)
ewoudj (1)
James Herdman (1)
Matt Colyer (1)

total 1844
```

## git-release

Release commit with the given &lt;tag&gt;:

```bash
$ git release 0.1.0
```

Does the following:
  
  - Executes _.git/hooks/pre-release.sh_ (if present)
  - Commits changes (to changelog etc) with message "Release &lt;tag&gt;"
  - Tags with the given &lt;tag&gt;
  - Push the branch / tags
  - Executes _.git/hooks/post-release.sh_ (if present)

## git-alias

Define, search and show aliases.

Define a new alias:

```bash
$ git alias last "cat-file commit HEAD"
```

Search for aliases that match a pattern (one argument):

```bash
$ git alias ^la
last = cat-file commit HEAD
```

Show all aliases (no arguments):

```bash
$ git alias
s = status
amend = commit --amend
rank = shortlog -sn --no-merges
whatis = show -s --pretty='tformat:%h (%s, %ad)' --date=short
whois = !sh -c 'git log -i -1 --pretty="format:%an <%ae>
```

## git-ignore [pattern ...]

Too lazy to open up `.gitignore`?  Me too!

```bash
$ git ignore build "*.o" "*.log"
... added 'build'
... added '*.o'
... added '*.log'
```

Add patterns from an existing template:

```bash
$ git ignore -t rails
```

Without any patterns, `git-ignore` displays currently ignored patterns:

```bash
$ git ignore
build
*.o
*.log
```

# git-info

Show information about the repo:

```bash
$ git info

    ## Remote URLs:

    origin              git@github.com:sampleAuthor/git-extras.git (fetch)
    origin              git@github.com:sampleAuthor/git-extras.git (push)

    ## Remote Branches:

    origin/HEAD -> origin/master
    origin/myBranch

    ## Local Branches:

    myBranch
    * master

    ## Most Recent Commit:

    commit e3952df2c172c6f3eb533d8d0b1a6c77250769a7
    Author: Sample Author <sampleAuthor@gmail.com>

    Added git-info command.

    Type 'git log' for more commits, or 'git show <commit id>' for full commit details.

    ## Configuration (.git/config):

    color.diff=auto
    color.status=auto
    color.branch=auto
    user.name=Sample Author
    user.email=sampleAuthor@gmail.com
    core.repositoryformatversion=0
    core.filemode=true
    core.bare=false
    core.logallrefupdates=true
    core.ignorecase=true
    remote.origin.fetch=+refs/heads/*:refs/remotes/origin/*
    remote.origin.url=git@github.com:mub/git-extras.git
    branch.master.remote=origin
    branch.master.merge=refs/heads/master

```

## git-create-branch &lt;name&gt;

Create local and remote branch `name`:

```bash
$ git create-branch development
```

## git-delete-branch &lt;name&gt;

Delete local and remote branch `name`:

```bash
$ git delete-branch integration
```

## git-delete-submodule &lt;name&gt;

Delete submodule `name`:

```bash
$ git delete-submodule lib/foo
```

## git-delete-tag &lt;name&gt;

Delete local and remote tag `name`:

```bash
$ git delete-tag 0.0.1
```

## git-fresh-branch &lt;name&gt;

Create empty local branch `name`:

```bash
$ git fresh-branch docs
```

## git-graft &lt;src-branch&gt; [dest-branch]

Merge commits from `src-branch` into `dest-branch`. (`dest-branch` defaults to `master`.)

```bash
$ git graft new_feature dev
$ git graft new_feature
```

## git-squash &lt;src-branch&gt; [msg]

Merge commits from `src-branch` into the current branch as a _single_ commit. When `[msg]` is given `git-commit(1)` will be invoked with that message. This is useful when small individual commits within a topic branch are irrelevant and you want to consider the topic as a single change.

```bash
$ git squash fixed-cursor-styling
$ git squash fixed-cursor-styling "Fixed cursor styling"
```

## git-changelog

Populate a file whose name matches `change|history -i_` with commits
since the previous tag.  (If there are no tags, populates commits since the project began.) 

Opens the changelog in `$EDITOR` when set.

```bash
$ git changelog && cat History.md

n.n.n / 2010-08-05
==================

* Docs for git-ignore. Closes #3
* Merge branch 'ignore'
* Added git-ignore
* Fixed <tag> in docs
* Install docs
* Merge branch 'release'
* Added git-release
* Passing args to git shortlog
* Added --all support to git-count
* Initial commit
```

List commits:

```bash
$ git changelog --list

* Docs for git-ignore. Closes #3
* Merge branch 'ignore'
* Added git-ignore
* Fixed <tag> in docs
* Install docs
* Merge branch 'release'
* Added git-release
* Passing args to git shortlog
* Added --all support to git-count
* Initial commit
```

## git-undo

Remove the latest commit:

```bash
git undo
```

Remove the latest 3 commits:

```bash
git undo 3
```

## git-setup [dir]

Set up a git repository (if one doesn't exist), add all files, and make an initial commit. `dir` defaults to the current working directory.

## git-touch [filename]

Call `touch` on the given file, and add it to the current index. One-step creation of new files.

## git-obliterate [filename]

Completely remove a file from the repository, including past commits and tags.

```bash
git obliterate secrets.json
```

## git-local-commits

List all commits on the local branch that have not yet been sent to origin. Any additional arguments will be passed directly to git log.


git.thomaslevine.com-fail1
------------------------------------------------------------------------------------------------
**Title**: git.thomaslevine.com-fail1

**Web address**: https://api.github.com/repos/tlevine/git.thomaslevine.com-fail1

**Date**: June 2012

**Description**: This is designed for use in a NearlyFreeSpeech.net site. Configure it,
push it somewhere like GitHub, then clone it like so.

    git clone git://github.com/tlevine/git.thomaslevine.com.git /home/public

Store local configuration in the `master` branch and pull upstream changes
from the `upstream` branch.

Read about viewgit in doc/README.


github-user-growth
------------------------------------------------------------------------------------------------
**Title**: github-user-growth

**Web address**: https://api.github.com/repos/tlevine/github-user-growth

**Date**: May 2012

**Description**: GitHub User Growth
======

We have [counts of GitHub users over time](https://api.scraperwiki.com/api/1.0/datastore/sqlite?format=csv&name=github_users_each_year&query=select+*+from+`swdata`&apikey=). Let's predict how this number changes.

Call new_users/(total_users - new_users) the interest_rate,
and skip the first 18 months as this growth rate doesn't fit them.
Here's interest rate over time:

	lm(formula = interest_rate ~ when_int, data = g[-1:-18, ])
        # when_int is the number of months after December 2007.
        # interest_rate is really last month's interest rate;
        # it's new_users/(total_users - new_users).

	Coefficients:
		      Estimate Std. Error t value Pr(>|t|)    
	(Intercept)  0.1467628  0.0080344  18.267  < 2e-16 ***
	when_int    -0.0015762  0.0002181  -7.226  3.3e-08 ***
	---
	Signif. codes:  0 a***a 0.001 a**a 0.01 a*a 0.05 a.a 0.1 a a 1 

	Residual standard error: 0.01248 on 32 degrees of freedom
	Multiple R-squared:  0.62,Adjusted R-squared: 0.6081 
	F-statistic: 52.21 on 1 and 32 DF,  p-value: 3.303e-08 

Note the that the interest rate decreases by about 0.15% every month.
The plots (github-growth[1-4].pdf) make that relationship look very
strong and makes the assumptions approximate linearity, homoskedasticity
(unvarying variance), &c. seem valid.

I tried converting to a continuous interest formula, (That's what's in
my notebook.) but I didn't get very far. So I did monthly compounding;
Here's an estimate of the number of GitHub-users over time

    A[t+1] = A[t] * ( 1 + r[t] )

where

    A[t]   = Number of users in the current month (This must come from a month after June 2009.)
    A[t+1] = Number of users in the next month
    r[t]   = 14.67% - 0.15% * (Current number of months since December 2007)

And you can fix that so it chains months together and
averages them and whatnot.

So the growth looks exponential with a decreasing exponent;
think of it as compounded interest on number of users with
a linearly decreasing interest rate.


gitmarks
------------------------------------------------------------------------------------------------
**Title**: gitmarks

**Web address**: https://api.github.com/repos/tlevine/gitmarks

**Date**: October 2011

**Description**: UPDATE: Far McKon's fork is the new official fork, follow that one instead! It's here: https://github.com/FarMcKon/gitmarks_2

========
Gitmarks
========

A web bookmark manager built on git and designed for github. It's searchable and social!

Gitmarks is a script that, given a URL, description, and tags, will download the content of the web page and store it along with the metadata so that you can easily search it (with grep!) and comment on it (with github!)

It's great for groups to collaboratively collect bookmarks in one spot (thanks to git itself!)

=======
Details
=======

For each URL, gitmarks will pull the content and store it under the 'content' directory. It stores the metadata under each tag in the 'tags' directory.

You can use git as usual to see who committed what and when, or you can grep your way to bookmark happiness on the command line.

=====
Usage
=====

python gitmark.py [url]

options:
	-p = do not push to origin (store bookmark locally only)
	-m = description of the bookmark
	-t = a comma-delimited list of tags
	
Example:

python gitmark.py -m 'my site' -t me,hilary_mason,code,bookmarks http://www.hilarymason.com


===============
OMG Delicious?!
===============

Yes, you can import your delicious bookmarks!

Usage:

python delicious_import.py [username] [password]

(be patient if you have a lot of them.)


=============================
Using the Browser Bookmarklet
=============================

First, run the gitmark_web server:

python gitmark_web.py

Then, go to the following URL and drag the bookmarklet into your browser's toolbar:

http://localhost:44865/

(where 44865 is the port you set in settings.py)


=======
License
=======

Copyright 2010 Hilary Mason.

This program is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with this program.  If not, see <http://www.gnu.org/licenses/>.

gitmarks_2
------------------------------------------------------------------------------------------------
**Title**: gitmarks_2

**Web address**: https://api.github.com/repos/tlevine/gitmarks_2

**Date**: December 2012

**Description**: ========
Gitmarks
========
Gitmarks is:

* A peer to peer web bookmark manager. This tool is a way to privately share bookmarks to your friends, without using a centralized server.  

* A local webpage cache system, so you can track websites over time, and keep copies of old content, along with an md5 hash, and other metadata to verify that the page is what you expect/know.

* A tool to pull bookmarks from a system like Delicious, and save them into gitmarks, so you do not lose existing bookmarks you may have saved in other systems. 

=======
Quickstart:
=======

1) Download gitmarks:
The best way is to get it via git.  Browse to a diretory to install it in, and run:  
 'git clone git://github.com/FarMcKon/gitmarks_2.git'

2) Run setup:
To setup what repository system you want to use for storing your gitmarks, you will need to run the setup program. You can do that at the command line (in your gitmarks code directory) by running.
  'python config.py'
You will be promoted to create a github account and directories if you use defaults. 

3) Start adding bookmarks!
You can import your delicious bookmarks via 
 'python delicious_imports.py'
Or you can add bookmarks directly at the command line by running 
 'gitmark.py [options] uri'

====
Details
====
Gitmarks uses 2 git repositories for your bookmarks. One stores public bookmarks (called 'public' ) and the second stores private bookmarks ( called 'private') a 3rd optional repository 'cache' can be used to store a local cache of all of your bookmark files.  The cache will be update <TBD>

Each time you bookmark a URL gitmarks will:
 - Create a bookmark file from the bookmark and tags
 - Generate a bit of bookmark metadata (you can tweak how much) in the 'tags' directory.
 - Cache a local file of that page (if you have cache enabled)
 - If you have more than 20 un-committed changes, gitmarks will commit those changes to the local repository.
 - If you bookmark something to a person, it will send your bookmark (unencrypted for now) to that 
 person. 
 
 You can use git as usual to see who committed what and when, or you can grep your way to bookmark happiness on the command line.

It's great for groups to collaboratively collect bookmarks in one spot (thanks to git itself!)


=====
Usage
=====

python gitmark.py [url]

options:
	-p = do not push to origin (store bookmark locally only)
	-m = description of the bookmark
	-t = a comma-delimited list of tags
	
Example:

python gitmark.py -m 'my site' -t me,hilary_mason,code,bookmarks http://www.hilarymason.com






===============
OMG Delicious?!
===============

Yes, you can import your delicious bookmarks!

Usage:

python delicious_import.py [username] [password]

(be patient if you have a lot of them.)


=============================
Using the Browser Bookmarklet
=============================

First, run the gitmark_web server:

python gitmark_web.py

Then, go to the following URL and drag the bookmarklet into your browser's toolbar:

http://localhost:44865/

(where 44865 is the port you set in settings.py)


=======
License
=======
Copyright 2011 Far McKon.  Based on code that is Copyright 2010 Hilary Mason.

This program is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with this program.  If not, see <http://www.gnu.org/licenses/>.


glacier
------------------------------------------------------------------------------------------------
**Title**: glacier

**Web address**: https://api.github.com/repos/tlevine/glacier

**Date**: August 2012

**Description**:        _            _           
      | |          (_)          
  __ _| | __ _  ___ _  ___ _ __ 
 / _` | |/ _` |/ __| |/ _ \ '__|
| (_| | | (_| | (__| |  __/ |   
 \__, |_|\__,_|\___|_|\___|_|   
  __/ |                         
 |___/    

A Python library for accessing Amazon Glacier

Requires Python 2.5 or later (earlier versions not tested).
No Python 3 support yet.

Website: http://glacier.github.com
Git Repository: http://github.com/paulengstler/glacier.git

The library is still work in progress and not complete but I'm working on it.
If you want to contribute feel free to do so and fork glacier on github!


guacamole
------------------------------------------------------------------------------------------------
**Title**: guacamole

**Web address**: https://api.github.com/repos/tlevine/guacamole

**Date**: April 2013

**Description**: Guacamole
=====
Based on the prices of guacamole ingredients in New York over time, I generate
guacamole consistently priced guacamole recipes by month, which adapt a
standard guacamole recipe to the changes in ingredient prices.

## Recipes
Here are some guacamole recipes off which I could base the standard guacamole
recipe.


## Ingredients
Here are some ingredients whose prices I would need.

* avacado
* lime
* salt
* cumin
* cayenne
* onion
* jalapeno pepper
* tomato
* garlic
* cilantro


hammock
------------------------------------------------------------------------------------------------
**Title**: hammock

**Web address**: https://api.github.com/repos/tlevine/hammock

**Date**: March 2013

**Description**: Hammock
====

Hammock is a ReST server in Haskell. You specify table schemas and validation rules,
and the resulting endpoints are created.

Data are sent as JSON lists of dicts, at least for now.

Data are stored in the server in an SQLite database. Or maybe acid-state.

GET, POST, PUT and PATCH requests are supported for ordinary tables.

    GET   /table -> SELECT * FROM table
    POST  /table -> INSERT INTO table
    PUT   /table -> UPDATE table
    PATCH /table -> UPDATE table

GET requests are supported for SQLite views.


HaskellNet
------------------------------------------------------------------------------------------------
**Title**: HaskellNet

**Web address**: https://api.github.com/repos/tlevine/HaskellNet

**Date**: July 2013

**Description**: HaskellNet
==========

This package provides client support for the E-mail protocols POP3,
SMTP, and IMAP.

Some examples of how to use the library are contained in the example/
directory.  You should be able to run them by adjusting the file for
your mail server settings and then loading the file in ghci and type
'main'. eg.

  ghci -hide-package monads-fd example/smtpMimeMail.hs
  main
  
If you encounter problems and want to debug the ghci 
debugger works well:

  :set -fbreak-on-exception
  :trace main


haskore-sandbox
------------------------------------------------------------------------------------------------
**Title**: haskore-sandbox

**Web address**: https://api.github.com/repos/tlevine/haskore-sandbox

**Date**: April 2013

**Description**: 

heroku-proxy
------------------------------------------------------------------------------------------------
**Title**: heroku-proxy

**Web address**: https://api.github.com/repos/tlevine/heroku-proxy

**Date**: June 2012

**Description**: A really simple node.js round-robin proxy meant to run on Heroku to load balance AWS elastic search instances.

To configure, make sure the ELASTIC_SEARCH_SERVERS is set:
ELASTIC_SEARCH_SERVERS="SOME.IP.ADDRESS.compute-1.amazonaws.com,ANOTHER.IP.ADDRESS.compute-1.amazonaws.com"

To run locally:
ELASTIC_SEARCH_SERVERS="SOME.IP.ADDRESS.compute-1.amazonaws.com,ANOTHER.IP.ADDRESS.compute-1.amazonaws.com" node server.js 


Todo
Remove a dead server from the pool

heroku-proxy-flask
------------------------------------------------------------------------------------------------
**Title**: heroku-proxy-flask

**Web address**: https://api.github.com/repos/tlevine/heroku-proxy-flask

**Date**: April 2013

**Description**: 

hh-ba-media-party-keynote
------------------------------------------------------------------------------------------------
**Title**: hh-ba-media-party-keynote

**Web address**: https://api.github.com/repos/tlevine/hh-ba-media-party-keynote

**Date**: August 2012

**Description**: Tom's keynote for [this](hhba.info/?p=3677)


highwall-fixtures
------------------------------------------------------------------------------------------------
**Title**: highwall-fixtures

**Web address**: https://api.github.com/repos/tlevine/highwall-fixtures

**Date**: February 2012

**Description**: Fixtures for [Highwall](https://github.com/tlevine/highwall) tests


hipstogram
------------------------------------------------------------------------------------------------
**Title**: hipstogram

**Web address**: https://api.github.com/repos/tlevine/hipstogram

**Date**: August 2012

**Description**: Hipstogram
======

> [Big Data Hipster](https://twitter.com/#!/bigdatahipster/status/202879778313867264): [@beaucronin](https://twitter.com/beaucronin) Pretty sure the hipstogram is a better visualization tool than the histogram. It's a photo of a histogram taken with Hipstamatic

    source('https://raw.github.com/tlevine/hipstogram/master/hipstogram.r')
    hipstogram(rnorm(42), filename='hipstogram.png')


history-analysis
------------------------------------------------------------------------------------------------
**Title**: history-analysis

**Web address**: https://api.github.com/repos/tlevine/history-analysis

**Date**: June 2013

**Description**: Tom analyzes his history files
=========



history-explorer
------------------------------------------------------------------------------------------------
**Title**: history-explorer

**Web address**: https://api.github.com/repos/tlevine/history-explorer

**Date**: March 2013

**Description**: History Explorer
=========
I save my terminal histories
([example](https://github.com/tlevine/.prophyl-teh-awesum/blob/master/source/history-sh)).
Here are some tools for exploring them.


housekeeping-scripts
------------------------------------------------------------------------------------------------
**Title**: housekeeping-scripts

**Web address**: https://api.github.com/repos/tlevine/housekeeping-scripts

**Date**: May 2013

**Description**: 

how-to-scrape
------------------------------------------------------------------------------------------------
**Title**: how-to-scrape

**Web address**: https://api.github.com/repos/tlevine/how-to-scrape

**Date**: January 2012

**Description**: 

hth
------------------------------------------------------------------------------------------------
**Title**: hth

**Web address**: https://api.github.com/repos/tlevine/hth

**Date**: August 2013

**Description**: 

HTML9-Responsive-Boilerstrap-js
------------------------------------------------------------------------------------------------
**Title**: HTML9-Responsive-Boilerstrap-js

**Web address**: https://api.github.com/repos/tlevine/HTML9-Responsive-Boilerstrap-js

**Date**: May 2012

**Description**: # [HTML9 Responsive Boilerstrap JS](http://html9responsiveboilerstrapjs.com)
===============================

Boom. Cross-universe compatible.

htmltable2matrix
------------------------------------------------------------------------------------------------
**Title**: htmltable2matrix

**Web address**: https://api.github.com/repos/tlevine/htmltable2matrix

**Date**: January 2012

**Description**: 

hub
------------------------------------------------------------------------------------------------
**Title**: hub

**Web address**: https://api.github.com/repos/tlevine/hub

**Date**: August 2013

**Description**: git + hub = github
==================

hub is a command line tool that wraps `git` in order to extend it with extra
features and commands that make working with GitHub easier.

~~~ sh
$ hub clone rtomayko/tilt

# expands to:
$ git clone git://github.com/rtomayko/tilt.git
~~~

hub is best aliased as `git`, so you can type `$ git <command>` in the shell and
get all the usual `hub` features. See "Aliasing" below.


Installation
------------

Dependencies:

* **git 1.7.3** or newer
* **Ruby 1.8.6** or newer

### Homebrew

Installing on OS X is easiest with Homebrew:

~~~ sh
$ brew install hub
~~~

### Standalone

`hub` is easily installed as a standalone script:

~~~ sh
$ curl http://hub.github.com/standalone -sLo ~/bin/hub &&
  chmod +x ~/bin/hub
~~~

Assuming "~/bin/" is in your `$PATH`, you're ready to roll:

~~~ sh
$ hub version
git version 1.7.6
hub version 1.8.3
~~~

#### On Windows

If you have mysysgit, open "Git Bash" and follow the steps above but put the
`hub` executable in `/bin` instead of `~/bin`.

Avoid aliasing hub as `git` due to the fact that mysysgit automatically
configures your prompt to include git information, and you want to avoid slowing
that down. See [Is your shell prompt slow?](#is-your-shell-prompt-slow)

### RubyGems

Though not recommended, hub can also be installed as a RubyGem:

~~~ sh
$ gem install hub
~~~

(It's not recommended for casual use because of the RubyGems startup
time. See [this gist][speed] for information.)

#### Standalone via RubyGems

~~~ sh
$ gem install hub
$ hub hub standalone > ~/bin/hub && chmod +x ~/bin/hub
~~~

This installs a standalone version which doesn't require RubyGems to
run, so it's faster.

### Source

You can also install from source:

~~~ sh
$ git clone git://github.com/github/hub.git
$ cd hub
$ rake install prefix=/usr/local
~~~

### Help! It's slow!

#### Is `hub` noticeably slower than plain git?

That is inconvenient, especially if you want to alias hub as `git`. Few things
you can try:

* Find out which ruby is used for the hub executable:

    ``` sh
    head -1 `which hub`
    ```

* That ruby should be speedy. Time it with:

    ``` sh
    time /usr/bin/ruby -e0
    #=> it should be below 0.01 s total
    ```

* Check that Ruby isn't loading something shady:

    ``` sh
    echo $RUBYOPT
    ```

* Check your [GC settings][gc]

General recommendation: you should change hub's shebang line to run with system
ruby (usually `/usr/bin/ruby`) instead of currently active ruby (`/usr/bin/env
ruby`). Also, Ruby 1.8 is speedier than 1.9.

#### Is your shell prompt slow?

Does your prompt show git information? Hub may be slowing down your prompt.

This can happen if you've aliased hub as `git`. This is fine when you use `git`
manually, but may be unacceptable for your prompt, which doesn't need hub
features anyway!

The solution is to identify which shell functions are calling `git`, and replace
each occurrence of that with `command git`. This is a shell feature that enables
you to call a command directly and skip aliases and functions wrapping it.


Aliasing
--------

Using hub feels best when it's aliased as `git`. This is not dangerous; your
_normal git commands will all work_. hub merely adds some sugar.

`hub alias` displays instructions for the current shell. With the `-s` flag, it
outputs a script suitable for `eval`.

You should place this command in your `.bash_profile` or other startup script:

~~~ sh
eval "$(hub alias -s)"
~~~

### Shell tab-completion

hub repository contains tab-completion scripts for bash and zsh. These scripts
complement existing completion scripts that ship with git.

* [hub bash completion](https://github.com/github/hub/blob/master/etc/hub.bash_completion.sh)
* [hub zsh completion](https://github.com/github/hub/blob/master/etc/hub.zsh_completion)


Commands
--------

Assuming you've aliased hub as `git`, the following commands now have
superpowers:

### git clone

    $ git clone schacon/ticgit
    > git clone git://github.com/schacon/ticgit.git

    $ git clone -p schacon/ticgit
    > git clone git@github.com:schacon/ticgit.git

    $ git clone resque
    > git clone git@github.com/YOUR_USER/resque.git

### git remote add

    $ git remote add rtomayko
    > git remote add rtomayko git://github.com/rtomayko/CURRENT_REPO.git

    $ git remote add -p rtomayko
    > git remote add rtomayko git@github.com:rtomayko/CURRENT_REPO.git

    $ git remote add origin
    > git remote add origin git://github.com/YOUR_USER/CURRENT_REPO.git

### git fetch

    $ git fetch mislav
    > git remote add mislav git://github.com/mislav/REPO.git
    > git fetch mislav

    $ git fetch mislav,xoebus
    > git remote add mislav ...
    > git remote add xoebus ...
    > git fetch --multiple mislav xoebus

### git cherry-pick

    $ git cherry-pick http://github.com/mislav/REPO/commit/SHA
    > git remote add -f mislav git://github.com/mislav/REPO.git
    > git cherry-pick SHA

    $ git cherry-pick mislav@SHA
    > git remote add -f mislav git://github.com/mislav/CURRENT_REPO.git
    > git cherry-pick SHA

    $ git cherry-pick mislav@SHA
    > git fetch mislav
    > git cherry-pick SHA

### git am, git apply

    $ git am https://github.com/defunkt/hub/pull/55
    > curl https://github.com/defunkt/hub/pull/55.patch -o /tmp/55.patch
    > git am /tmp/55.patch

    $ git am --ignore-whitespace https://github.com/davidbalbert/hub/commit/fdb9921
    > curl https://github.com/davidbalbert/hub/commit/fdb9921.patch -o /tmp/fdb9921.patch
    > git am --ignore-whitespace /tmp/fdb9921.patch

    $ git apply https://gist.github.com/8da7fb575debd88c54cf
    > curl https://gist.github.com/8da7fb575debd88c54cf.txt -o /tmp/gist-8da7fb575debd88c54cf.txt
    > git apply /tmp/gist-8da7fb575debd88c54cf.txt

### git fork

    $ git fork
    [ repo forked on GitHub ]
    > git remote add -f YOUR_USER git@github.com:YOUR_USER/CURRENT_REPO.git

### git pull-request

    # while on a topic branch called "feature":
    $ git pull-request
    [ opens text editor to edit title & body for the request ]
    [ opened pull request on GitHub for "YOUR_USER:feature" ]

    # explicit title, pull base & head:
    $ git pull-request -m "Implemented feature X" -b defunkt:master -h mislav:feature

    $ git pull-request -i 123
    [ attached pull request to issue #123 ]

### git checkout

    $ git checkout https://github.com/defunkt/hub/pull/73
    > git remote add -f -t feature git://github:com/mislav/hub.git
    > git checkout --track -B mislav-feature mislav/feature

    $ git checkout https://github.com/defunkt/hub/pull/73 custom-branch-name

### git merge

    $ git merge https://github.com/defunkt/hub/pull/73
    > git fetch git://github.com/mislav/hub.git +refs/heads/feature:refs/remotes/mislav/feature
    > git merge mislav/feature --no-ff -m 'Merge pull request #73 from mislav/feature...'

### git create

    $ git create
    [ repo created on GitHub ]
    > git remote add origin git@github.com:YOUR_USER/CURRENT_REPO.git

    # with description:
    $ git create -d 'It shall be mine, all mine!'

    $ git create -p
    [ private repo created on GitHub ]
    > git remote add origin git@github.com:YOUR_USER/CURRENT_REPO.git

    $ git create recipes
    [ repo created on GitHub ]
    > git remote add origin git@github.com:YOUR_USER/recipes.git

    $ git create sinatra/recipes
    [ repo created in GitHub organization ]
    > git remote add origin git@github.com:sinatra/recipes.git

### git init

    $ git init -g
    > git init
    > git remote add origin git@github.com:YOUR_USER/REPO.git

### git push

    $ git push origin,staging,qa bert_timeout
    > git push origin bert_timeout
    > git push staging bert_timeout
    > git push qa bert_timeout

### git browse

    $ git browse
    > open https://github.com/YOUR_USER/CURRENT_REPO

    $ git browse -- commit/SHA
    > open https://github.com/YOUR_USER/CURRENT_REPO/commit/SHA

    $ git browse -- issues
    > open https://github.com/YOUR_USER/CURRENT_REPO/issues

    $ git browse schacon/ticgit
    > open https://github.com/schacon/ticgit

    $ git browse schacon/ticgit commit/SHA
    > open https://github.com/schacon/ticgit/commit/SHA

    $ git browse resque
    > open https://github.com/YOUR_USER/resque

    $ git browse resque network
    > open https://github.com/YOUR_USER/resque/network

### git compare

    $ git compare refactor
    > open https://github.com/CURRENT_REPO/compare/refactor

    $ git compare 1.0..1.1
    > open https://github.com/CURRENT_REPO/compare/1.0...1.1

    $ git compare -u fix
    > (https://github.com/CURRENT_REPO/compare/fix)

    $ git compare other-user patch
    > open https://github.com/other-user/REPO/compare/patch

### git submodule

    $ hub submodule add wycats/bundler vendor/bundler
    > git submodule add git://github.com/wycats/bundler.git vendor/bundler

    $ hub submodule add -p wycats/bundler vendor/bundler
    > git submodule add git@github.com:wycats/bundler.git vendor/bundler

    $ hub submodule add -b ryppl --name pip ryppl/pip vendor/pip
    > git submodule add -b ryppl --name pip git://github.com/ryppl/pip.git vendor/pip

### git ci-status

    $ hub ci-status [commit]
    > (prints CI state of commit and exits with appropriate code)
    > One of: success (0), error (1), failure (1), pending (2), no status (3)


### git help

    $ git help
    > (improved git help)
    $ git help hub
    > (hub man page)


Configuration
-------------

### GitHub OAuth authentication

Hub will prompt for GitHub username & password the first time it needs to access
the API and exchange it for an OAuth token, which it saves in "~/.config/hub".

### HTTPS instead of git protocol

If you prefer using the HTTPS protocol for GitHub repositories instead of the git
protocol for read and ssh for write, you can set "hub.protocol" to "https".

~~~ sh
# default behavior
$ git clone defunkt/repl
< git clone >

# opt into HTTPS:
$ git config --global hub.protocol https
$ git clone defunkt/repl
< https clone >
~~~


Contributing
------------

These instructions assume that _you already have hub installed_ and aliased as
`git` (see "Aliasing").

1. Clone hub:  
    `git clone github/hub && cd hub`
1. Ensure Bundler is installed:  
    `which bundle || gem install bundler`
1. Install development dependencies:  
    `bundle install`
2. Verify that existing tests pass:  
    `bundle exec rake`
3. Create a topic branch:  
    `git checkout -b feature`
4. **Make your changes.** (It helps a lot if you write tests first.)
5. Verify that tests still pass:  
    `bundle exec rake`
6. Fork hub on GitHub (adds a remote named "YOUR_USER"):  
    `git fork`
7. Push to your fork:  
    `git push -u YOUR_USER feature`
8. Open a pull request describing your changes:  
    `git pull-request`


Meta
----

* Home: <https://github.com/github/hub>
* Bugs: <https://github.com/github/hub/issues>
* Gem: <https://rubygems.org/gems/hub>
* Authors: <https://github.com/github/hub/contributors>

### Prior art

These projects also aim to either improve git or make interacting with
GitHub simpler:

* [eg](http://www.gnome.org/~newren/eg/)
* [github-gem](https://github.com/defunkt/github-gem)


[speed]: http://gist.github.com/284823
[gc]: https://twitter.com/brynary/status/49560668994674688


human-trafficking
------------------------------------------------------------------------------------------------
**Title**: human-trafficking

**Web address**: https://api.github.com/repos/tlevine/human-trafficking

**Date**: February 2013

**Description**: # Human trafficking search

Download the advertisements and the phone numbers, combine everything into
one big huge SQLite database, run a big query and save it as a spreadsheet.

Install dependencies

    npm install

Then run.

    # Download and join and stuff
    run

    # Serve
    ./app.coffee


hyde-h5bp
------------------------------------------------------------------------------------------------
**Title**: hyde-h5bp

**Web address**: https://api.github.com/repos/tlevine/hyde-h5bp

**Date**: November 2011

**Description**: 

instant-runoff
------------------------------------------------------------------------------------------------
**Title**: instant-runoff

**Web address**: https://api.github.com/repos/tlevine/instant-runoff

**Date**: June 2013

**Description**: 

interactive-data
------------------------------------------------------------------------------------------------
**Title**: interactive-data

**Web address**: https://api.github.com/repos/tlevine/interactive-data

**Date**: April 2013

**Description**: Interactive Data
===

Basu [tells me](https://www.facebook.com/photo.php?fbid=10151604466650452&set=p.10151604466650452&type=1)
that I'm [giving a talk](https://sphotos-a.xx.fbcdn.net/hphotos-prn1/64128_10151604466650452_924988096_n.jpg) ([mirror](poster.jpg)).
So here are some slides.

## Outline
I account for 20 minutes, but will probably be more like 16 minutes. *Use a timer while giving the presentation.*

* OMG Data (2 minutes)
* Is this interactive?
  * Introduce (0.5 minutes)
  * For each of some examples, raise your hand for yes, no, or stupid question (3 minutes, that is, 40 seconds each)
  * For each of the same examples, ask people why they thought so. (8 minutes, that is, 2 minutes each, with 40 seconds per answer)
    * Ask one person for yes, one person for no and one person for neither
* Flashy pseudo interactivity (1.5 minutes)
  * Show a few examples
* I think interactivity is about finding your own stories. (3 minutes, that is, 1 minute each)
  * News apps
  * Static images that do this
  * ...
* Conslusionary slide with the main points (1 minute)

## How to
Edit the slides by editing `slides.md`. Vertically-separated slides are
separated by three line breaks, and horizontally-separated slides are separated
by four line breaks.

Serve the current directory on a web server.


interactive-data-talk
------------------------------------------------------------------------------------------------
**Title**: interactive-data-talk

**Web address**: https://api.github.com/repos/tlevine/interactive-data-talk

**Date**: March 2013

**Description**: Interactive Data
====
This is for the "Future of Interaction" symposium at Cornell University on
Thursday, April 4, 2013.

Things to consider talking about.

* "Data"?
  * Today, it's cheap to collect information, store it in computers, and use
      math to learn something from the information.
    * Examples
* "Interactive"? Answer with yes, no or an abstention
    (irrelevent, don't care, not a useful dichotomy)
  * Is this ordinary plot interactive?
  * Is fancy data journaly thing interactive?
  * What about this ordinary plot that moves uselessly when you click on it?
  * What about this static plot that conveys stories?
* More data interactives
  *
  *
  *
* Aside from their utility, "data" and empiricism are sexy.
  * Popular articles
    * Harvard Business Review
    * Nate Silver
    * White House
    * Data scientology
  * How to make your organization sound sexy
    * Use the phrase "big data" or "data science" when you think you should say data.
    * Use the word "data" whenever you can, even if you think it's not data.
    * Examples
  * My band CSV Soundsystem


internet-play
------------------------------------------------------------------------------------------------
**Title**: internet-play

**Web address**: https://api.github.com/repos/tlevine/internet-play

**Date**: November 2012

**Description**: 

intherooms
------------------------------------------------------------------------------------------------
**Title**: intherooms

**Web address**: https://api.github.com/repos/tlevine/intherooms

**Date**: March 2013

**Description**: In The Rooms meetings database
===

I got locations from here.
http://meetings.intherooms.com/meetings/search

## Report on the full dataset.
I have a complete-enough database of the In The Rooms meetings for
the continental United States. It also happens to include some
meetings in Mexico and Canada. It's [here](http://chainsaw.thomaslevine.com/intherooms.db).

The schema hasn't remarkably changed since when I explained it to
Samir before. I think that Addicaid needs only to concern itself
with the `meeting` view and the `location` table.

### Coverage
I accidentally deleted four of the meetings. If I run the job again
(not hard), I would get them back, but I don't think it's worth it.

That leaves 83760 meetings, of which 28 are broken on the In The
Rooms website, and 37293 locations, of which 5 are broken on the In
The Rooms website. This database thus contains 83732 meetings and
37288 locations.

### Correctness
I haven't reviewed this resulting dataset as much as I would like.
You could help with that by visiting the urls in these spreadsheets,
checking that the data in those urls match the data in the
spreadsheets, writing "yes" or "no" in the "Correct?" column and
then sending the result back to me.

* http://chainsaw.thomaslevine.com/intherooms-meeting-sample.csv
* http://chainsaw.thomaslevine.com/intherooms-location-sample.csv

In case you're curious, those spreadsheets are random samples.
https://github.com/tlevine/intherooms/blob/c73efa99c0e4ff772ed60d83b11a501f98d1991a/validity-test-sample.sh

### Tangential applications
I fixed some glaring errors in the In The Rooms information, so this
database cleaner in some ways than the one powering In The Rooms.
I wonder whether it could be helpful to In The Rooms.

Also, I'll probably eventually play with this dataset as I did
[with the New York data](http://thomaslevine.com/!/new-york-addiction-recovery-meetings/)

Tell me if there are any particular theories of addiction recovery
that I might be able to test or if there are any analyses that could
be useful to you.

### Logistics
Tell me if you think meeting in person would help. Or call me
whenever. I'm planning on going out of town this weekend and coming
back about a week after, but that's flexible.

## How to run

First, activate

    . activate

Now, you can run any of the scripts.

    sqlite3 intherooms.db < schema.sql
    ./download_searches.py
    ./download_meeting.sh [url]
    ./download_location.sh [url]
    ./parse_search.py intherooms.db [coordinates] [page number]
    ./parse_meeting.py intherooms.db [url]
    ./parse_location.py intherooms.db [url]

I made scripts for batch processing of the location and meeting pages.

    ./download_all_meetings.sh intherooms.db
    ./download_all_locations.sh intherooms.db
    ./parse_all_meetings.sh intherooms.db
    ./parse_all_locations.sh intherooms.db

Some manual fixes to weird pages are included.

    sqlite3 intherooms.db < manual_fixes.sql

Everything is wrapped up in one script.

    ./run

Generate a spreadsheet.

    sqlite3 -header -csv intherooms.db 'select * from meeting' > intherooms.csv

Or a geojson

    ./geojson.py intherooms.db > intherooms.json

Diagnose things

    ./counts.sh
    sqlite3 intherooms.db < todo.sql
    sqlite3 intherooms.db < interesting.sql
    sqlite3 intherooms.db 'SELECT page, count(*) FROM meeting_search GROUP BY page'
    ./validity-test-sample.sh

## Temporary notes
I accidentally deleted these records on the run of the week of February 25.
If I remove the `done-parsing` files and load the searches again, they will
come back, without tabs.

    [OrderedDict([(u'Meeting Title Link', u'/aa/ue\tpm0700-0800\tSpringfield-Study-Group\tSpringfield-\t/99181')]),
     OrderedDict([(u'Meeting Title Link', u'/alanon/\tOLD-TOWN-DAYTIME-AL-ANON/85924')]),
     OrderedDict([(u'Meeting Title Link', u'/na/Baja-Group--\t/85867')]),
     OrderedDict([(u'Meeting Title Link', u'/na/HOPE-II-\t/98904')])]

## Scope
The `robots.txt` permits crawling of the whole site.

    $ curl http://meetings.intherooms.com/robots.txt
    User-agent: *
    Allow: /

Here are my questions/considerations from when we first discussed this.

1. Can we do without the additional meeting information for now
    (`/oa`, `/na`, &c. endpoints)? I think we need it, and Addicaid agrees.
2. Do we need the additional location information for now (`/locations`
    endpoint)? Yes, mainly longitude and latitude.
3. The MVP will automatically update itself from In The Rooms; rather, the meetings
    will be imported from In The Rooms once, and we'll decide later whether and how
    to apply further updates.
4. Addicaid is fine with the resulting database being free and available to everyone
    in a convenient format?
5. The search is weird. In particular, you only get 400 pages at once (easy to
    get around), and you need to specify a boundary on the location (annoying).
    It will be faster if we limit this to a few specific locations at first.
    Can we choose these?
  * Tom started with New York.
  * Then Tom did San Francisco for the demo.
  * Then Tom did the whole country.
6. Tom may request Addicaid's help for writing test fixtures and for other less
    specialized tasks.
7. Consider telling In The Rooms about the MVP as a way to convince them that
    it will be useful for them to provide Addicaid with the data. (Addicaid
    had already contacted In The Rooms about getting the data, but they didn't
    hear back or something.)

## About In the Rooms
In the Rooms is run by
[Ken Pomerance](http://kenpomerance.com/)
("[Mr Clean](http://opencorporates.com/companies/us_fl/P06000144749)")
and RON "RT" TANNEBAUM
([two](http://100interviews.com/post/2050725736/71)
[interviews](http://www.recoverymonth.gov/Multimedia/Ask-the-Expert/Bio-Ronald-Tannebaum.aspx)).

Here are some more links

* [Open Corporates](http://opencorporates.com/companies/us_fl/P07000095303)
* [Corporation Wiki](http://www.corporationwiki.com/Florida/Plantation/ronald-d-tannebaum-P2117824.aspx)
* [Ken's Twitter](https://twitter.com/Mrclean1982)
* [Crunchbase](http://www.crunchbase.com/company/intherooms)


intherooms-analysis
------------------------------------------------------------------------------------------------
**Title**: intherooms-analysis

**Web address**: https://api.github.com/repos/tlevine/intherooms-analysis

**Date**: February 2013

**Description**: Welcome to ProjectTemplate!

This file introduces you to ProjectTemplate, but you should eventually replace
the contents of this file with an introduction to your project. People who
work with your data in the future will thank you for it, including your future
self.

ProjectTemplate is an R package that helps you organize your statistical
analysis projects. Since you're reading this file, we'll assume that you've
already called `create.project()` to set up this project and all of its
contents.

To load your new project, you'll first need to `setwd()` into the directory
where this README file is located. Then you need to run the following two
lines of R code:

	library('ProjectTemplate')
	load.project()

After you enter the second line of code, you'll see a series of automated
messages as ProjectTemplate goes about doing its work. This work involves:
* Reading in the global configuration file contained in `config`.
* Loading any R packages you listed in he configuration file.
* Reading in any datasets stored in `data` or `cache`.
* Preprocessing your data using the files in the `munge` directory.

Once that's done, you can execute any code you'd like. For every analysis
you create, we'd recommend putting a separate file in the `src` directory.
If the files start with the two lines mentioned above:

	library('ProjectTemplate')
	load.project()

You'll have access to all of your data, already fully preprocessed, and
all of the libraries you want to use.

For more details about ProjectTemplate, see http://projecttemplate.net


intherooms-locations
------------------------------------------------------------------------------------------------
**Title**: intherooms-locations

**Web address**: https://api.github.com/repos/tlevine/intherooms-locations

**Date**: February 2013

**Description**: intherooms-locations
====================

intherooms-locations-ny
------------------------------------------------------------------------------------------------
**Title**: intherooms-locations-ny

**Web address**: https://api.github.com/repos/tlevine/intherooms-locations-ny

**Date**: December 2012

**Description**: 

intherooms-locations-sf
------------------------------------------------------------------------------------------------
**Title**: intherooms-locations-sf

**Web address**: https://api.github.com/repos/tlevine/intherooms-locations-sf

**Date**: January 2013

**Description**: San francisco meetings


intherooms-meetings
------------------------------------------------------------------------------------------------
**Title**: intherooms-meetings

**Web address**: https://api.github.com/repos/tlevine/intherooms-meetings

**Date**: February 2013

**Description**: intherooms-meetings
===================

intherooms-meetings-ny
------------------------------------------------------------------------------------------------
**Title**: intherooms-meetings-ny

**Web address**: https://api.github.com/repos/tlevine/intherooms-meetings-ny

**Date**: December 2012

**Description**: 

intherooms-meetings-sf
------------------------------------------------------------------------------------------------
**Title**: intherooms-meetings-sf

**Web address**: https://api.github.com/repos/tlevine/intherooms-meetings-sf

**Date**: January 2013

**Description**: San francisco meetings


intherooms-searches-ny
------------------------------------------------------------------------------------------------
**Title**: intherooms-searches-ny

**Web address**: https://api.github.com/repos/tlevine/intherooms-searches-ny

**Date**: December 2012

**Description**: 

intherooms-searches-sf
------------------------------------------------------------------------------------------------
**Title**: intherooms-searches-sf

**Web address**: https://api.github.com/repos/tlevine/intherooms-searches-sf

**Date**: January 2013

**Description**: San francisco meetings


intherooms-searches-usa
------------------------------------------------------------------------------------------------
**Title**: intherooms-searches-usa

**Web address**: https://api.github.com/repos/tlevine/intherooms-searches-usa

**Date**: February 2013

**Description**: 

jsonp-client
------------------------------------------------------------------------------------------------
**Title**: jsonp-client

**Web address**: https://api.github.com/repos/tlevine/jsonp-client

**Date**: July 2013

**Description**: # jsonp-client

[![Build Status](https://secure.travis-ci.org/bermi/jsonp-client.png?branch=master)](http://travis-ci.org/bermi/jsonp-client) [![Dependency Status](https://david-dm.org/bermi/jsonp-client/status.png)](http://david-dm.org/bermi/jsonp-client)

jsonp minimal client for the browser (1.4K or 0.74K gzipped) and Node.js

On Node.js jsonp JavaScript code will run on a sandbox.

## Installation

    $ npm install jsonp-client

## Usage

Include the library on Node.js

    var jsonpClient = require('jsonp-client');

or include the script for browser usage

     <script src="https://raw.github.com/bermi/jsonp-client/master/dist/jsonp-client.min.js" type="text/javascript"></script>

On the browser you must supply a valid callback on the URL. On Node.js the callback
will be taken from the script contents.

Single jsonp resource URL

    jsonpClient.get(url, function (err, data) {
    });

Multiple URLs

    jsonpClient.get(url1, url2, function (err, data1, data2) {
    });

or as an array of URLs

    jsonpClient.get([url1, url2], function (err, data1, data2) {
    });

## Testing

    $ make test

### On the browser

    $ make test-browser

### Code coverage

You will need to install https://github.com/visionmedia/node-jscoverage
and then run

    $ make test-coverage

## Development watcher and test runner

### Continuous linting

    $ make dev

### Continuous testing

    $ make test-watch

### Continuous linting + testing

    $ make dev-test


## License

(The MIT License)

Copyright (c) 2013 Bermi Ferrer &lt;bermi@bermilabs.com&gt;

Permission is hereby granted, free of charge, to any person obtaining
a copy of this software and associated documentation files (the
'Software'), to deal in the Software without restriction, including
without limitation the rights to use, copy, modify, merge, publish,
distribute, sublicense, and/or sell copies of the Software, and to
permit persons to whom the Software is furnished to do so, subject to
the following conditions:

The above copyright notice and this permission notice shall be
included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED 'AS IS', WITHOUT WARRANTY OF ANY KIND,
EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

junar-api-python-client
------------------------------------------------------------------------------------------------
**Title**: junar-api-python-client

**Web address**: https://api.github.com/repos/tlevine/junar-api-python-client

**Date**: November 2012

**Description**: # Junar API Python Client #
Unofficial API Python Client implementation for the Junar.com API, a service to collect, organize, use and share data.

## Usage ##

    from junar import ApiClient
    junar_api_client = ApiClient('MY_AUTH_KEY')
    datastream = junar_api_client.datastream('GUID')
    print datastream.invoke(output = 'json_array')

## License ##
(Released under MIT License since v0.0.1)

Copyright (c) 2011 JoaquAn NAoA+-ez <josejnv[at]gmail.com>

Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the 'Software'), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED 'AS IS', WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.


juvenile-fish-abundance
------------------------------------------------------------------------------------------------
**Title**: juvenile-fish-abundance

**Web address**: https://api.github.com/repos/tlevine/juvenile-fish-abundance

**Date**: October 2013

**Description**: This accompanies a blog post for DataKind.

https://data.wa.gov/dataset/Puget-Sound-Juvenile-Abundance-10302012/p6mz-hz4g?
https://data.wa.gov/api/views/p6mz-hz4g/rows.csv?accessType=DOWNLOAD

https://fortress.wa.gov/dfw/gispublic/apps/salmonscape/salmonscapeJSP/summaryStockReport.jsp?SasiStkNum=1715
http://stateofsalmon.wa.gov/statewide/fish-summary


kryptonite
------------------------------------------------------------------------------------------------
**Title**: kryptonite

**Web address**: https://api.github.com/repos/tlevine/kryptonite

**Date**: June 2013

**Description**: Kryptonite
=====

**Alex**: {{contextual alert safety paragraph}}

**Sam (demo)**:
We thought we could do better, so we created Kryptonite,
a violent crime alert and witness reporting system. To a civilian,
Kryptonite is a context-sensitive alert app that
she can install to on her phone. Kryptonite uses GPS to
see where the civilian is. If a violent crime happens nearby,
the civilian receives an alert from the police in real time.
Civilians install the app so they can stay safe.
But that's not all.

**Tom**: There was another interesting thing about the Boston shooting incident.
Through major advances in technology (blah blah),
we now live in a world where ordinary people are
constantly producing digital records of the world
around them. This situation has enabled the use
crowdsourcing to solve problems that could not be
solved before. And crowdsourcing is catching on
in law-enforcement. For example, law-enforcement
officials used social media data in the Boston
marathon investigation.
http://www.wired.com/dangerroom/2013/04/boston-crowdsourced/
http://www.computerworld.com/s/article/9238446/Boston_bombings_Forensics_on_crowdsourced_video_will_be_a_challenge

Law enforcement uses Kryptonite to crowdsource evidence.

**Tom**:
While they recognize the value of crowdsourced data,
the investigators did not have a good tool for
collecting the data; they simply published a press
release and set up a phone hotline.
http://www.fbi.gov/boston/press-releases/2013/fbi-assists-boston-police-department-regarding-the-explosions-along-the-marathon-route-and-remains-on-scene
http://www.bloomberg.com/news/2013-04-19/boston-s-surreal-crowdsourced-manhunt.html
http://petapixel.com/2013/04/17/boston-marathon-bombing-investigators-using-crowdsourced-photographs/
http://news.cnet.com/8301-13578_3-57579707-38/crowdsourced-videos-photos-could-aid-boston-blast-investigations/

**Sam (demo)**:
After the situation has cooled down, law enforcement can
send a request for information to the people who were near
the scene of the violent crime. Citizens are empowered to assist with the
investigation by calling a phone number, talking to an
investigator and providing any relevant information they have.

**Tom**:
You can think of Kryptonite as a targeted press release.
When law-enforcement needs to keep civilians away from a
dangerous area, it can alert the specific people who are
at risk. And when they need evidence for an investigation,
they can alert the specific people who are likely to have
the evidence.

**Alex**:
Kryptonite facilitates communication between law-enforcement
officials and civilians. It helps civilians stay clear of
dangerous situations, and it helps law-enforcement officials
innovate their investigation of violent crime.

**Sam**:
Future functionality
* Sending images
* Mining Google Latitude for past locations
* Integrating with existing crime alert systems to make it easier
    for law-enforcement organizations to adopt Kryptonite


lambdajs
------------------------------------------------------------------------------------------------
**Title**: lambdajs

**Web address**: https://api.github.com/repos/tlevine/lambdajs

**Date**: August 2013

**Description**: LambdaJS
========
The full ECMAScript API done a functional way.

## RULES

 1. The data comes last. E.g: str.method(arg) -> method(arg, str)
 2. Everything is curried
 3. Functions with optional arguments are split into two functions. One with `_` at the end that takes the options. E.g: `indexOf(x,str)` & `indexOf_(x,y,str)`
 4. Every function is pure

Thanks so much to @casperin for doing a ton of the work on this!

## USAGE

In the browser

```html
<script src="utils.js"></script>
<script src="lambda.js"></script>
<script>LambdaJS.expose();</script>
```

In node

```sh
npm install lambda
```

```javascript
require('pipeish');
LambdaJS.expose();
```


lastpass2upm
------------------------------------------------------------------------------------------------
**Title**: lastpass2upm

**Web address**: https://api.github.com/repos/tlevine/lastpass2upm

**Date**: December 2012

**Description**: lastpass2upm
====
Convert a [Lastpass](https://lastpass.com) database to a
[UPM](http://upm.sourceforge.net) database.

    # Specify a file
    lastpass2upm lastpass.csv > upm.csv

    # Or stdin
    cat lastpass.csv | lastpass2upm > upm.csv


lazydriver
------------------------------------------------------------------------------------------------
**Title**: lazydriver

**Web address**: https://api.github.com/repos/tlevine/lazydriver

**Date**: January 2012

**Description**: Lazydriver
======
Lazydriver simplifies the building of a Chrome extension to scrape a site.

How it works
-------
Different sites will have different logic, so you need to write a script
to navigate the pages. 

It is possible to parse the pages as you are navigating them, but
I recommend simply saving the raw html and parsing it later. This gives
you a wider choice of approaches/libraries/languages for parsing and allows.

Writing the script
------
Write your script to navigate the pages. Lazydriver includes jQuery
automatically, so feel free to use it. Somewhere in that script,
you need to run save_page to push the text to a couch.
You may specify up to three options, `pageId`, `text` and `couch`. For
any that you don't specify, these defaults will be used.

    function save_page({
      pageId : document.URL
    , text   : document.documentElement.innerHTML
    , couch  : "http://localhost:5984/lazydriver"
    },callback)

In addition to writing a script to navigate the page, you need to write a
[manifest.json](http://code.google.com/chrome/extensions/manifest.html).

Building
-----------
Once you've finished the script and manifest.json,
run `build.sh <chrome extension dir>`, and an unpacked chrome extension
will appear in a `deploy` directory inside the chrome extension directory.

Then you can go to the [chrome extensions panel](chrome://settings/extensions)
to load the extension or pack it.


licences
------------------------------------------------------------------------------------------------
**Title**: licences

**Web address**: https://api.github.com/repos/tlevine/licences

**Date**: May 2013

**Description**: # ScraperWiki Box: scraperwiki/licences #

https://box.scraperwiki.com/scraperwiki/licences

This box contains scripts to check ScraperWiki repositories on Github have
explicit licences and to apply such licences.

## How to install (on a new server) ##

    $ git clone scraperwiki.licences@box.scraperwiki.com:. licences
    $ cd licences
    $ npm install
    $ echo "export PATH=$PATH:~/node_modules/.bin" >> ~/.profile

## How to use ##

Check licences.

    $ cd licences
    $ ./check_licences_in_github.coffee

Add licences to repositories that lack them.

    $ cd licences
    $ ./add_licence_to_repository.sh git@github.com:scraperwiki/licences.git


linkedin-profile
------------------------------------------------------------------------------------------------
**Title**: linkedin-profile

**Web address**: https://api.github.com/repos/tlevine/linkedin-profile

**Date**: March 2013

**Description**: First, I wanted to see what would happen if I created a meaning-void LinkedIn
profile of buzzwords.

Compile and run `big-data-scientist.hs`. It will give a list of meat words and
fascia words. These lists will be in a random order. Use these lists to compose
a meaningless bulleted description of your past experience.

1. Start the first bullet with a fascia word. You will follow it by a meat
    word, and you should adjust the parts of speech to make them fit.
2. Continue stringing meat words together, adjusting their parts of speech and
    adding stop words and other small words as you think makes sense.
3. If adjusting the parts of speech doesn't help, add a fascia word in between
    two meat words.
4. If adding the fascia word doesn't help connect adjacent meat words, end the
    current bullet at the first of the two meat words, and use the fascia word
    and the second of the meat words to start the next bullet.
5. Repeat steps 2 to 4 until you have four bullets.

Then, I realized that it might be sort of interesting to amass a large set of
recruiter spam and to figure out what sort of things recruiters search for on
LinkedIn, so now I'm just trying to get lots of recruiter spam.

I can specify 50 "skills". What should they be

* [Top 10 languages from GitHub](https://github.com/languages)
* GitHub hot searches
* Stack Overflow [top technologies](http://careers.stackoverflow.com/employer/search)

Social media things might be most hilarious because they are such nonsense.


lisp-sandbox
------------------------------------------------------------------------------------------------
**Title**: lisp-sandbox

**Web address**: https://api.github.com/repos/tlevine/lisp-sandbox

**Date**: December 2012

**Description**: 

mailfest-scoreboard
------------------------------------------------------------------------------------------------
**Title**: mailfest-scoreboard

**Web address**: https://api.github.com/repos/tlevine/mailfest-scoreboard

**Date**: March 2013

**Description**: Mailfest Scoreboard
======

**Mailfest** is a hypothetical email-writing event. People meet with food and
alcohol to write emails that they've been meaning to write in company of other
nice people. The company of others also helps them deal with difficult emails.

* People can work together on the composition of individual emails.
* People might discuss problems with email and create small tools to help with
    email composition.
* People can ask for help on a particular sort of email.

The **Mailfest Scoreboard** keeps track of how much has been accomplished.
Once it has been installed, mailfest participants just need to BCC all emails
that they write during the mailfest to the email address for which the
scoreboard has been configured. The scoreboard will produce statistics about
the emails that have been sent and about the people who have been sending them.

The emails are deleted immediately after they are read, and information about
recipients is deleted, but you should still avoid sending particularly secret
things to the mailfest scoreboard.

## Install
Specify the IMAP server credentials, and then run `./scoreboard`.

## Statistics
The following features are extracted from each email.

* `From` field
* Date
* Size of email, ignoring attachments
* Size of the `Subject`
* Number of attachments
* Number of recipients in the `To` field
* Number of recipients in the `Cc` field
* Whether the title starts with "Re:"
* Whether the title starts with "Fwd:"
* Whether it was sent from a Google Mail server

Whenever "size" is mentioned, both of the following metrics are collected.

* Number of characters
* Number of words

To be clear: The name and email address of the person who sent the email
will be recorded so that we can report things like the number of emails that
each person sent. Aside from this feature, the features shouldn't be able to
reveal much private information.


mechanize
------------------------------------------------------------------------------------------------
**Title**: mechanize

**Web address**: https://api.github.com/repos/tlevine/mechanize

**Date**: October 2011

**Description**: See INSTALL.txt for installation instructions.

See docs/html/index.html and docstrings for documentation.

If you have a git working tree rather than a release, you'll only have
the markdown source, e.g. mechanize/index.txt; release.py is used to
build the HTML docs.


medial_temporal_lobe
------------------------------------------------------------------------------------------------
**Title**: medial_temporal_lobe

**Web address**: https://api.github.com/repos/tlevine/medial_temporal_lobe

**Date**: April 2013

**Description**: Medial Temporal Lobe
==========
The Medial Temporal Lobe remembers websites for you.

    mtl [action] [url] [-ht] [--mtl-dir directory]

    -h  Help
    -t  Tags, comma-separated, allowed characters: [a-z_]

For now, the only `action` is `add`. I might implement `remove` and `show`
eventually. So you might do something like this.

    mtl add -t superheroes,pink,letterpress thomaslevine.com 

## Dependencies
Medial Temporal Lobe depends on uuidgen.

Websites are saved to the `~/.medial_temporal_lobe` directory, or to whatever
directory you specify with `--mtl-dir`. Either way, you must create it.

    mkdir ~/.medial_temporal_lobe

You might as well make it a git repository. If you initialize it and set and
upstream remote. The Medial Temporal Lobe won't synchronize it for you, or at
least not yet.

    cd ~/.medial_temporal_lobe
    git init
    git remote add origin [address]
    echo Memories > README
    git add README
    git commit README -m create\ repository
    git push -u origin master

## Technical stuff
Medial Temporal Lobe makes that directory looks like this.

    .medial_temporal_lobe/
        memories/
            7949356f-df36-4938-9dc2-424cf1d127e9/
                source
                info
        tags/
            cats/
                9e3b9b9e-2316-4bf5-bd11-7571af95d713/ -> ../../memories/9e3b9b9e-2316-4bf5-bd11-7571af95d713
            data/
                7949356f-df36-4938-9dc2-424cf1d127e9/ -> ../../memories/7949356f-df36-4938-9dc2-424cf1d127e9
            pizza/
                7949356f-df36-4938-9dc2-424cf1d127e9/ -> ../../memories/7949356f-df36-4938-9dc2-424cf1d127e9

The `source` of each memory is the source of whatever url was specified
(an ordinary GET request), and the `info` looks like this.

    url=http://thomaslevine.com/!/new-york-pizza
    added=2012-12-24 18:08:15.631858487-05:00

It might have other lines eventually.

Here's an [example](https://github.com/tlevine/.medial_temporal_lobe) of this
directory.


meetup-email
------------------------------------------------------------------------------------------------
**Title**: meetup-email

**Web address**: https://api.github.com/repos/tlevine/meetup-email

**Date**: May 2013

**Description**: Change Meetup email settings
====
This disables the emails that Tom finds annoying for all of your meetups.
If you want emails for some meetups, you could run this and then change those
later. Or you can extend this to handle the exceptions. Run it like so.

    export MEETUP_EMAIL_ADDRESS=abc@def.ghi
    export MEETUP_PASSWORD=aBcd3fGhi
    ./meetup_settings.py


meetup-users-meetup
------------------------------------------------------------------------------------------------
**Title**: meetup-users-meetup

**Web address**: https://api.github.com/repos/tlevine/meetup-users-meetup

**Date**: May 2012

**Description**: 

merry
------------------------------------------------------------------------------------------------
**Title**: merry

**Web address**: https://api.github.com/repos/tlevine/merry

**Date**: December 2012

**Description**: Someone was named "Merry". That's weird. Was that previously more popular?

1. `query.js` is a mapreduce on Tom's SSDMF Mongo database.
2. `merry.json` is the output.
3. `csv.js` converts that to csv. It doesn't quite work.
4. `merry.csv` is the csv
5. `merry.r` converts that to some plots.
6. `merry.pdf` is the plots.

The plots show that merry was particularly popular in the 1940s and 1950s.


meta-data-science-socrata
------------------------------------------------------------------------------------------------
**Title**: meta-data-science-socrata

**Web address**: https://api.github.com/repos/tlevine/meta-data-science-socrata

**Date**: August 2013

**Description**: Meta Data Science
==================
Thomas Levine


# Meta data science

Datasets on [Socrata portals](https://data.seattle.gov) feel like files rather than data.

* Data science about data science
* Science about metadata


# Outline

1. Data science mindset <!-- or just my mindset -->
2. What I did
3. What I learned <!-- construction and usage -->
4. Things to consider



# Data science mindset

Exploit cheap computers to study how the world works.

1. Store everything.
2. Anything can be counted.
3. Numbers can be turned into anything.
4. Boring work should be sent to robots.
5. Get more data rather than tuning your model.

<!--
And this is what I was doing with the Socrata data
-->


## Store everything
<!--
Storage is cheap, so you should store everything that is easy to collect.
Store it in the most raw form that is convenient, and don't worry very
much about how or even whether you're going to analyze it.
-->

* Storage is cheap.
* You don't need a full research plan.


## Anything can be counted

* [Scans of letters](http://scott.thomaslevine.com)
* [Bikes](http://www.capitalbikeshare.com/system-data)
* [Turnstiles](http://www.theatlanticcities.com/commute/2013/05/visualizing-impact-mega-storms-transit/5660/)
* [Comments on rulemaking dockets](http://docketwrench.sunlightfoundation.com/)
  <!-- http://overview.ap.org/blog/2013/05/video-text-analysis-in-transparency/ -->


## Numbers can be turned into anything

* [FMS Symphony](http://fms.csvsoundsystem.com)
* [Ridership Rachenitsa](http://thomaslevine.com/!/ridership-rachenitsa)
* [gastronomify](http://github.com/csv/gastronomify)
* [Data](https://twitter.com/melsmo/status/352240097049071616) [cookies](https://twitter.com/internetrebecca/status/352955293291913217)


## Boring work should be sent to robots

* Computers can perform mindless tasks <!-- , just like people can. -->
* Computers can also make complex decisions <!--, just like people can. -->
* All analyses should be scripted.


## Get more data rather than tuning your model

<!--
* When I'm asked a question about the world, I adapt the question so that it can
    be approximately answered with an existing and convenient dataset.
* I look for opportunities to use existing stores of data in unintended ways.
-->

* Modeling problems versus computation/storage problems
* Confidence versus validity


![Banko & Brill](Datado.058.png)


* Don't collect new data to answer your new questions.
* Look for new ways of using existing data sources.
* Store raw data! Don't aggregate prematurely.



# What I did

Data science about open data


## Store everything
<!--
Most of the work was already done for me; people had connected
siloed government data into Socrata portals, and I just needed
to get it out. But I did get it out and store it on S3.
-->

![Architecture of the Socrata downloader](architecture.jpg)


## Anything can be counted

<!--
I think people thing of "metadata" as something you don't analyze quantitatively.

* Title
* Description
* Tags

But this is also metadata:

* Whether the title contains a particular word
* Number of rows, columns
* View counts
* Number of tags

-->

![Public meetings by day of week](day-of-week.png)


![When datasets got uploaded](datasets_when_uploaded.png)


## Numbers can be turned into anything

[AppGen](http://www.appgen.me/browse)


## Boring work should be sent to robots
<!--
My colleague Jonathan played with the Site Analytics page
for the San Francisco portal and found something strange.
This diagram shows how we could detect something programmatically.
-->

[Site analytics](Datado.033.png)

[Scripted analyses](https://github.com/tlevine/socrata-analysis/tree/master/numbers)


## Get more data rather than tuning your model

* Metadata files (SODA 1 API)
* CSV datasets (only for New York)
* [Find users from datasets](http://thomaslevine.com/!/socrata-users)
* [Site metrics](http://thomaslevine.com/!/socrata-metrics-api/)
* [Other portal software](http://openprism.thomaslevine.com)



# What I learned

1. Nobody knows much
2. How Socrata Open Data portal is constructed
2. How people use Socrata Open Data portal


## What people know

* Portal administrators
* Portal developers
* Anecdotes


## Construction of Socrata Open Data Portal


### Data provenance

> Every view on Socrata has an "owner" and a "table author".
> What's an owner, and what's a table author?


![](user-model.jpg)


![](family.jpg)

<!--
Answer: XXX The view type diagram
Also note that this is not very strongly presented in the interface and that this makes it hard to tell which views are official
-->


### API limits

> What are Socrata's API limits?


I don't know, but they apply across all portals.


### Form validation

> What must be true about the form fields?

[!["Suggest a Dataset" form](form-validation.png)](https://data.seattle.gov/nominate)


![](unique-title.png)


### One web application

With a some software, you have many different installations that might be able to communicate with each other.

* Wordpress
* CKAN

With other software, a single web application runs everything.

* Tumblr
* Socrata

<!--
Related:

* Geocoding
* They all go down at once.
-->


## How people use Socrata


### Analysis tools exist.

![](family.jpg)


### People use them.

![](hits.png)


### But not really.

* [Bots](http://thomaslevine.com/!/socrata-users#also-no-tables)
* [Few users with many views](http://thomaslevine.com/!/socrata-users#with-a-profile-image)
* [VinylFox](https://twitter.com/VinylFox/status/362001457626611715)


[![VinylFox tweet](vinylfox.png)](https://twitter.com/VinylFox/status/362001457626611715)


# Benefits of the data portal

(As I see it)

1. Import data from various formats.
2. Standard way of discovering datasets.
3. Convert data to standard formats.
4. Mark datasets as official in some sense.

But not a lot of analysis


# Things to consider

### Data science

* Store/expose everything
* Datasets are data points, and metadata is data
* You can automate human work, even if it seems complicated.

### Socrata

* What if the different portals were more connected?
    <!-- Currently, Socrata sort of fakes having separate applications. Having everything in the same application has different benefits from having separate applications, and maybe you can make use of them. -->
* Are the analysis tools important?



# References

* [My articles about open data](http://thomaslevine.com/socrata)
* [Most of the source code](https://github.com/tlevine/socrata-analysis/)
* [Data Donuts](http://zipfianacademy.com/presos/)
* [CSV Soundsystem](http://csvsoundsystem.com)



mincemeatpy
------------------------------------------------------------------------------------------------
**Title**: mincemeatpy

**Web address**: https://api.github.com/repos/tlevine/mincemeatpy

**Date**: November 2012

**Description**: mincemeat.py: MapReduce on Python
=================================

Introduction
------------
mincemeat.py is a Python implementation of the [MapReduce](http://en.wikipedia.org/wiki/Mapreduce) distributed computing framework.

mincemeat.py is:

* Lightweight - All of the code is contained in a single Python file (currently weighing in at <13kB) that depends only on the Python Standard Library. Any computer with Python and mincemeat.py can be a part of your cluster.
* Fault tolerant - Workers (clients) can join and leave the cluster at any time without affecting the entire process.
* Secure - mincemeat.py authenticates both ends of every connection, ensuring that only authorized code is executed.
* Open source - mincemeat.py is distributed under the [MIT License](http://en.wikipedia.org/wiki/Mit_license), and consequently is free for all use, including commercial, personal, and academic, and can be modified and redistributed without restriction.


Download
--------

* Just [mincemeat.py](https://raw.github.com/michaelfairley/mincemeatpy/master/mincemeat.py) (v 0.1.2)
* The [full 0.1.2 release](https://github.com/michaelfairley/mincemeatpy/zipball/v0.1.2) (includes documentation and examples)
* Clone this git repository: `git clone https://github.com/michaelfairley/mincemeatpy.git`

Example
-------

Let's look at the canonical MapReduce example, word counting:

example.py:

```python
#!/usr/bin/env python
import mincemeat

data = ["Humpty Dumpty sat on a wall",
        "Humpty Dumpty had a great fall",
        "All the King's horses and all the King's men",
        "Couldn't put Humpty together again",
        ]

def mapfn(k, v):
    for w in v.split():
        yield w, 1

def reducefn(k, vs):
    result = 0
    for v in vs:
        result += v
    return result

s = mincemeat.Server()

# The data source can be any dictionary-like object
s.datasource = dict(enumerate(data))
s.mapfn = mapfn
s.reducefn = reducefn

results = s.run_server(password="changeme")
print results
```

You need to start the server before the clients. Execute this script on the server:

```bash
python example.py
```

Run mincemeat.py as a worker on a client:

```bash
python mincemeat.py -p changeme [server address]
```
And the server will print out:

```python
{'a': 2, 'on': 1, 'great': 1, 'Humpty': 3, 'again': 1, 'wall': 1, 'Dumpty': 2, 'men': 1, 'had': 1, 'all': 1, 'together': 1, "King's": 2, 'horses': 1, 'All': 1, "Couldn't": 1, 'fall': 1, 'and': 1, 'the': 2, 'put': 1, 'sat': 1} 
```

This example was overly simplistic, but changing the datasource to be a collection of large files and running the client on multiple machines will work just as well. In fact, mincemeat.py has been used to produce a word frequency lists for many gigabytes of text using a slightly modified version of this code.

Imports
-------

One potential gotcha when using mincemeat.py: Your `mapfn` and `reducefn` functions don't have access to their enclosing environment, including imported modules. If you need to use an imported module in one of these functions, be sure to include `import whatever` in the functions themselves.

modernomad
------------------------------------------------------------------------------------------------
**Title**: modernomad

**Web address**: https://api.github.com/repos/tlevine/modernomad

**Date**: May 2013

**Description**: Modernomad is guest, event and community management software for coliving
houses and other experimental living arrangements focused on openness,
collaboration and participation. 

Modernomad is licensed under the [Affero General Public License](agpl-3.0.txt),
which is like the GPL but *requires* you provide access to the source code for
any modified versions that are running publicly (among other things). The
[intent](http://www.gnu.org/licenses/why-affero-gpl.html) is to make sure that
anyone improving the software makes those improvements available to others, as
we have to them. 

<img src="media/img/agplv3-88x31.png" />

## About 
Modernomad is designed to integrate guests, residents and a broader community
around a colivng house. Although a large portion of the functionality revolves
around managing guests, the underlying ethos is one in which the reservation is
the beginning of a shared experience and participation in a community. If you
are just looking for reservation software, you could probably use this but you
might be better off with something like
[CheckFront](http://www.checkfront.com/) which seems to integrate nicely with
custom sites, or even [AirBnB](http://airbnb.com). 

Main Features:

- Accept reservation requests online
- Full profile system for guests and residents
- Guests can edit and delete reservations online, create multiple reservations
  associated wth their account. 
- Define rooms with default rate and privacy settings (specify if guests or
  only admins can book it)
- Set custom rate or comp for reservations on case-by-case basis
- Ability to take credit card payments on the site using stripe
- Issue invoices and receipts by email
- Guest and resident profiles 
- Separate groups for residents and house admins
- Automated email workflow around new reservation requests - email house admins
  With new requests, email guest with approval or confirmation info (house
  Access, rules, general info, etc.)
- Calendar view of reservations 
- 'Today' view of residents and guests at the house "today"
- Occupancy view for admins showing breakdown of income, paid status and guests
  Stats on a month-by-month basis. 
- House admins can add hosted reservations, for guests without profiles.

### Pre-requisites, system dependencies and environment setup intructions:
see [Environment Setup](docs/environment-setup.md)

### First-time Setup
see [How to Run](docs/how-to-run.md)

### Configuration
see [Configuration](docs/configuration.md)


mpi-cluster
------------------------------------------------------------------------------------------------
**Title**: mpi-cluster

**Web address**: https://api.github.com/repos/tlevine/mpi-cluster

**Date**: October 2012

**Description**: mpi-cluster
======
Build an MPI cluster. First, install software.

    # Install mpi on the master (localhost)
    mpi-cluster build localhost

    # Install mpi on two slave boxes.
    mpi-cluster build root@host1.domain not_root@host2.domain

Then configure slaves; run this on the master.

    # Add two slaves on username@host1.domain
    mpi-cluster add username@host1.domain 2
    
    # Add four slaves on username@host1.domain
    mpi-cluster add username@host2.domain 4

## Running R
(Directions come from [here](http://support.rstudio.org/help/discussions/questions/618-using-rstudio-server-with-rmpi-and-an-mpi-cluster).)

Run something like this to open an interactive R session with access to the
cluster.

    mpirun --hostfile .mpi_hostfile -np 1 R --no-save --interactive

Then, from the session,

    library(Rmpi)
    cl <- mpi.spawn.Rslaves(nslaves=1)                  

And register it with `foreach` (from [here](http://cran.r-project.org/web/packages/doMPI/vignettes/doMPI.pdf))

    library(doMPI)
    registerDoMPI(cl)


I changed my mind. Snowfall looks way easier.
http://cran.r-project.org/web/packages/snowfall/vignettes/snowfall.pdf


ms-Dropdown
------------------------------------------------------------------------------------------------
**Title**: ms-Dropdown

**Web address**: https://api.github.com/repos/tlevine/ms-Dropdown

**Date**: December 2011

**Description**: You may use msDropDown under the terms of either the MIT License or 
the Gnu General Public License (GPL) Version 2.

The MIT License is recommended for most projects. 
It is simple and easy to understand, and it places almost no restrictions on what you can do with msDropDown.

If the GPL suits your project better, you are also free to use msDropDown under that license.

You don't have to do anything special to choose one license or the other, and you don't have to notify anyone which license you are using. 



multisensory-data-zipfian
------------------------------------------------------------------------------------------------
**Title**: multisensory-data-zipfian

**Web address**: https://api.github.com/repos/tlevine/multisensory-data-zipfian

**Date**: July 2013

**Description**: <style>p.comment { display: none; }</style>
# Multisensory data experiences
<!-- For the Zipfian Academy July 2 class -->
Thomas Levine ([thomaslevine.com](http://thomaslevine.com)),

CSV Soundsystem ([csvsoundsystem.com](http://csvsoundsystem.com))

[tlevine.github.io/multisensory-data-zipfian](http://tlevine.github.io/multisensory-data-zipfian)



## Information overload
![A stack of papers](http://farm9.staticflickr.com/8434/7827785878_4a9c041ff8_o.jpg)
<p class="comment">
  Today, we produce more information than we can handle.
  To cope with this, we data scientists convert this information
  to structured data that we can make sense of in a more
  automated way.
</p>



## Big data
[![For handle big data, solution is very simple: buy bigger monitor and use smaller font in the terminal.](borat.png)](https://twitter.com/mysqlborat/status/306078371182428161)
<p class="comment">
  That helps, but now we have to deal with all of these data.
  We need new tools to help us with these data.
</p>
<p class="comment">
  In previous classes, you've learned about tools for
  storing insane quantities of data and running calculations
  on all of the data. We need more tools like that, but
  we also need new tools for exploring and presenting data.
</p>



## Data table
```
Special Operations,2005,2006,2007,2008,2009,Total
Emergency Service,0%,0%,1%,1%,0%,2%
Harbor Unit,0%,0%,0%,0%,0%,0%
Aviation Unit,0%,0%,0%,0%,0%,0%
Taxi Unit,0%,0%,0%,0%,0%,0%
Canine Unit,0%,0%,0%,0%,0%,0%
Mounted Unit,0%,0%,0%,0%,0%,0%
Headquarters,0%,0%,0%,0%,0%,0%
Special Operations Division Total,0%,0%,1%,1%,0%,2%
Percent of All Subject Officers Against Whom Allegations were Substantiated,0%,0%,0.2%,0.3%,0%,0.1%
```
<p class="comment">
  A standard data visualization tool is the data table.
  At CSV Soundsystem, we prefer CSV files.
  As MySQL borat suggests, we could just make bigger tables.
  Tables are great for finding specific values,
  but it takes a long time to spot broader trends in tables.
</p>
<p class="comment">
  This is a table of complaints against New York City police officers, I think.
  It's small enough that it fits on the screen and that
  we can get the general picture that most values are, but
  this wouldn't work for larger tables.
</p>



## Graphs
[![Plots of New York subway use surrounding mega storms](turnstile.png)](http://www.theatlanticcities.com/commute/2013/05/visualizing-impact-mega-storms-transit/5660/)
<p class="comment">
  So we make graphs.
  If you listen to Tufte, you will use graphs to present data in a
  multivariate way and to present the relationships among different variables.
</p>
<p class="comment">
  Let's talk more about multivariate systems.
  If you have a lot of variables, it's hard to think about all of them at once.
  Instead of doing that, we think of all of these variables as one concept.
  We can use statistical methods to convert many variables into one, or we can
  use data visualization methods to represent many variables at once.
</p>
<p class="comment">
  When we're analyzing a multivariate system, we can first look at the
  collective trend across all of the variables without worrying about the
  specific changes.
</p>
<p class="comment">
  This graph depicts the turns of New York subway turnstiles. This graph
  represents about four varibles: Date, year, subway stop, and number of
  turnstile entries per day. For the past three years, New York city has had
  one pretty big storm per year, each of them in the fall. The red lines
  are the dates of these storms, and the x axis is the date relative the
  storm, going from three weeks before the storm to three weeks after the storm.
  The y axis is the number of times a person entered a subway station each day,
  and each line is a different subway station. The first panel is for the 2010
  microburst, the second is for the 2011 Huricane Irene, and the third is for
  the 2012 Hurricane Sandy.
</p>
<p class="comment">
  Let's get back to our discussion of multivariate data. We are looking at
  four variables, but we are thinking about them as one concept.
</p>
<p class="comment">
  Look at these bumps that happen about five times per line.
  When I look at these, I'm not noticing the individual lines or the individual
  dates; I'm just noticing that there's some pattern.
  Now that we've found some broader pattern, let's focus on it and start
  picking apart the individual variables. Looking at the date variable, I see that
  this bump happens every seven days, so it might be the weekend. I guess people
  use the subway less on weekends. Looking at the subway station variable
  (the different lines), I notice that this bump happens across most stations.
  I also notice that some stations drop really low. That might be an error, or
  it might be that the stations or closed for maintenance. Or something else.
</p>
<p class="comment">
  Now let's look at the elephant in the room that I've been ignoring.
  We've noticed some pattern in the multivariate system; the lines are mostly
  straight, with these bumps on the weekends. But this pattern gets inturrupted
  at the red line. What's going on there? 
  Well that line is the date of the storms.
  Subway stations got shut during the storms, and that explains the drops
  in ridership.
  But when we look more closely, we see what might be errors in the data.
  These lines going straight across might be missing data for those days
  that should be represented as zero riders.
</p>
<p class="comment">
  You probably didn't need my narration in order to read this graph;
  I explain it like this to convey this approach of aggregating multivariate
  data. First, we looked for broader trends without focusing on specific
  variables. Then, once we identified an interesting trend, we iteratively
  focused on interesting subsets and interesting variables.
  This is related to a concept of hierarchy from graphic design; if you make
  one thing really big, people will know what to look at first, and then they
  can focus on the details if they're still interested.
  This is helpful for producing data visualizations, but I also keep this in
  mind when I'm doing data analyses in general.
</p>
<p class="comment">
  I just talked a lot about a how to think of graphs and data analysis
  in a multivariate way. Following Edward Tufte's advice, I aim to do this
  in data visualizations, but there are only so many variables I can fit
  on a graph. The previous visual used four variables, and I think
  six variables is pretty close to the most that can reliably be fit nicely
  on one static data visualization. So we need something more.
</p>



## Videos
[![complicated plot](4l-FixedScale-NoMuProf2-preview.png)](4l-FixedScale-NoMuProf2.gif)
<p class="comment">
  An obvious first step is video. This video relates a bunch of different
  variables from some experiment at CERN. I don't know what it means, but you
  can see how adding a time dimension allows for more variables to be presented
  at once.
</p>



## Data sonification
[![FMS Symphony](fms-symphony-preview.png)](http://fms.csvsoundsystem.com)
<p class="comment">
  When we add a time dimension, we get more possibilities.
  Of the five senses, vision is somewhat special in that we can look at many
  different things at once. It's hard to listen to many different things at once,
  so it's hard to present multiple variables through a few static notes,
  but if we add the time dimension, we can represent complex multivariate in sound.
</p>
<p class="comment">
  This is CSV Soundsystem's first music video, the FMS Symphony.
  It's a story about the federal treasury, the federal interest rate, the debt
  ceiling and the financial crisis. The high note is ..., the chords and flourishes
  are .... And the video. And the data come from here. And the Chernoff face is
  from the 15 first principal components of the table. And the thing that happens
  at the financial crisis.
</p>



## Data sonification
[Ridership Rachenitsa](transit.webm)
<!-- <video src="transit.webm" controls width="100%"> -->
<p class="comment">
  Here's another CSV soundsystem video.
  It uses both the New York City subway data we saw before
  and similar data about Chicago buses.
  New York subways get ten times the ridership, so these two figures are on
  different scales, but they have a very consistent pattern of ridership.
  There's one instrument for each city. High notes indicate more riders,
  and low notes indicate less riders. There's this dip every measure for
  the weekend. And once in a while we hear some change that we might want
  to investigate further. For example, in the winter....
</p>
<p class="comment">
  Sound opens up more possibilities, but there are still three more senses
  we haven't looked at. If we're going to leverage our full sensory capabilities
  to experience data, we're going to need to use touch, taste and smell too.
  And I think the best way of doing that is through food.
</p>



## Data gastronomification
[gastronomify R package](https://github.com/csv/gastronomify)
[![Data guacamole](data-guacamole.jpg)](https://github.com/tlevine/data-guacamole)
<p class="comment">
  To begin my exploration into this concept, I wrote an R package that varies
  a base recipe based on a dataset. Then I produced two bowls of guacamole to
  represent New York City math test scores for different years.
</p>
<p class="comment">
  Each ingredient corresponds to the average score for a particular school grade
  that year. There's more guacamole in the 2012 bowl because scores were higher
  across the board that year. (So grade inflation.) And one of them was spicier
  than the other because the sixth graders did proportionately better that year.
</p>
<p class="comment">
  As interesting as that may be, this is a pretty boring data gastronomification.
  The texture, visual appearance, smell and sound of the two bowls is quite
  similar, so we're not really leveraging our sensory capacity.
</p>



## Take-aways

* Present the multivariate world; escape Flatland.
* Explore and present data in a hierarchical way; start with the big picture,
    then get more detailed.
* Anything can be turned into data, and data can be mapped to anything.



## More resources

* [Music videos in R](http://livestre.am/4pN67)
* [Data-driven rhythms](https://github.com/csv/ddr)
* [A blog post](http://thomaslevine.com/!/sensory-data-experiences/)



## One last thing: Data cookies

* Cookie base: Brownie for garage and chocolate chip cookie for open lot
* Nuts: Public lots have nuts
* Length (inches): Log number of parking spaces
* Toppings correspond with the type of parking lot
  * Oreo: Paid publicly available
  * Reese's Piece: Customer parking only
  * marshmallow: Permit holders only (monthly, employee, student, car share, etc)
  * cherry: Commercial/government only
  * jam: Free parking available
* Sprinkles
  * Red sprinkles if it has valet
  * Blue sprinkles if it has motorcycle parking

<p class="comment">
  Data guacamole doesn't really use all the senses, and I wanted to make something
  that did, so we prepared some data cookies for tonight.
  Each cookie represents a parking lot in San Francisco.
  [Read the different ingredients]
</p>


music-videos-in-r
------------------------------------------------------------------------------------------------
**Title**: music-videos-in-r

**Web address**: https://api.github.com/repos/tlevine/music-videos-in-r

**Date**: April 2013

**Description**: Music Videos in R
====
The two examples are at

* https://github.com/csvsoundsystem/fms-treasury-statements-analysis.git
* http://gitorious.org/tlevine/12daysplot.git

Generate the slides by running `Rscript slides.r`. This loads the
`slides.Rmd` template, knits it, converts it to HTML and puts the output
in `index.html`. All of the webby files in this directory allow that
file to be a slideshow.

## Things to remember to talk about

* Point out that creating pretty graphs in base R graphics is quite unpleasant;
    this is why you should ordinarily use ggplot2.
* If symph.io were a real company, we'd note that we're hiring. But we're not a
    real company, so we instead note that we're available for hire.

## OUTLINE:

### Small talk
I'm Thomas - Data Superhero
I'm Brian - Data Scientologist

### Intro (Trade Off) - 5 minutes
    * big data is complex
    * visuals cannot fully capture the dimensionality of big data
    * we need new sensory experiences to express more dimensions
    * obama video - what do kids like? dubsteb and big data
    * we need poppy big data music videos
    * we've built these ideas are into symph.io
        - we have a proprietary data experience engine that
        big data companies are using them tame their big data tsunamis in real time.
        - but we're just gonna talk about some early prototypes we built in R and are now open-sourcing.

### 12 days of Christmas (Thomas)
    * drawing base R graphics
        - slides
        - base R drawing functions
            - building up a plot from scratch <- live code
    * playing slides to songs
        - slides
        - live performance ???

### FMS Symphony (Thomas, then Brian)
    * Video
        - how to make chernoff faces
            - slides
        - putting this in a browser
            - slides

    #### switch computers

    * Music
        - how to make music from data in R - 5 minutes
            - basic packages
            - limitations --->
        - intro to ddr package - 5 minutes
            - live coding
            - ??csvsoundsystem
        - building fms symphony - 5 minutes
            - live coding
            - mixing down in ableton

    * Combining Music and Video
        - slide (javascript)
        - adding links
        - precise temporal navigation
        -

### Conclusion
    * Thomas is available for hire.


mvn-www2-backup
------------------------------------------------------------------------------------------------
**Title**: mvn-www2-backup

**Web address**: https://api.github.com/repos/tlevine/mvn-www2-backup

**Date**: April 2013

**Description**: ## Downloading old documents
You can see all the previous permit applications from Scott's
manual spreadsheet parsings here,
except for a couple that went missing.

If you want to integrate them with your csv file,
switch everything in the respective urls before the
final (sixth) slash for
"`https://raw.github.com/tlevine/mvn-www2-backup/master/documents`"

For example,
[`http://www2.mvn.usace.army.mil/ops/regulatory/pdf/document2011-09-09-103911.pdf`](http://www2.mvn.usace.army.mil/ops/regulatory/pdf/document2011-09-09-103911.pdf)
becomes
[`https://raw.github.com/tlevine/mvn-www2-backup/master/documents/document2011-09-09-103911.pdf`](https://raw.github.com/tlevine/mvn-www2-backup/master/documents/document2011-09-09-103911.pdf)

You can also download them all at once
[here](https://github.com/tlevine/mvn-www2-backup/archive/master.zip)



n
------------------------------------------------------------------------------------------------
**Title**: n

**Web address**: https://api.github.com/repos/tlevine/n

**Date**: October 2012

**Description**: # n

 My own flavour of node binary management, no subshells, no profile setup, no convoluted api, just _simple_.

## Installation

    $ npm install -g n

or

    $ make install

### Installing Binaries

Install a few nodes ("v" is optional), the version given becomes the active node binary once installation is complete.

    $ n 0.2.6
    $ n v0.3.3

List installed binaries:

    $ n

      0.2.5
    I? 0.2.6
      0.3.3

Pass some config flags to _./configure_:

    $ n 0.2.6 --debug

List installed binaries, config flags are shown:

      0.2.3
    I? 0.2.6 --debug
      0.3.4
      0.3.5

Use or install the latest official release:

    $ n latest

Use or install the stable official release:

    $ n stable

Install a custom or patched version of node from a tarball:

    $ n custom 0.6.5 https://github.com/dshaw/node/tarball/patch/v0.6.5status

### Removing Binaries

Remove some versions:

    $ n rm 0.2.4 v0.3.0

Instead of using `rm` we can simply use `-`:

    $ n - 0.2.4

### Binary Usage

When running multiple versions of node, we can target
them directly by asking `n` for the binary path:

    $ n bin 0.3.3
    /usr/local/n/versions/0.3.3/bin/node

Execute a script with 0.3.3 regardless of the active version:

    $ n use 0.3.3 some.js

with flags:

    $ n as 0.3.3 --debug some.js

Execute npm with 0.6.3 regardless of the active version:

    $ n npm 0.6.3 install coffee-script
    $ n npm 0.6.3 list

## Usage

 Output from `n --help`:

     Usage: n [options] [COMMAND] [config]

     Commands:

       n                           Output versions installed
       n latest [config ...]       Install or activate the latest node release
       n stable [config ...]       Install or activate the latest stable node release
       n <version> [config ...]    Install and/or use node <version>
       n custom <version> <tarball> [config ...]  Install custom node <tarball> with [args ...]
       n use <version> [args ...]  Execute node <version> with [args ...]
       n npm <version> [args ...]  Execute npm <version> with [args ...]
       n bin <version>             Output bin path for <version>
       n rm <version ...>          Remove the given version(s)
       n --latest                  Output the latest node version available
       n --stable                  Output the latest stable node version available
       n ls                        Output the versions of node available

     Options:

       -V, --version   Output current version of n
       -h, --help      Display help information

     Aliases:

       -       rm
       which   bin
       use     as
       custom  c

## Details

 `n` by default installs node to _/usr/local/n/versions_, from
 which it can see what you have currently installed, and activate previously installed versions of node when `n <version>` is invoked again.

 Activated nodes are then installed to the prefix _/usr/local_, which of course may be altered via the __N_PREFIX__ environment variable.

 To alter where `n` operates simply export __N_PREFIX__ to whatever you prefer.

## License

(The MIT License)

Copyright (c) 2010 TJ Holowaychuk &lt;tj@vision-media.ca&gt;

Permission is hereby granted, free of charge, to any person obtaining
a copy of this software and associated documentation files (the
'Software'), to deal in the Software without restriction, including
without limitation the rights to use, copy, modify, merge, publish,
distribute, sublicense, and/or sell copies of the Software, and to
permit persons to whom the Software is furnished to do so, subject to
the following conditions:

The above copyright notice and this permission notice shall be
included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED 'AS IS', WITHOUT WARRANTY OF ANY KIND,
EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.


n.an
------------------------------------------------------------------------------------------------
**Title**: n.an

**Web address**: https://api.github.com/repos/tlevine/n.an

**Date**: July 2012

**Description**: Noncomprehensive Dotfile Archive Network (n.an)
=======
This stores both public and private dotfiles.

### Public dotfiles
Unpack n.an to your home director; when you do that, the public
dotfiles will be created there, and you can already make updates
to them and commit them.

### Private
Some dotfiles contain private things, like passwords, or things
that are only relevant to one person, like email addresses. These
dotfiles are stored in dotfile-templates. Run `~/bin/mkdotfiles`
(or just `mkdotfiles` because the `.bashrc` adds `~/bin` to
the path) to build them. You will be prompted to fill in the variables,
and the resulting files will be placed in `~`.

### Writing templates
"Templates" just are normal files with variables indicated by
double-curly-brace-enclosed variables. For example, `.gitconfig`
looks like this.

    [user]
    name = {{fullname}}
    email = {{emailaddress}}

Here are the rules.

* Variables are defined by a phrase enclosed in a pair of double curly braces.
* One line have no more than one variable.
* Variables can contain any character aside from a newline, so spaces
  and curly braces are allowed.


new-york-pizza
------------------------------------------------------------------------------------------------
**Title**: new-york-pizza

**Web address**: https://api.github.com/repos/tlevine/new-york-pizza

**Date**: November 2012

**Description**: New York Pizza
======

`pizza.csv` lists pizzarias grouped by type. If I have eaten a pizzaria's
pizza, I have noted when; NA indicates that I've eaten there many times and
have forgotten when.


To do
----
### September 21, 2012
I've had pizza from the coal, neo-neapolitan, squares and specialty categories;
in order to have had one from each of Jared's categories, I want to go to Joe's
(New York), at 7 Carmine Street, and Numero 28 (Roman), at 28 Carmine Street.


NFLRanking
------------------------------------------------------------------------------------------------
**Title**: NFLRanking

**Web address**: https://api.github.com/repos/tlevine/NFLRanking

**Date**: February 2013

**Description**: NFLRanking
==========

Ranking NFL teams

[Slides](http://seanjtaylor.github.com/NFLRanking/web/slides)


nfsn-helpers
------------------------------------------------------------------------------------------------
**Title**: nfsn-helpers

**Web address**: https://api.github.com/repos/tlevine/nfsn-helpers

**Date**: April 2013

**Description**: Some helpful things for
[NearlyFreeSpeech.NET](https://nearlyfreespeech.net)

## Notes
While upgrading Piwik, disable PHP safe mode.


nicole-video
------------------------------------------------------------------------------------------------
**Title**: nicole-video

**Web address**: https://api.github.com/repos/tlevine/nicole-video

**Date**: August 2013

**Description**: 

node-restify
------------------------------------------------------------------------------------------------
**Title**: node-restify

**Web address**: https://api.github.com/repos/tlevine/node-restify

**Date**: January 2013

**Description**: # restify

[![Build Status](https://travis-ci.org/mcavage/node-restify.png)](https://travis-ci.org/mcavage/node-restify)

[restify](http://mcavage.github.com/node-restify) is a smallish framework,
similar to [express](http://expressjs.com) for building REST APIs.  For full
details, see http://mcavage.github.com/node-restify.

# Usage

## Server

    var restify = require('restify');

    var server = restify.createServer({
      name: 'myapp',
      version: '1.0.0'
    });
    server.use(restify.acceptParser(server.acceptable));
    server.use(restify.queryParser());
    server.use(restify.bodyParser());

    server.get('/echo/:name', function (req, res, next) {
      res.send(req.params);
      return next();
    });

    server.listen(8080, function () {
      console.log('%s listening at %s', server.name, server.url);
    });

## Client

    var assert = require('assert');
    var restify = require('restify');

    var client = restify.createJsonClient({
      url: 'http://localhost:8080',
      version: '~1.0'
    });

    client.get('/echo/mark', function (err, req, res, obj) {
      assert.ifError(err);
      console.log('Server returned: %j', obj);
    });

# Installation

    $ npm install restify

## License

The MIT License (MIT)
Copyright (c) 2012 Mark Cavage

Permission is hereby granted, free of charge, to any person obtaining a copy of
this software and associated documentation files (the "Software"), to deal in
the Software without restriction, including without limitation the rights to
use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of
the Software, and to permit persons to whom the Software is furnished to do so,
subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

## Bugs

See <https://github.com/mcavage/node-restify/issues>.

## Mailing list

See the
[Google group](https://groups.google.com/forum/?hl=en&fromgroups#!forum/restify)
.


node-XMLHttpRequest
------------------------------------------------------------------------------------------------
**Title**: node-XMLHttpRequest

**Web address**: https://api.github.com/repos/tlevine/node-XMLHttpRequest

**Date**: July 2013

**Description**: # node-XMLHttpRequest #

node-XMLHttpRequest is a wrapper for the built-in http client to emulate the
browser XMLHttpRequest object.

This can be used with JS designed for browsers to improve reuse of code and
allow the use of existing libraries.

Note: This library currently conforms to [XMLHttpRequest 1](http://www.w3.org/TR/XMLHttpRequest/). Version 2.0 will target [XMLHttpRequest Level 2](http://www.w3.org/TR/XMLHttpRequest2/).

## Usage ##

Here's how to include the module in your project and use as the browser-based
XHR object.

	var XMLHttpRequest = require("xmlhttprequest").XMLHttpRequest;
	var xhr = new XMLHttpRequest();

Note: use the lowercase string "xmlhttprequest" in your require(). On
case-sensitive systems (eg Linux) using uppercase letters won't work.

## Versions ##

Prior to 1.4.0 version numbers were arbitrary. From 1.4.0 on they conform to
the standard major.minor.bugfix. 1.x shouldn't necessarily be considered
stable just because it's above 0.x.

Since the XMLHttpRequest API is stable this library's API is stable as
well. Major version numbers indicate significant core code changes.
Minor versions indicate minor core code changes or better conformity to
the W3C spec.

## License ##

MIT license. See LICENSE for full details.

## Supports ##

* Async and synchronous requests
* GET, POST, PUT, and DELETE requests
* All spec methods (open, send, abort, getRequestHeader,
  getAllRequestHeaders, event methods)
* Requests to all domains

## Known Issues / Missing Features ##

For a list of open issues or to report your own visit the [github issues
page](https://github.com/driverdan/node-XMLHttpRequest/issues).

* Local file access may have unexpected results for non-UTF8 files
* Synchronous requests don't set headers properly
* Synchronous requests freeze node while waiting for response (But that's what you want, right? Stick with async!).
* Some events are missing, such as abort
* getRequestHeader is case-sensitive
* Cookies aren't persisted between requests
* Missing XML support
* Missing basic auth


nose
------------------------------------------------------------------------------------------------
**Title**: nose

**Web address**: https://api.github.com/repos/tlevine/nose

**Date**: June 2013

**Description**: 
Basic usage
***********

Use the nosetests script (after installation by setuptools):

   nosetests [options] [(optional) test files or directories]

In addition to passing command-line options, you may also put
configuration options in a .noserc or nose.cfg file in your home
directory. These are standard .ini-style config files. Put your
nosetests configuration in a [nosetests] section, with the -- prefix
removed:

   [nosetests]
   verbosity=3
   with-doctest=1

There are several other ways to use the nose test runner besides the
*nosetests* script. You may use nose in a test script:

   import nose
   nose.main()

If you don't want the test script to exit with 0 on success and 1 on
failure (like unittest.main), use nose.run() instead:

   import nose
   result = nose.run()

*result* will be true if the test run succeeded, or false if any test
failed or raised an uncaught exception. Lastly, you can run nose.core
directly, which will run nose.main():

   python /path/to/nose/core.py

Please see the usage message for the nosetests script for information
about how to control which tests nose runs, which plugins are loaded,
and the test output.


Extended usage
==============

nose collects tests automatically from python source files,
directories and packages found in its working directory (which
defaults to the current working directory). Any python source file,
directory or package that matches the testMatch regular expression (by
default: *(?:^|[b_.-])[Tt]est)* will be collected as a test (or source
for collection of tests). In addition, all other packages found in the
working directory will be examined for python source files or
directories that match testMatch. Package discovery descends all the
way down the tree, so package.tests and package.sub.tests and
package.sub.sub2.tests will all be collected.

Within a test directory or package, any python source file matching
testMatch will be examined for test cases. Within a test module,
functions and classes whose names match testMatch and TestCase
subclasses with any name will be loaded and executed as tests. Tests
may use the assert keyword or raise AssertionErrors to indicate test
failure. TestCase subclasses may do the same or use the various
TestCase methods available.


Selecting Tests
---------------

To specify which tests to run, pass test names on the command line:

   nosetests only_test_this.py

Test names specified may be file or module names, and may optionally
indicate the test case to run by separating the module or file name
from the test case name with a colon. Filenames may be relative or
absolute. Examples:

   nosetests test.module
   nosetests another.test:TestCase.test_method
   nosetests a.test:TestCase
   nosetests /path/to/test/file.py:test_function

You may also change the working directory where nose looks for tests
by using the -w switch:

   nosetests -w /path/to/tests

Note, however, that support for multiple -w arguments is now
deprecated and will be removed in a future release. As of nose 0.10,
you can get the same behavior by specifying the target directories
*without* the -w switch:

   nosetests /path/to/tests /another/path/to/tests

Further customization of test selection and loading is possible
through the use of plugins.

Test result output is identical to that of unittest, except for the
additional features (error classes, and plugin-supplied features such
as output capture and assert introspection) detailed in the options
below.


Configuration
-------------

In addition to passing command-line options, you may also put
configuration options in your project's *setup.cfg* file, or a .noserc
or nose.cfg file in your home directory. In any of these standard
.ini-style config files, you put your nosetests configuration in a
"[nosetests]" section. Options are the same as on the command line,
with the -- prefix removed. For options that are simple switches, you
must supply a value:

   [nosetests]
   verbosity=3
   with-doctest=1

All configuration files that are found will be loaded and their
options combined. You can override the standard config file loading
with the "-c" option.


Using Plugins
-------------

There are numerous nose plugins available via easy_install and
elsewhere. To use a plugin, just install it. The plugin will add
command line options to nosetests. To verify that the plugin is
installed, run:

   nosetests --plugins

You can add -v or -vv to that command to show more information about
each plugin.

If you are running nose.main() or nose.run() from a script, you can
specify a list of plugins to use by passing a list of plugins with the
plugins keyword argument.


0.9 plugins
-----------

nose 1.0 can use SOME plugins that were written for nose 0.9. The
default plugin manager inserts a compatibility wrapper around 0.9
plugins that adapts the changed plugin api calls. However, plugins
that access nose internals are likely to fail, especially if they
attempt to access test case or test suite classes. For example,
plugins that try to determine if a test passed to startTest is an
individual test or a suite will fail, partly because suites are no
longer passed to startTest and partly because it's likely that the
plugin is trying to find out if the test is an instance of a class
that no longer exists.


0.10 and 0.11 plugins
---------------------

All plugins written for nose 0.10 and 0.11 should work with nose 1.0.


Options
-------

-V, --version

   Output nose version and exit

-p, --plugins

   Output list of available plugins and exit. Combine with higher
   verbosity for greater detail

-v=DEFAULT, --verbose=DEFAULT

   Be more verbose. [NOSE_VERBOSE]

--verbosity=VERBOSITY

   Set verbosity; --verbosity=2 is the same as -v

-q=DEFAULT, --quiet=DEFAULT

   Be less verbose

-c=FILES, --config=FILES

   Load configuration from config file(s). May be specified multiple
   times; in that case, all config files will be loaded and combined

-w=WHERE, --where=WHERE

   Look for tests in this directory. May be specified multiple times.
   The first directory passed will be used as the working directory,
   in place of the current working directory, which is the default.
   Others will be added to the list of tests to execute. [NOSE_WHERE]

--py3where=PY3WHERE

   Look for tests in this directory under Python 3.x. Functions the
   same as 'where', but only applies if running under Python 3.x or
   above.  Note that, if present under 3.x, this option completely
   replaces any directories specified with 'where', so the 'where'
   option becomes ineffective. [NOSE_PY3WHERE]

-m=REGEX, --match=REGEX, --testmatch=REGEX

   Files, directories, function names, and class names that match this
   regular expression are considered tests.  Default:
   (?:^|[b_./-])[Tt]est [NOSE_TESTMATCH]

--tests=NAMES

   Run these tests (comma-separated list). This argument is useful
   mainly from configuration files; on the command line, just pass the
   tests to run as additional arguments with no switch.

-l=DEFAULT, --debug=DEFAULT

   Activate debug logging for one or more systems. Available debug
   loggers: nose, nose.importer, nose.inspector, nose.plugins,
   nose.result and nose.selector. Separate multiple names with a
   comma.

--debug-log=FILE

   Log debug messages to this file (default: sys.stderr)

--logging-config=FILE, --log-config=FILE

   Load logging config from this file -- bypasses all other logging
   config settings.

-I=REGEX, --ignore-files=REGEX

   Completely ignore any file that matches this regular expression.
   Takes precedence over any other settings or plugins. Specifying
   this option will replace the default setting. Specify this option
   multiple times to add more regular expressions [NOSE_IGNORE_FILES]

-e=REGEX, --exclude=REGEX

   Don't run tests that match regular expression [NOSE_EXCLUDE]

-i=REGEX, --include=REGEX

   This regular expression will be applied to files, directories,
   function names, and class names for a chance to include additional
   tests that do not match TESTMATCH.  Specify this option multiple
   times to add more regular expressions [NOSE_INCLUDE]

-x, --stop

   Stop running tests after the first error or failure

-P, --no-path-adjustment

   Don't make any changes to sys.path when loading tests [NOSE_NOPATH]

--exe

   Look for tests in python modules that are executable. Normal
   behavior is to exclude executable modules, since they may not be
   import-safe [NOSE_INCLUDE_EXE]

--noexe

   DO NOT look for tests in python modules that are executable. (The
   default on the windows platform is to do so.)

--traverse-namespace

   Traverse through all path entries of a namespace package

--first-package-wins, --first-pkg-wins, --1st-pkg-wins

   nose's importer will normally evict a package from sys.modules if
   it sees a package with the same name in a different location. Set
   this option to disable that behavior.

-a=ATTR, --attr=ATTR

   Run only tests that have attributes specified by ATTR [NOSE_ATTR]

-A=EXPR, --eval-attr=EXPR

   Run only tests for whose attributes the Python expression EXPR
   evaluates to True [NOSE_EVAL_ATTR]

-s, --nocapture

   Don't capture stdout (any stdout output will be printed
   immediately) [NOSE_NOCAPTURE]

--nologcapture

   Disable logging capture plugin. Logging configurtion will be left
   intact. [NOSE_NOLOGCAPTURE]

--logging-format=FORMAT

   Specify custom format to print statements. Uses the same format as
   used by standard logging handlers. [NOSE_LOGFORMAT]

--logging-datefmt=FORMAT

   Specify custom date/time format to print statements. Uses the same
   format as used by standard logging handlers. [NOSE_LOGDATEFMT]

--logging-filter=FILTER

   Specify which statements to filter in/out. By default, everything
   is captured. If the output is too verbose, use this option to
   filter out needless output. Example: filter=foo will capture
   statements issued ONLY to  foo or foo.what.ever.sub but not foobar
   or other logger. Specify multiple loggers with comma:
   filter=foo,bar,baz. If any logger name is prefixed with a minus, eg
   filter=-foo, it will be excluded rather than included. Default:
   exclude logging messages from nose itself (-nose). [NOSE_LOGFILTER]

--logging-clear-handlers

   Clear all other logging handlers

--with-coverage

   Enable plugin Coverage:  Activate a coverage report using Ned
   Batchelder's coverage module.  [NOSE_WITH_COVERAGE]

--cover-package=PACKAGE

   Restrict coverage output to selected packages [NOSE_COVER_PACKAGE]

--cover-erase

   Erase previously collected coverage statistics before run

--cover-tests

   Include test modules in coverage report [NOSE_COVER_TESTS]

--cover-inclusive

   Include all python files under working directory in coverage
   report.  Useful for discovering holes in test coverage if not all
   files are imported by the test suite. [NOSE_COVER_INCLUSIVE]

--cover-html

   Produce HTML coverage information

--cover-html-dir=DIR

   Produce HTML coverage information in dir

--cover-branches

   Include branch coverage in coverage report [NOSE_COVER_BRANCHES]

--cover-xml

   Produce XML coverage information

--cover-xml-file=FILE

   Produce XML coverage information in file

--pdb

   Drop into debugger on errors

--pdb-failures

   Drop into debugger on failures

--no-deprecated

   Disable special handling of DeprecatedTest exceptions.

--with-doctest

   Enable plugin Doctest:  Activate doctest plugin to find and run
   doctests in non-test modules.  [NOSE_WITH_DOCTEST]

--doctest-tests

   Also look for doctests in test modules. Note that classes, methods
   and functions should have either doctests or non-doctest tests, not
   both. [NOSE_DOCTEST_TESTS]

--doctest-extension=EXT

   Also look for doctests in files with this extension
   [NOSE_DOCTEST_EXTENSION]

--doctest-result-variable=VAR

   Change the variable name set to the result of the last interpreter
   command from the default '_'. Can be used to avoid conflicts with
   the _() function used for text translation.
   [NOSE_DOCTEST_RESULT_VAR]

--doctest-fixtures=SUFFIX

   Find fixtures for a doctest file in module with this name appended
   to the base name of the doctest file

--with-isolation

   Enable plugin IsolationPlugin:  Activate the isolation plugin to
   isolate changes to external modules to a single test module or
   package. The isolation plugin resets the contents of sys.modules
   after each test module or package runs to its state before the
   test. PLEASE NOTE that this plugin should not be used with the
   coverage plugin, or in any other case where module reloading may
   produce undesirable side-effects.  [NOSE_WITH_ISOLATION]

-d, --detailed-errors, --failure-detail

   Add detail to error output by attempting to evaluate failed asserts
   [NOSE_DETAILED_ERRORS]

--with-profile

   Enable plugin Profile:  Use this plugin to run tests using the
   hotshot profiler.   [NOSE_WITH_PROFILE]

--profile-sort=SORT

   Set sort order for profiler output

--profile-stats-file=FILE

   Profiler stats file; default is a new temp file on each run

--profile-restrict=RESTRICT

   Restrict profiler output. See help for pstats.Stats for details

--no-skip

   Disable special handling of SkipTest exceptions.

--with-id

   Enable plugin TestId:  Activate to add a test id (like #1) to each
   test name output. Activate with --failed to rerun failing tests
   only.  [NOSE_WITH_ID]

--id-file=FILE

   Store test ids found in test runs in this file. Default is the file
   .noseids in the working directory.

--failed

   Run the tests that failed in the last test run.

--processes=NUM

   Spread test run among this many processes. Set a number equal to
   the number of processors or cores in your machine for best results.
   [NOSE_PROCESSES]

--process-timeout=SECONDS

   Set timeout for return of results from each test runner process.
   [NOSE_PROCESS_TIMEOUT]

--process-restartworker

   If set, will restart each worker process once their tests are done,
   this helps control memory leaks from killing the system.
   [NOSE_PROCESS_RESTARTWORKER]

--with-xunit

   Enable plugin Xunit: This plugin provides test results in the
   standard XUnit XML format. [NOSE_WITH_XUNIT]

--xunit-file=FILE

   Path to xml file to store the xunit report in. Default is
   nosetests.xml in the working directory [NOSE_XUNIT_FILE]

--all-modules

   Enable plugin AllModules: Collect tests from all python modules.
   [NOSE_ALL_MODULES]

--collect-only

   Enable collect-only:  Collect and output test names only, don't run
   any tests.  [COLLECT_ONLY]


numm.org
------------------------------------------------------------------------------------------------
**Title**: numm.org

**Web address**: https://api.github.com/repos/tlevine/numm.org

**Date**: June 2013

**Description**: 

nvm
------------------------------------------------------------------------------------------------
**Title**: nvm

**Web address**: https://api.github.com/repos/tlevine/nvm

**Date**: August 2013

**Description**: # Node Version Manager

## Installation

First you'll need to make sure your system has a c++ compiler.  For OSX, XCode will work, for Ubuntu, the build-essential and libssl-dev packages work.

### Install script

To install you could use the [install script](https://github.com/creationix/nvm/blob/master/install.sh) (requires Git) using cURL:

    curl https://raw.github.com/creationix/nvm/master/install.sh | sh

or Wget:

    wget -qO- https://raw.github.com/creationix/nvm/master/install.sh | sh

<sub>The script clones the Nvm repository to `~/.nvm` and adds the source line to your profile (`~/.bash_profile` or `~/.profile`).</sub>


### Manual install

For manual install create a folder somewhere in your filesystem with the `nvm.sh` file inside it.  I put mine in a folder called `nvm`.

Or if you have `git` installed, then just clone it:

    git clone https://github.com/creationix/nvm.git ~/.nvm

To activate nvm, you need to source it from your bash shell

    source ~/.nvm/nvm.sh

I always add this line to my `~/.bashrc` or `~/.profile` file to have it automatically sourced upon login.   
Often I also put in a line to use a specific version of node.
    
## Usage

To download, compile, and install the latest v0.10.x release of node, do this:

    nvm install 0.10

And then in any new shell just use the installed version:

    nvm use 0.10

Or you can just run it:

    nvm run 0.10

If you want to see what versions are installed:

    nvm ls

If you want to see what versions are available to install:

    nvm ls-remote

To restore your PATH, you can deactivate it.

    nvm deactivate

To set a default Node version to be used in any new shell, use the alias 'default':

    nvm alias default 0.10

## License

Nvm is released under the MIT license.


Copyright (C) 2010-2013 Tim Caswell

Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

## Running tests
Tests are written in [Urchin](http://www.urchin.sh). Install Urchin like so.

    wget -O /usr/local/bin https://raw.github.com/scraperwiki/urchin/0c6837cfbdd0963903bf0463b05160c2aecc22ef/urchin
    chmod +x /usr/local/bin/urchin

(Or put it some other place in your PATH.)

There are slow tests and fast tests. The slow tests do things like install node
and check that the right versions are used. The fast tests fake this to test
things like aliases and uninstalling. From the root of the nvm git repository,
run the fast tests like this.

    urchin test/fast

Run the slow tests like this.

    urchin test/slow

Run all of the tests like this

    urchin test

Nota bene: Avoid running nvm while the tests are running.

## Bash completion

To activate, you need to source `bash_completion`:

  	[[ -r $NVM_DIR/bash_completion ]] && . $NVM_DIR/bash_completion

Put the above sourcing line just below the sourcing line for NVM in your profile (`.bashrc`, `.bash_profile`).

### Usage

nvm

	$ nvm [tab][tab]
	alias          copy-packages  help           list           run            uninstall      version        
	clear-cache    deactivate     install        ls             unalias        use

nvm alias

	$ nvm alias [tab][tab]
	default

	$ nvm alias my_alias [tab][tab]
	v0.4.11        v0.4.12       v0.6.14
	
nvm use

	$ nvm use [tab][tab]
	my_alias        default        v0.4.11        v0.4.12       v0.6.14
	
nvm uninstall

	$ nvm uninstall [tab][tab]
	my_alias        default        v0.4.11        v0.4.12       v0.6.14
	
## Problems

If you try to install a node version and the installation fails, be sure to delete the node downloads from src (~/.nvm/src/) or you might get an error when trying to reinstall them again or you might get an error like the following:
    
    curl: (33) HTTP server doesn't seem to support byte ranges. Cannot resume.

Where's my 'sudo node'? Checkout this link:
    
https://github.com/creationix/nvm/issues/43

on Arch Linux and other systems using python3 by default, before running *install* you need to

      export PYTHON=python2

After the v0.8.6 release of node, nvm tries to install from binary packages. But in some systems, the official binary packages don't work due to incompatibility of shared libs. In such cases, use `-s` option to force install from source:

    nvm install -s 0.8.6



ny-history-regents
------------------------------------------------------------------------------------------------
**Title**: ny-history-regents

**Web address**: https://api.github.com/repos/tlevine/ny-history-regents

**Date**: February 2013

**Description**: Download the history regents and convert them into a format useful for
prediction of answers.

Download

    ./download.py

Then parse it

    ./parse-exam.sh [exam file pdf]

Parse all of the US history regents.

    ./parse-us.sh

Extract the features

    ghc features.hs && echo Running features ... && ./features

Export to csv if you like.

    sqlite3 -csv -header /tmp/history-regents.db 'select * from question;' > question.csv
    sqlite3 -csv -header /tmp/history-regents.db 'select * from answer;' > answer.csv

## Results
If I guess the answer that is Leveschtein-closest to the others,
I get 254 of 795 correct, or about one-fourth. Boring.

## Feature selection

### Neutral answers win.
I think that sentiment will tell us something. Question 43 from the January 2004 test is one question that makes me think so.

> 43 The changes shown in the graph support the recent concerns of Americans about the
>
> 1. future of Social Security and Medicare
> 2. return to an agrarian society
> 3. surplus of health care workers
> 4. shortage of schools and colleges

Without looking at the graph, I correctly guessed choice 1. The words "surplus" and "shortage" felt too strong,
so narrowed it down to the first two choices. A return to an agrarian society seemed unreasonable, so I said 1.

### Short answers win.
Short answers seem to be correct, or at least long answers seem to be wrong. Naively, we might look at

* Number of characters
* Number of words

It might be related to the linguistic complexity, so here are some other things worth considering

* Number of clauses
* How fancy the words are
* Number of adjectives

This is implemented as `getNCharacters` and `getNWords`.

### The conjunction "and" wins.

Somewhat contrary to my suspicion that short answers win, I also suspect that
the presense of the conjunction "and" normally occurs in correct answers. For
question 30 of June 2007, I first eliminated 1 and 3 because they both included
the same phraes "Latin American". Based on my shortness criterion, I would have
chosen choice 4, but that didn't feel right, so I correctly chose choice 2.
I still haven't looked at the map mentioned in the question.

> 30 The conclusion that can best be supported by the
> information on this map is that construction of
> the Panama Canal was motivated by the desire of
> the United States to
> 
> (1) raise the living standards of Latin American people
> (2) increase naval mobility and expand overseas markets
> (3) improve relations with Latin American and Asian nations
> (4) maintain a policy of collective security

This is implemented as `getContainsAnd`.

### Qualitative, abstract, synthesized statements about graphs win

I correctly excluded answers 1 and 2 from question 45 of January 2004 because they sounded too quantitative.

> 45 Which statement is most clearly supported by the
> information in the graph?
> 
> 1. More children were under age 6 in 1990 than in 1950.
> 2. Since 1990, women have made up more than half of the workforce.
> 3. The gap between male and female incomes has declined.
> 4. Fewer women are staying home to raise their young children.

This is implemented as `getIsQualitativeAnswerAboutGraph`.

### Equivalent answers lose.

Without reading the three headlines, I correctly guessed choice 1 because the others were all equivalent.
As a side note, even after reading the headlines, I had no idea that these were talking about compromises.

> 2
>
> * "New Congress to Have Two Houses"
> * "Slaves to Count as Three-Fifths of a Person"
> * "President to be Chosen by Electoral Vote"
>
> Which conclusion about the Constitutional Convention is best supported by these headlines?
>
> 1. The framers of the Constitution were able to compromise on important issues.
> 2. States that were small in area would lose power in the new Constitution.
> 3. States with large populations controlled the outcome of the convention.
> 4. The president and Congress would have equal power under the new constitution.

## Questions whose answers I can't guess well

### Question 20 of the January 2004 test

> 20 The common purpose of these legislative acts was to
>
> 1. protect the nationas natural resources
> 2. improve conditions for recent immigrants to the United States
> 3. advance the growth of big business
> 4. promote the general welfare of the American public

Maybe a hint is that "resources" and "conditions" are used in the table of legislative acts.
I've generally seen that vocabularity is rarely repeated within an entire
test, so repeating vocabulary might be an attempt to trick students.

This is implemented as `getContainsCommonWord`.

### Question 36 of the June 2007 test
Compared to other questions, this was quite easy for me to guess, but it
involved my knowledge of history and not my knowledge of the trends I
document above.

> 36 In which pair of events is the second event a
> response to the first?
>
> (1) Truman Doctrine a D-Day Invasion
> (2) Manhattan Project a Lend-Lease Act
> (3) Holocaust a Nuremberg War Crimes trials
> (4) Germany's invasion of Poland a Munich Conference

### Question 42 of the June 2007 test

> 42 In the Camp David Accords (1978), President
> Jimmy Carter succeeded in
> (1) returning the Panama Canal Zone to Panama
> (2) suspending grain sales to the Soviet Union
> and China
> (3) providing a foundation for a peace treaty
> between Egypt and Israel
> (4) freeing hostages being held in Iran


nyc-cyclist-count
------------------------------------------------------------------------------------------------
**Title**: nyc-cyclist-count

**Web address**: https://api.github.com/repos/tlevine/nyc-cyclist-count

**Date**: April 2013

**Description**: Music video of NYC Cyclist Count
====

https://data.cityofnewyork.us/widgets/yqzy-qebu


nyc-data
------------------------------------------------------------------------------------------------
**Title**: nyc-data

**Web address**: https://api.github.com/repos/tlevine/nyc-data

**Date**: April 2013

**Description**: AppGen Resources
====

## References
APIs

* http://www.ykombinator.com/
* http://www.namemesh.com/
  * This plus csrf token plus cookie should do it: `http POST http://www.namemesh.com/company-name-generator/ word1=domestic,violence,nyc,health word2=cyclist,bridge,monthly fun=8`

Other tools

* http://stackoverflow.com/questions/8626829/possible-to-create-random-numbers-in-sass-compass

Big Apps information

* http://www.nycedc.com/services/nyc-bigapps/past-competitions
* http://nycbigapps.com/prizes
* http://nycbigapps.com/rules

## Socrata SODA API
Get the schema and metadata of a dataset (with httpie).

    http --json https://data.cityofnewyork.us/views/f4yq-wry5

More about the API

    http://dev.socrata.com/docs/endpoints

We can probably use the API directly in our apps.

## Using
Download the listings from Socrata.

    ./datasets-download.sh

Extract the viewids from them.

    ./datasets-parse.py

Download the metadata for each view.

    ./views-download.sh

## Things to randomize

* Tile server
* Url file extension (php, asp, cgi)
* Font size of title
* App name (from a startup name generator)
* App text (from the TED talk generator)
* App logo (from a logo generator)
* Typeface
* Background texture
* Colors
* Footer stickiness

## Useful queries
Longitude and latitude

    select distinct dataset from (select * from c where column like 'Longitude%' or column like 'Latitude%');

Things with addresses

    cd data/downloads/rows
    head -n1 *|grep -i -B1 zip

## Something
Something gets the address and description from the metadata, then geocodes the addresses
from the rows, caching the geocoder results, and returns a GeoJSON of the table, with the
description concatenated in an appropriate place.

## Things to extract for the map

* Longitude, latitude
* Description (JSON list of all the strings? Biggest cells?)
* Title
* Contact information (Phone number? Website?)


nyc-data-downloads
------------------------------------------------------------------------------------------------
**Title**: nyc-data-downloads

**Web address**: https://api.github.com/repos/tlevine/nyc-data-downloads

**Date**: April 2013

**Description**: 

nyc-data-geojson
------------------------------------------------------------------------------------------------
**Title**: nyc-data-geojson

**Web address**: https://api.github.com/repos/tlevine/nyc-data-geojson

**Date**: April 2013

**Description**: 

nyc-data-music
------------------------------------------------------------------------------------------------
**Title**: nyc-data-music

**Web address**: https://api.github.com/repos/tlevine/nyc-data-music

**Date**: June 2013

**Description**: 

nyc-open-data
------------------------------------------------------------------------------------------------
**Title**: nyc-open-data

**Web address**: https://api.github.com/repos/tlevine/nyc-open-data

**Date**: April 2013

**Description**: NYC Open Data
=======
These are almost all of the datasets from New York City's Socrata open
data portal. `views` are the metadata files, and `rows` are the tables.

The biggest few datasets (311 requests, Brooklyn public library catalog
and taxi drivers) are not in here.


nyc-subway-usage
------------------------------------------------------------------------------------------------
**Title**: nyc-subway-usage

**Web address**: https://api.github.com/repos/tlevine/nyc-subway-usage

**Date**: March 2013

**Description**: This shows New York City subway usage based on [turnstile data](http://www.mta.info/developers/turnstile.html). Hovering over a day shows the number of entries through subway turnstiles in that day.

Notice the clear effects of hurricanes Irene and Sandy.

More details and source code [available](https://github.com/ajschumacher/datathon). Visualization based (heavily) on Mike Bostock's excellent [example](http://bl.ocks.org/mbostock/4063318).


oakland-fppc
------------------------------------------------------------------------------------------------
**Title**: oakland-fppc

**Web address**: https://api.github.com/repos/tlevine/oakland-fppc

**Date**: October 2013

**Description**: * [dataset](https://data.oaklandnet.com/dataset/E-Filed-FPPC-Form-460-496-497-461-465-transactions/68fg-z9fi)
* [form](http://www.fppc.ca.gov/forms/1-05forms/460.pdf)


occupar
------------------------------------------------------------------------------------------------
**Title**: occupar

**Web address**: https://api.github.com/repos/tlevine/occupar

**Date**: March 2013

**Description**: External stresses mediate an association between negative perception of activism and a decision never to participate
======
Thomas Levine and Mimiko Watanabe



<style>
/* http://stackoverflow.com/questions/7698764/custom-bullet-symbol-for-li-elements-in-ul-that-is-a-regular-character-and */
ul {
  list-style: none;
  margin-left: 0;
  padding-left: 1em;
  text-indent: -1em;
}
li:before { content: "a! "; }
</style>
## Research question
We wanted to test whether a desire to participate in activism was associated
with increased participation in activism and whether this relationship was
mediated by external stresses.

    +-----------------+                   +-----------------+
    | Interest in     |                   | Participation   |
    | participating   |   ------------>   | in activism     |
    | in activism     |                   |                 |
    +-----------------+        / \        +-----------------+
                                | 
                        
                       +-----------------+
                       | External stress |
                       | (Barriers to    |
                       | participation)  |
                       +-----------------+

When we discuss "interest in participation", we are referring to more
fundamental drivers in considering whether to participate, such as
agreement with the goals of the activism or belief that the activism
will be effective.

"External stresses" are secondary, exogenous considerations that might
prevent participants from participating in activism that they would
support in concept. For example, one strong external stress would
be working multiple jobs.

## The questionnaire
A questionnaire was developed using critical PAR[*] as the methodological approach.
It was based on the collective experiences of the researchers as CUNY students.
Several pilot surveys were distributed on CUNY campuses and at the Free University, an activist event in Madison Square Park that occurred on May 1, 2012. This enabled researchers to revise and build the survey based on feedback from students. Insight from CUNY students was essential in the construction of the survey. Distribution of the final version of the survey began May 1, 2012 and continued until September 16, 2012. Surveys were collected on CUNY campuses, in classrooms, at CUNY activist events, and online.

<small>[*] Torre, M. E. and Fine, M. (2011), A Wrinkle in Time: Tracing a Legacy of Public Science through Community Self-Surveys and Participatory Action Research. Journal of Social Issues, 67: 106a121. doi: 10.1111/j.1540-4560.2010.01686.x</small>

### Questionnaire distribution
Researchers made contact on various CUNY campuses with professors who might be willing to distribute surveys to students during class time. An effort was made to ensure that the various types of CUNY campuses were included in the study (i.e. community colleges, senior colleges, etc) as well as various types of classes (day classes, night classes, and summer classes). If professors responded positively, a time was arranged during class when researchers could distribute surveys. Surveys
were distributed to childrenas studies classes at Brooklyn College, sociology classes at Hunter College, psychology classes at LaGuardia, research methods classes at the CUNY Graduate Center, classes at Medgar Evers college, and classes at the School of Professional Studies. A researcher was present in each classroom to oversee the completion of surveys and collect them. Researchers gave participants a short description of the survey, indicating that researchers were CUNY students interested
in studying activism on CUNY campuses. Anonymity of participants was assured. Consent from participants was obtained verbally and they were given about ten minutes to complete the survey. One professor provided students with a link to fill out the survey online. The remaining surveys were distributed during activist events, including the Free University at Madison Square Park on May 1st, 2012. The Free University was an event where workshops and classes were set up by student activists
in a public space (Madison Square Park) and open to the public. Many CUNY students attended the event, and some took the time to fill out the survey. Surveys were also collected at the CUNY Graduate Center general assembly, which is a student activist event where students meet to discuss issues and organize. Surveys were also distributed in the Hunter College cafeteria and student lounge, as well as at a student speak-out activist event to students who were willing to complete them,. In
these cases, researchers gave participants ten minutes to complete the survey and acquired verbal consent. When researchers obtained a substantial number of surveys from a diverse group of CUNY campuses, data collection ceased. A code book for the survey was created, and all researchers participated in data entry, and eventually compiled all entered data into one Excel spreadsheet.

### Representativeness of the questionnaire
This sample consisted of 304 current students attending various CUNY campuses. The survey was distributed on a total of twelve CUNY campuses, mostly from Hunter College (29% of sample), Brooklyn College (19%), Medgar Evers College (16%), Laguardia Community College (13%), and the Graduate Center (10%). The remaining CUNY schools include the School of Professional Studies (5%), Baruch College (1.6%), John Jay College of Criminal Justice (.7%), Borough of Manhattan Community College
(.3%), City College of New York (.3%), Lehman College (.3%), and Queens College (.3%). Several participants did not provide their campus name (5%). Students were pursuing various degrees, most commonly a Bachelors degree (60%), followed by Associates (13%), Doctorate (10%), Masters (5%), Certificate (1%), or Non-degree seeking students (1%), and 11% were unknown. When asked for their gender, the majority of participants  identified as female (67.4%), while 27% were male, 2% were queer, and
3% did not include their answer in this portion of the survey. Official records state that CUNY students are 58% female and 42% male (CUNY Office of Institutional Research and Assessment, 2011), although it should be noted they do not include alternative gender categories. It also states that 28.2% of CUNY students are age 25 or older. The ages of this sample ranged from 18 to 58, with 50% of participants between the ages of 21 and 29. The average age of this sample was 26.5.

Based on all this, the sample seems fairly representative of the CUNY student body. 

## Approach
Preferably, we would have had each participant answer the same questions
about her level of interest of participation (such as whether she agrees
with the movement's views or whether she think the movement will have
positive impact) and her external stresses (like working overtime) and her
level of participation. The questionnaire wasn't quite like this, so
we had to adapt the questions somewhat.

The questionnaire asked one question about level of participation
to all participants (question 8).

> **8** Have you ever attended an event on Occupy CUNY/ activism for higher education (e.g., a march, a talk, a general assembly/GA)?
>
> * Yes, and I want to participate again because ________
> * Yes, but I do not want to participate again because ________
> * No, and I will not participate because ________
> * No. I would like to participate but ________

The questions about interest in participation (question 7) and
about external stress (question 4) were arranged such that people answered different
questions depending on whether they had previously participated in
OccupyCUNY, which is quite related to their level of participation
in activism.

> **3** Are you participating in activism that addresses issues in higher education?  
>
> * Yes (go to 4a and 4b)
> * No  (go to 4c)
> 
> **4a** If Yes, what motivates you? ________
> 
> **4b** If yes, what enables you to participate? 
>
> * Financial resources
> * Job security
> * Childcare
> * Sense of belonging/community
> * Training in activism skills
> * Having a flexible schedule
> * Support from family, friends, peers, partners 
> * Encouragement at work and union
> * Access to ongoing sources of info
> * Encouragement at school (e.g., Class credits, teacher support, teacher allows me to miss class, inclusion in my academic research)
> * Other concrete things like _______
> 
> **4c** If no, what prevents you from participating? 
>
> * Lack of financial resources
> * Job insecurity
> * No childcare   
> * Inflexible schedule/No time
> * No support from family, friends, peers, partners 
> * Institutional discouragement/limitations at work and union
> * No access to ongoing sources of info
> * Institutional discouragement/limitations at school
> * Surveillance
> * Police violence/threat
> * Consequence of arrests
> * Other _______
> 

Question 4 is above, and question 7 is below.

> **7a** Overall, how do you feel about Occupy CUNY/ activism for higher education? Check only one. 
> 
> * Positive
> * Negative
> * Ambivalent/ mixed feelings  
> * I donat know much about it
> * I donat have an opinion
> 
> **7b** I feel positive about Occupy CUNY/ activism for higher education because: (Check all that apply.)
>
> * It opens a dialog about privatization of public higher education 
> * It is led by students for students   
> * I feel a part of a movement
> * It represents common issues faced by the majority of the population
> * It uses a democratic process  
> * It uses nonviolent methods
> * It uses direct action
> * Other (please specify) ________
> 
> **7c** I feel negative about Occupy CUNY/ activism for higher education because: (Check all that apply.)
>
> * It is not effective
> * It does not deal with issues that concern me
> * It is unfocused, and does not have a coherent message
> * It is disruptive to classes 
> * Other (please specify) ________


We thus had to separate the analysis by whether people had
previously participated in OccupyCUNY.

There wasn't much separation by level of participation within the
people who said they had participated. (There wer 64 people in the
higher group and 7 in the lower group.) Thus, we only conducted the
analysis for the respondents who said they had not participated in
OccupyCUNY. This led to a reversed framing of the relationship 
described above.

    +-----------------+                   +-----------------+
    | Negative atti-  |                   | Decisiveness    |
    | tude towards    |   ------------>   | about not       |
    | activism        |                   | participating   |
    +-----------------+        / \        +-----------------+
      (question 7c)             |            (question 8)
                      
                       +-----------------+
                       | External stress |
                       | (Barriers to    |
                       | participation)  |
                       +-----------------+
                          (question 4c)

### Variables
The questionnaire requested that question 4c only be answered by people who had
answered with one of the two "No" responses to question 8, so we used only the
subset of the data for which question 8 was one of these no responses. We thus
had a binary participation variable, `no.never`, which was true if the
participant said she would never participate and false if she said that she
had not participated but (would like to ?).

For both questions 4c and 7c, almost people checked either zero or one box.
For simplicity, we simply checked whether at least one of the respective
groups of check marks was checked. Thus, our external stress variable,
`has.external.stress`, was true if at least one of question 4c's boxes was checked
and false if none was checked. Similarly, our variable for negative perception
of activism, `has.negative.perception`, was true if at least one of 7c's
boxes was checked and false if none were.

We chose variable names that start with the word "has" so that it would be less
confusing to talk about, but both groups obviously have both external stresses
and some negative perception of activism; if these names bother you, mentally
switch the "has" to "has more".

## Analysis
For our sample, here are the proportions of people within each of the four groups
who said "never" (rather than "no, ... but").


```
  has.external.stress has.negative.perception p.no.never
1                 Yes                     Yes        40%
2                 Yes                      No      32.4%
3                  No                     Yes      55.6%
4                  No                      No       5.7%
```


### Logistic regression
We fit two logistic regressions and compared them with a likelihood ratio test.
As a null model, we fit the simple logistic regression of `no.never` as a
function of `has.negative perception`, ignorant of `has.external.stress`.


```

Call:
glm(formula = no.never ~ has.negative.perception, family = "binomial", 
    data = o)

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-1.047  -0.774  -0.774   1.314   1.644  

Coefficients:
                        Estimate Std. Error z value Pr(>|z|)    
(Intercept)               -1.053      0.191   -5.51  3.5e-08 ***
has.negative.perception    0.737      0.317    2.33     0.02 *  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 256.03  on 206  degrees of freedom
Residual deviance: 250.67  on 205  degrees of freedom
AIC: 254.7

Number of Fisher Scoring iterations: 4
```


The `has.negative.perception` coefficient is significantly different from zero,
indicating that negative perception is associated with participation.

Then we fit a logistic regression for the full relationship described above,
which adds the `has.external.stress` term.


```

Call:
glm(formula = no.never ~ has.external.stress * has.negative.perception, 
    family = "binomial", data = o)

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-1.274  -0.885  -0.885   1.354   2.393  

Coefficients:
                                            Estimate Std. Error z value Pr(>|z|)    
(Intercept)                                   -2.803      0.728   -3.85  0.00012 ***
has.external.stress                            2.068      0.757    2.73  0.00627 ** 
has.negative.perception                        3.027      0.990    3.06  0.00224 ** 
has.external.stress:has.negative.perception   -2.697      1.048   -2.57  0.01007 *  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 256.03  on 206  degrees of freedom
Residual deviance: 237.79  on 203  degrees of freedom
AIC: 245.8

Number of Fisher Scoring iterations: 5
```


All of the non-intercept coefficients significantly different from zero,
indicating that negative perception and external stress, separately, are both
associated with participation and that the associations are different
when combined. Specifically, someone with just negative perception or external stress
has higher odds of saying "never" than someone with neither, but the odds
are somewhere in between for someone who has both negative perception and
external stress.

Finally, we compared the two with a likelihood ratio test.


```
Likelihood ratio test for MLE method 
Chi-squared 2 d.f. =  12.88 , P value =  0.001594 
```


The likelihood ratio test finds that the data are significantly more likely
given the full model than the null model, suggesting, again, that external stress
significantly mediates the relationship between negative perception and
participation in activism.

### Ordinary least squares
We repeated this analysis with ordinary least squares (OLS) regressions,
swapping the likelihood ratio test for an F test. Logistic regression is more
appropriate for these data because they have a binary response, but the OLS
results are similar and may be easier for some people to understand.


```

Call:
lm(formula = no.never ~ has.negative.perception, data = o)

Residuals:
   Min     1Q Median     3Q    Max 
-0.422 -0.259 -0.259  0.578  0.741 

Coefficients:
                        Estimate Std. Error t value Pr(>|t|)    
(Intercept)               0.2587     0.0383    6.75  1.5e-10 ***
has.negative.perception   0.1631     0.0689    2.37    0.019 *  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Residual standard error: 0.458 on 205 degrees of freedom
Multiple R-squared: 0.0266,	Adjusted R-squared: 0.0219 
F-statistic:  5.6 on 1 and 205 DF,  p-value: 0.0188 
```

```

Call:
lm(formula = no.never ~ has.external.stress * has.negative.perception, 
    data = o)

Residuals:
   Min     1Q Median     3Q    Max 
-0.556 -0.324 -0.324  0.600  0.943 

Coefficients:
                                            Estimate Std. Error t value Pr(>|t|)   
(Intercept)                                   0.0571     0.0759    0.75   0.4526   
has.external.stress                           0.2669     0.0874    3.06   0.0026 **
has.negative.perception                       0.4984     0.1679    2.97   0.0034 **
has.external.stress:has.negative.perception  -0.4225     0.1836   -2.30   0.0224 * 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Residual standard error: 0.449 on 203 degrees of freedom
Multiple R-squared: 0.0734,	Adjusted R-squared: 0.0598 
F-statistic: 5.36 on 3 and 203 DF,  p-value: 0.00142 
```

```
Analysis of Variance Table

Model 1: no.never ~ has.negative.perception
Model 2: no.never ~ has.external.stress * has.negative.perception
  Res.Df RSS Df Sum of Sq    F Pr(>F)   
1    205  43                            
2    203  41  2      2.07 5.13 0.0067 **
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
```


## Not experimental
We intuitively suspect a particular direction of causality and have fit models
that suggest that direction, but the questionnaire gives us no formal idea
about the direction of causality; for example, it could be that the choice never
to participate leads to overstatement of reasons against participation and
of external stresses.

## Conclusion
External stresses mediate an association between negative perception of activism
and a decision never to participate in Occupy CUNY. For interpreting the
direction of this relationship, the following plot is helpful.

![plot of chunk summary.plot](figure/summary.plot.png) 


Negative perception makes people more likely to say they will "never"
participate, regardless of the level of external stress, and having external stress
makes this difference larger.

## Ideas for future study
It would be nice to pin down our nebulous concepts of external stress and negative perception.
Rather than qualifying the sorts of stresses and perceptions, we simply ignored which boxes people checked for questions 4c and 7c. It may be
informative to see how the particular box that was checked relates to
the question 8 result or other results.

We could also test the same relationship with the larger and more general
[Occupy Research General Survey](http://www.occupyresearch.net/orgs/).
It contains questions similar to the ones from the OccuPAR questionnaire
that we used as indicators of external stress, perception of activism and
participation in activism, but it has more responses from a wider movement.

## Relevance
The conclusion of this study can be applied to the marketing surrounding
activism. Aside from trying to influence people's perception of the
value of activism, marketing should consider activists' external stresses.

People who are trying to recruit and sustain activists should
focus on people with fewer external stresses or should provide support to
current and potential activists who have more external stresses.

People who are trying to suppress activism should focus on people with
more external stresses or should try to make the lives of potential and
current activists more stressful.

## The research team
The research team was initially composed of twelve individuals, including four CUNY undergraduate students, seven CUNY graduate students, and one alumnus. Most individuals on the research team had previous experience as student activists addressing issues at CUNY schools. A The group met consistently from February, 2012 to May, 2012 in order to develop a survey designed to find the supports and barriers to student activism on CUNY campuses. During this time, one undergraduate student and one graduate student dropped out of the study. The research team had ten members when the survey was finalized. Tom joined this team at the OccupyData hacakathon on March 1 to assist in the present analysis.


occupy-research-general-survey
------------------------------------------------------------------------------------------------
**Title**: occupy-research-general-survey

**Web address**: https://api.github.com/repos/tlevine/occupy-research-general-survey

**Date**: April 2013

**Description**: Analysis of the Occupy Research General Survey
=====

http://bit.ly/openqueshighlightoccupygen
http://www.occupyresearch.net/2012/03/23/preliminary-findings-occupy-research-demographic-and-political-participation-survey/


I started looking for similar responses to question 42.
Then I got bored. But I think it makes sense to calculate
pairwise Levenshtein distance and then make a network diagram.


I could also do something like what I did with OccuPAR


oh-my-zsh
------------------------------------------------------------------------------------------------
**Title**: oh-my-zsh

**Web address**: https://api.github.com/repos/tlevine/oh-my-zsh

**Date**: October 2012

**Description**: A handful of functions, auto-complete helpers, and stuff that makes you shout...


bq. "OH MY ZSHELL!"

h2. Setup

@oh-my-zsh@ should work with any recent release of "zsh":http://www.zsh.org/, the minimum recommended version is 4.3.9.

h3. The automatic installer... (do you trust me?)

You can install this via the command line with either `curl` or `wget`.

h4. via `curl`

@curl -L https://github.com/robbyrussell/oh-my-zsh/raw/master/tools/install.sh | sh@

h4. via `wget`

@wget --no-check-certificate https://github.com/robbyrussell/oh-my-zsh/raw/master/tools/install.sh -O - | sh@

h3. The manual way


1. Clone the repository

  @git clone git://github.com/robbyrussell/oh-my-zsh.git ~/.oh-my-zsh@

2. Create a new zsh config by copying the zsh template we've provided.

  *NOTE*: If you already have a ~/.zshrc file, you should back it up. @cp ~/.zshrc ~/.zshrc.orig@ in case you want to go back to your original settings.

  @cp ~/.oh-my-zsh/templates/zshrc.zsh-template ~/.zshrc@

3. Set zsh as your default shell:

  @chsh -s /bin/zsh@

4. Start / restart zsh (open a new terminal is easy enough...)

h3. Problems?

You _might_ need to modify your PATH in ~/.zshrc if you're not able to find some commands after switching to _Oh My Zsh_.

h2. Usage

* enable the plugins you want in your @~/.zshrc@ (take a look at @plugins/@ to see what's possible)
** example: @plugins=(git osx ruby)@
* Theme support: Change the @ZSH_THEME@ environment variable in @~/.zshrc@.
** Take a look at the "current themes":https://wiki.github.com/robbyrussell/oh-my-zsh/themes that come bundled with _Oh My Zsh_.
* much much more...  take a look at @lib/@ what _Oh My Zsh_ offers...

h2. Useful

the "refcard":http://www.bash2zsh.com/zsh_refcard/refcard.pdf is pretty tasty for tips.

h3. Customization

If you want to override any of the default behavior, just add a new file (ending in @.zsh@) into the @custom/@ directory.
If you have many functions which go well together you can put them as a *.plugin.zsh file in the @custom/plugins/@ directory and then enable this plugin.
If you would like to override the functionality of a plugin distributed with oh-my-zsh, create a plugin of the same name in the @custom/plugins/@ directory and it will be loaded instead of the one in @plugins/@.


h3. Uninstalling

If you want to uninstall it, just run @uninstall_oh_my_zsh@ from the command line and it'll remove itself and revert you to bash (or your previous zsh config).

h2. Help out!

I'm far from being a zsh-expert and suspect there are many ways to improve. If you have ideas on how to make the configuration easier to maintain (and faster), don't hesitate to fork and send pull requests!

h3. (Don't) Send us your theme! (for now)

-I'm hoping to collect a bunch of themes for our command prompts. You can see existing ones in the @themes/@ directory.-

We have enough themes for the time being. Please fork the project and add on in there, you can let people know how to grab it from there. 



h2. Contributors

This project wouldn't exist without all of our awesome users and contributors.

* "View our growing list of contributors":https://github.com/robbyrussell/oh-my-zsh/contributors

Thank you so much!


open-data-download
------------------------------------------------------------------------------------------------
**Title**: open-data-download

**Web address**: https://api.github.com/repos/tlevine/open-data-download

**Date**: October 2013

**Description**: Download all the datasets on open data portals.


openlawoakland
------------------------------------------------------------------------------------------------
**Title**: openlawoakland

**Web address**: https://api.github.com/repos/tlevine/openlawoakland

**Date**: October 2013

**Description**: ## Steps

### 1. Download
Download Oakland's laws as three PDF files containing scans of paper.
Save them as `pdf/${volume}.pdf`.

```sh
./1-download.sh
```

### 2. Images
Extract the images. Save them as `tmp/${volume}-${page}-${imageId}.${extension}`.

```sh
./2-images.sh
```

### 3. Text
Convert the images to text. Save them as `tmp/${volume}-${page}-${imageId}.txt`.

```sh
./3-text.sh
```

### 4. Chunk
Chunk the text into reasonable groupings, like generalized sections

```sh
./4-chunk.py
```

### 5. Hierarchy
Model the text as the specific hierarchy that the code follows, with titles,
sections, subsections, &c.

```sh
./5-hierarchy.py
```
### 6. Cross-references

```sh
./6-cross-references.py
```

### 7. Spellings
Correct spellings somewhere along the way. I don't know whether the beginning
or end will be easier. Maybe we'll eventually need two steps for this, with one
rough pass in the beginning and one better pass in the end.

### 8. Schema
Convert it into the DC code browser schema.

## Notes
I might skip some of the structuring and just extract citations for an
exercise in network analysis.


openlawoakland-pdf
------------------------------------------------------------------------------------------------
**Title**: openlawoakland-pdf

**Web address**: https://api.github.com/repos/tlevine/openlawoakland-pdf

**Date**: July 2013

**Description**: Source documents for [Open Law Oakland](https://github.com/tlevine/openlawoakland)


openlawsf
------------------------------------------------------------------------------------------------
**Title**: openlawsf

**Web address**: https://api.github.com/repos/tlevine/openlawsf

**Date**: July 2013

**Description**: Open San Francisco's law!

It's all here.
http://www.amlegal.com/library/ca/sfrancisco.shtml

The mobile version might be easier.
http://www.amlegal.com/nxt/gateway.dll/?fn=altmain-nf.htm$f=templates$3.0

It could go into a site like this.

* http://chicagocouncilmatic.org/
* http://marylandcode.org/

But for now, it's just gonna look like [OpenLaw DC](http://dccode.org/).


## Schema of the DC code browser
I'm fitting this into OpenLaw DC's schema. It's not documented,
so I document it here.

### Directory structure
The code is in JSON files and is accessed via AJAX requests.
There are two index files.

* [`/browser/index.json`](schema/index.json)
* [`/browser/sids.json`](schema/sids.json)

And there are a bunch of section files.

* [`/browser/sections/:section.json`](schema/4-102.json)

A section might be "[4-102](schema/4-102.json)"

### `index.json`
`index.json` has a list of titles and a list of sections.
Each title or section is a two-element list of the number
followed by the heading.

    {
      "sections":[
        ["1-101","Territorial area."],
        ["1-102","District created body corporate for municipal purposes."],
        ["1-103","Officers of corporation."],
        ...
      ],
      "titles":[
        ["1","Government Organization"],
        ["2","Government Administration"],
        ["3","District of Columbia Boards and Commissions"],
        ...
      ]
    }

I have a feeling that a DC "section" is equivalent to an
SF "section" and that a DC "title" is equivalent to a SF "code".

### `sids.json`
`sids.json` to be a list of most of the sections in `index.json`.
All sections in `index.json` are contained within `sids.json`, but
not vice-versa.

```python
sids = json.load(open('schema/sids.json'))
index = json.load(open('schema/index.json'))
print set(map(tuple, sids)).difference(set(map(tuple, index['sections'])))
print set(map(tuple, index['sections'])).difference(set(map(tuple, sids)))
```

### `:section.json`
This file is named after one of the sections in `sids.json`, and it looks like this.

    { "text":"",
      "historical":"Omission of text\n\nThe provisions of former ASS 4-102 have been omitted as obsolete, the Board referred to herein having been abolished.\n\nBoard of Public Welfare abolished\n\nThe Board of Public Welfare was abolished and the functions thereof transferred to the Board of Commissioners of the District of Columbia by Reorganization Plan No. 5 of 1952. Reorganization Order No. 58, as amended, redesignated as Organization Order No. 140 and amended, established, under the direction and control of a Commissioner, a Department of Public Welfare, headed by a Director with the purpose of planning, implementing, and directing public welfare programs. Reorganization Order No. 58 provided that the previously existing Board of Public Welfare would be abolished. That Order also transferred specified functions of the former Board to the Department of Public Health and the Department of Public Welfare. Functions of the Department of Public Welfare and of the Department of Public Health, as set forth in Organization Order Nos. 140 and 141, respectively, were transferred to the Department of Human Resources by Commissioner's Order No. 69-96, dated March 7, 1969, as amended by Commissioner's Order No. 70-83, dated March 6, 1970. The Department of Human Resources was replaced by the Department of Human Services by Reorganization Plan No. 2 of 1979, dated February 21, 1980.\n\nDC CODE ASS 4-102\n\nCurrent through December 11, 2012",
      "credits":"",
      "sections":[],
      "division":{
        "identifier":"I",
        "text":"Government of District."
      },
      "title":{
        "identifier":"4",
        "text":"Public Care Systems. (Refs & Annos)"
      },
      "chapter":{
        "identifier":"1",
        "text":"Public Welfare Supervision. (Refs & Annos)"
      },
      "heading":{
        "title":"4",
        "chaptersection":"102",
        "identifier":"4-102",
        "catch_text":"Board of Public Welfare--Created; successor to abolished Boards; employees transferred. [Omitted]"
      }
    }

I haven't yet figured out what all of that means.

## Changes in Schema for SF
In DC, the hierarchy is title, section, 


openprism
------------------------------------------------------------------------------------------------
**Title**: openprism

**Web address**: https://api.github.com/repos/tlevine/openprism

**Date**: October 2013

**Description**: Type your search in one search bar, and get results from all of the Socrata and CKAN portals.

## Technical
This is a static website that calls the Socrata and CKAN APIs. Build like so.

```sh
npm install -g browserify
npm install
browserify web/index.js -o bundle.js
```

## References

Data portal search API documentation

* [Junar](http://wiki.junar.com/index.php/API)
* [Socrata](https://github.com/jasonlally/open-data-browser/blob/dev/data/dataportalapi.py)
* [CKAN](http://docs.ckan.org/en/ckan-1.7/apiv3.html)

## To do

* Add [OpenDataPhilly](http://opendataphilly.org/api-doc/).


openstates
------------------------------------------------------------------------------------------------
**Title**: openstates

**Web address**: https://api.github.com/repos/tlevine/openstates

**Date**: June 2013

**Description**: The Open State Project collects and makes available data about state legislative activities, including bill summaries, votes, sponsorships and state legislator information. This data is gathered directly from the states and made available in a common format for interested developers, through a JSON API and data dumps.

Links
=====

* `Open State Project API <http://openstates.org/api/>`_
* `Contributing Guidelines <http://openstates.org/contributing/>`_
* `Code on GitHub <http://github.com/sunlightlabs/openstates/>`_
* `Issue Tracker <http://code.google.com/p/openstates/issues/list>`_
* `Open State Project Google Group <http://groups.google.com/group/fifty-state-project>`_
* `Sunlight Labs <http://sunlightlabs.com>`_


openstreetmap-website
------------------------------------------------------------------------------------------------
**Title**: openstreetmap-website

**Web address**: https://api.github.com/repos/tlevine/openstreetmap-website

**Date**: June 2013

**Description**: # Description

This is the Rails port, the [Ruby on Rails](http://rubyonrails.org/)
application that powers [OpenStreetMap](http://www.openstreetmap.org).

The Rails port provides almost all the services which are available 
on the OpenStreetMap site, including:

* The web site itself, including the edit pages.
* The editing [API](http://wiki.openstreetmap.org/wiki/API_v0.6).
* Browse pages - a web front-end to the OpenStreetMap data.
* The user system, including preferences, diary entries, friends and
  user-to-user messaging.
* GPX uploads, browsing and API.

There are some non-Rails services which the site includes, for 
example; tiles, geocoding, GPX file loading. There are also some
utilities which provide other services on the OpenStreetMap site,
or improve its function, but are not integrated with the Rails 
port, for example; Osmosis, CGImap.

# License

This software is licensed under the [GNU General Public License 2.0](http://www.gnu.org/licenses/old-licenses/gpl-2.0.txt),
a copy of which can be found in the LICENSE file.

# Running it

You can find documentation on [how to setup and
run](http://wiki.openstreetmap.org/wiki/The_Rails_Port) the software
on the OpenStreetMap wiki.

# Hacking it

The canonical Git repository for this software is hosted at
[git.openstreetmap.org](http://git.openstreetmap.org/?p=rails.git),
but much of the development is done on GitHub and for most people
[this repository on Github](https://github.com/openstreetmap/openstreetmap-website)
will be a better place to start.

Anybody hacking on the code is welcome to join the
[rails-dev](http://lists.openstreetmap.org/listinfo/rails-dev) mailing
list where other people hacking on the code hang out and will be happy
to help with any problems you may encounter. If you are looking for a
project to help out with, please take a look at the list of 
[Top Ten Tasks](http://wiki.openstreetmap.org/wiki/Top_Ten_Tasks) that
EWG maintains on the wiki.

There are also weekly IRC meetings, at 1800 GMT on Mondays in #osm-ewg on
the OFTC network where questions can be asked and ideas discussed. For more 
information, please see [the EWG page]
(http://www.osmfoundation.org/wiki/Engineering_Working_Group#Meetings). You can
join the channel using your favourite IRC client or [irc.openstreetmap.org](http://irc.openstreetmap.org/).

## Rails

If you're not already familiar with Ruby on Rails then it's probably
worth having a look at [Rails Guides](http://guides.rubyonrails.org/) for an introduction.

While working with Rails you will probably find the [API documentation](http://api.rubyonrails.org/)
helpful as a reference.

## Coding style

When writing code it is generally a good idea to try and match your
formatting to that of any existing code in the same file, or to other
similar files if you are writing new code. Consistency of layout is
far more important that the layout itself as it makes reading code
much easier.

One golden rule of formatting -- please don't use tabs in your code
as they will cause the file to be formatted differently for different
people depending on how they have their editor configured.

## Testing

Having a good suite of tests is very important to the stability and
maintainability of any code base. The tests in the Rails port code are
by no means complete, but they are extensive, and must continue to be
so with any new functionality which is written. Tests are also useful
in giving others confidence in the code you've written, and can
greatly speed up the process of merging in new code.

When hacking, you should:

* Write new tests to cover the new functionality you've added.
* Where appropriate, modify existing tests to reflect new or changed
functionality.
* Never comment out or remove a test just because it doesn't pass.

## Comments

Sometimes it's not apparent from the code itself what it does, or,
more importantly, **why** it does that. Good comments help your fellow
developers to read the code and satisfy themselves that it's doing the
right thing.

When hacking, you should:

* Comment your code - don't go overboard, but explain the bits which
might be difficult to understand what the code does, why it does it
and why it should be the way it is.
* Check existing comments to ensure that they are not misleading.

## Committing

When you submit patches, the project maintainer has to read them and
understand them. This is difficult enough at the best of times, and
misunderstanding patches can lead to them being more difficult to
merge. To help with this, when submitting you should:

* Split up large patches into smaller units of functionality.
* Keep your commit messages relevant to the changes in each individual
unit.

When writing commit messages please try and stick to the same style as
other commits, namely:

* A one line summary, starting with a capital and with no full stop.
* A blank line.
* Full description, as proper sentences with capitals and full stops.

For simple commits the one line summary is often enough and the body
of the commit message can be left out.

## Sending the patches

If you have forked on GitHub then the best way to submit your patches is to
push your changes back to GitHub and then send a "pull request" on GitHub.

Otherwise you should either push your changes to a publicly visible git repository
and send the details to the [rails-dev](http://lists.openstreetmap.org/listinfo/rails-dev)
list or generate patches with `git format-patch` and send them to the
[rails-dev](http://lists.openstreetmap.org/listinfo/rails-dev) list.



opt
------------------------------------------------------------------------------------------------
**Title**: opt

**Web address**: https://api.github.com/repos/tlevine/opt

**Date**: December 2012

**Description**: ~/opt
====
This is Tom's ~/opt directory for scripts that he hasn't yet felt like
packaging properly.


orientation-codes
------------------------------------------------------------------------------------------------
**Title**: orientation-codes

**Web address**: https://api.github.com/repos/tlevine/orientation-codes

**Date**: October 2012

**Description**: orientation code's for Becca's student's thing

convert original codes to angles

* `a`: first euler angle
* `b`: first euler angle
* `c`: first euler angle


orm-finalip
------------------------------------------------------------------------------------------------
**Title**: orm-finalip

**Web address**: https://api.github.com/repos/tlevine/orm-finalip

**Date**: June 2013

**Description**: 

overflows-ecohack3
------------------------------------------------------------------------------------------------
**Title**: overflows-ecohack3

**Web address**: https://api.github.com/repos/tlevine/overflows-ecohack3

**Date**: November 2012

**Description**: Using recent rainfall figures to predict Newtown Creek sewer overflows
===

## Goal
We want to predict whether a sewer will overflow soon based on current
rainfall data in order to make [DontFlush.me](http://dontflush.me)'s alerts
more relevant.


## Data sources

### Overflow statistic
We received a book from a FOIA request. This book contains graphs of, among
other things, sewer overflow incidents in New York City during the top 10
storms of 2011 for each of the 14 sewage treatment plants.

> Dates of the top ten storms, in the order they are presented in the book
> 
> * 8/14/2011
> * 8/27/2011
> * 9/6/2011
> * 3/6/2011
> * 4/16/2011
> * 11/22/2011
> * 10/29/2011
> * 9/23/2011
> * 5/17/2011
> * 12/7/2011

For each of these 10 storms, the graphs present a 48-hour or 60-hour window
of data around the storm. For 60-hour widows, we only used the first 48-hours.
This results in a total of 7720 observation-hours, with one observation per
hour.

![Sewer activity plot from December 7, 2012](2012-12-07.jpg)

### Rainfall statistic
We collected rainfall statistics from Weather Underground. We chose this
because of the [practical applications blah blah]


## Methods

### Data collection
We focused on the Newtown Creek sewershed. We have 480 observation-hours
(10 storms at 14 plants) in the book. We measured overflow as hours between
an NC throttle start (solid diamond) and an NC throttle end (empty diamond),
inclusive. For example, the two darkened bands on this plot, from hours t1
to t2, indicate overflow periods.

![Sewer activity plot from December 7, 2012 with shaded regions representing overflow periods as explained in the above paragraph](2012-12-07-annotated.jpg)

The MPS throttle start (solid circle) and end (empty circle) apparently
indicates overflows coming from the Manhattan pump station, which gets sent
through the Newtown Creek station. We ignored these overflows.

For each of these hours, we also acquired the most recent observation from
Weather Underground. This resulted in a table that looked like this.

    Date  Hour After 9 am  Overflowing?  Last rainfall figure
    ----  ---- ----------  ------------  --------------------
               No          Yes

To account for the diurnal flow, we added an "after 9 am" variable, which was
"no" for midnight to 9 am and "yes" for 10 am on.

    Date  Hour After 9 am  Overflowing?  Last rainfall figure
    ----  ---- ----------  ------------  --------------------
               No          Yes

### Model
We used the following model (in R formula syntax).

    overflowing ~ Last rainfall figure + after 9 am

We also made some plot

blah blah


Figure out
* that weird spike of falses
* 100 records lost in the join
* what family of distribution should the rainfall follow theoretically

Add the pictures.
Verify the paper API data by connecting frank's, tom's and casey's.

## Conclusions
2 mm is a decent cut-off for the guess

![This is a plot of stacked dots with one dot per observation hour, precipitation rate on the x axis and number of occurrences on the y-axis. It indicates that overflows mainly occur when precipitation rate is above 2 mm per hour.](threshold.identification-custom.png)

something about whether after 9 am matters








## Remaining tasks

1. ~~Get throttling events: Input throttling data from Paper API~~
2. ~~Get precipitation events: Convert Weather data from JSON to CSV~~
3. ~~Build algorithm to correlate throttling events with precipitation events~~
4. Refactor alert system to leverage algorithm from step 3
5. Visualize alert data (TBD)

## Contributors
* Leif: gathering weather data and data entry (http://github.com/lpercifield)
* Tom: understanding the stats and data entry (http://github.com/tlevine)
* Carl: visualization of weather data (http://github.com/c4rl)
* Casey: data entry and calculations (http://github.com/caseytwebb)
* Mike: data visualzation and structure in tableau (http://github.com/acceleratormt)
* Frank: Excel expert

## Building the website
Run `./build.sh`, then check out the `gh-pages` branch, wrap the `index.html`
in the head and foot, and move the files in `tmp` to the repository root.

    git checkout master
    ./build.sh
    git checkout gh-pages
    mv tmp/*.png tmp/*.jpg .
    cat template/index.html.head tmp/index.html template/index.html.foot > index.html
    git commit .


packrat
------------------------------------------------------------------------------------------------
**Title**: packrat

**Web address**: https://api.github.com/repos/tlevine/packrat

**Date**: August 2012

**Description**: PackRat: Backups that you can't delete
=====

I back things up on email, Gitorious, SmugMug, GitHub, Google Docs, &c. rather
than on my own hardware because it's **hard to delete** them from these
services and hard to lose my authentication credentials. The delete buttons are
buried inside of menus, and that button normally puts the stuff in some
intermediate place (the "trash") before deleting them. And I can change the
authentication credentials as long as I have access to the right email account.

Why not have a more general, less clumsy thing that works like that?

I propose a tool that makes it easy to take backups and hard to delete them.
Let's call it "PackRat". Minimally, it has a command-line interface with four
commands.

    # Upload one file. Name clashes are reported;
    # existing files are never overwritten.
    packrat put foobar.tar.gz
    
    # List the files that have been uploaded
    packrat list

    # Retrieve one file that has been backed up
    packrat get foobar.tar.gz
    
    # "Trash" a file. The file will be deleted in 30 days, and multiple
    # stages of confirmation will be requested.
    packrat delete foobar.tar.gz

It could also have more commands and be a website and whatnot. If I make it,
there will probably be a minimal client that sends files and commands to a
server over SSH. The server would send files to some fancy storage backend,
like a cloud hosting provider or a distributed filesystem. I can make this
in a weekend.

**If I made this or something like it and let you use it for free, would you
use it?**


pandas
------------------------------------------------------------------------------------------------
**Title**: pandas

**Web address**: https://api.github.com/repos/tlevine/pandas

**Date**: May 2013

**Description**: =============================================
pandas: powerful Python data analysis toolkit
=============================================

.. image:: https://travis-ci.org/pydata/pandas.png
        :target: https://travis-ci.org/pydata/pandas

What is it
==========

**pandas** is a Python package providing fast, flexible, and expressive data
structures designed to make working with "relational" or "labeled" data both
easy and intuitive. It aims to be the fundamental high-level building block for
doing practical, **real world** data analysis in Python. Additionally, it has
the broader goal of becoming **the most powerful and flexible open source data
analysis / manipulation tool available in any language**. It is already well on
its way toward this goal.

Main Features
=============

Here are just a few of the things that pandas does well:

  - Easy handling of **missing data** (represented as NaN) in floating point as
    well as non-floating point data
  - Size mutability: columns can be **inserted and deleted** from DataFrame and
    higher dimensional objects
  - Automatic and explicit **data alignment**: objects can be explicitly
    aligned to a set of labels, or the user can simply ignore the labels and
    let `Series`, `DataFrame`, etc. automatically align the data for you in
    computations
  - Powerful, flexible **group by** functionality to perform
    split-apply-combine operations on data sets, for both aggregating and
    transforming data
  - Make it **easy to convert** ragged, differently-indexed data in other
    Python and NumPy data structures into DataFrame objects
  - Intelligent label-based **slicing**, **fancy indexing**, and **subsetting**
    of large data sets
  - Intuitive **merging** and **joining** data sets
  - Flexible **reshaping** and pivoting of data sets
  - **Hierarchical** labeling of axes (possible to have multiple labels per
    tick)
  - Robust IO tools for loading data from **flat files** (CSV and delimited),
    Excel files, databases, and saving / loading data from the ultrafast **HDF5
    format**
  - **Time series**-specific functionality: date range generation and frequency
    conversion, moving window statistics, moving window linear regressions,
    date shifting and lagging, etc.

Where to get it
===============

The source code is currently hosted on GitHub at: http://github.com/pydata/pandas

Binary installers for the latest released version are available at the Python
package index::

    http://pypi.python.org/pypi/pandas/

And via ``easy_install`` or ``pip``::

    easy_install pandas
    pip install pandas

Dependencies
============

  * `NumPy <http://www.numpy.org>`__: 1.6.1 or higher
  * `python-dateutil <http://labix.org/python-dateutil>`__ 1.5 or higher
  * `pytz <http://pytz.sourceforge.net/>`__
     * Needed for time zone support with ``date_range``

Highly Recommended Dependencies
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  * `numexpr <http://code.google.com/p/numexpr/>`__: to accelerate some expression evaluation operations
       also required by `PyTables`
  * `bottleneck <http://berkeleyanalytics.com/bottleneck>`__: to accelerate certain numerical operations

Optional dependencies
~~~~~~~~~~~~~~~~~~~~~

  * `Cython <http://www.cython.org>`__: Only necessary to build development
    version. Version 0.17.1 or higher.
  * `SciPy <http://www.scipy.org>`__: miscellaneous statistical functions
  * `PyTables <http://www.pytables.org>`__: necessary for HDF5-based storage
  * `matplotlib <http://matplotlib.sourceforge.net/>`__: for plotting
  * `statsmodels <http://statsmodels.sourceforge.net/>`__
     * Needed for parts of :mod:`pandas.stats`
  * `openpyxl <http://packages.python.org/openpyxl/>`__, `xlrd/xlwt <http://www.python-excel.org/>`__
     * openpyxl version 1.6.1 or higher, for writing .xlsx files
     * xlrd >= 0.9.0
     * Needed for Excel I/O


Installation from sources
=========================

To install pandas from source you need ``cython`` in addition to the normal dependencies above,
which can be installed from pypi::

    pip install cython

In the ``pandas`` directory (same one where you found this file after cloning the git repo), execute::

    python setup.py install

or for installing in `development mode <http://www.pip-installer.org/en/latest/usage.html>`__::

    python setup.py develop

Alternatively, you can use `pip` if you want all the dependencies pulled in automatically
(the optional ``-e`` option is for installing it in
`development mode <http://www.pip-installer.org/en/latest/usage.html>`__)::

    pip install -e .

On Windows, you will need to install MinGW and execute::

    python setup.py build --compiler=mingw32
    python setup.py install

See http://pandas.pydata.org/ for more information.

License
=======

BSD

Documentation
=============

The official documentation is hosted on PyData.org: http://pandas.pydata.org/

The Sphinx documentation should provide a good starting point for learning how
to use the library. Expect the docs to continue to expand as time goes on.

Background
==========

Work on ``pandas`` started at AQR (a quantitative hedge fund) in 2008 and
has been under active development since then.

Discussion and Development
==========================

Since ``pandas`` development is related to a number of other scientific
Python projects, questions are welcome on the scipy-user mailing
list. Specialized discussions or design issues should take place on
the pystatsmodels mailing list / Google group, where
``scikits.statsmodels`` and other libraries will also be discussed:

http://groups.google.com/group/pystatsmodels

  .. _NumPy: http://numpy.scipy.org/


paperdb
------------------------------------------------------------------------------------------------
**Title**: paperdb

**Web address**: https://api.github.com/repos/tlevine/paperdb

**Date**: January 2013

**Description**: * `author [schema.json]` prints an sql schema to stdout. You might save
    this to `schema.sql`.
* `compose [schema.json] [id]` prints an SVG paper document to stdout. You
    might save this to `[id].sql`.
* `readpaper [jpeg image]` prints an sql insert from a scan of a paper
    record.


phone-numbers
------------------------------------------------------------------------------------------------
**Title**: phone-numbers

**Web address**: https://api.github.com/repos/tlevine/phone-numbers

**Date**: September 2013

**Description**: Most phone numbers are not active, so we expect a
rather low response rate when we use random phone
numbers to sample people. This gets expensive.

To improve our response rate, we select numbers
based on historical responses. `phone.py` selects
new random numbers based on historically responsive
phone numbers.

This is how you generate 20 numbers based on the `BG_10K_12.csv` file.

    ./phone.py BG_10K_12.csv 20

## To do
In approximate order of priority

1. Explain how it works
2. Make pretty visuals to see what the groupings are;
    local area codes and lucky numbers will pop out.
3. Connect this to other information about the phone numbers so
    we can run fancy stratified samples. Possibilities include
  * how many times the number has been called
  * how many times the number has responded
  * which cell phone towers the calls came from
4. Consider whether short phone numbers should be handled differently.
5. Improve the smoothing algorithm
    Currently it's just +1 smoothing, and it doesn't handle short phone numbers.


php-heroku-proxy
------------------------------------------------------------------------------------------------
**Title**: php-heroku-proxy

**Web address**: https://api.github.com/repos/tlevine/php-heroku-proxy

**Date**: March 2013

**Description**: 

PKGBUILDs
------------------------------------------------------------------------------------------------
**Title**: PKGBUILDs

**Web address**: https://api.github.com/repos/tlevine/PKGBUILDs

**Date**: February 2012

**Description**: 

plan-things
------------------------------------------------------------------------------------------------
**Title**: plan-things

**Web address**: https://api.github.com/repos/tlevine/plan-things

**Date**: October 2013

**Description**: Plan Things
=====
Like notecards-on-a-table, but portable, digitized, scriptable
and in a git repository

    $ plan help

    Plan things, in a kind-of-xtreme way. Plans get saved in the
    ~/.plans directory, which you can version nicely in git.

    USAGE: plan show GROUP
           plan move [thing id] GROUP
           plan edit [thing id] [[task id]]
           plan help

    GROUP is one of the following.

      proposed
      current
      passed
      done

    plan show: List the things and tasks in a group.
      In addition to the four groups above, you can
      specify "all", which will list all the things.

    plan move: Move a thing to a different group.

    plan help: Show this help.

    plan edit: Edit the description of a story or task
               and the estimated duration of a task.

## Structure of the `~/.plans` directory
The plans directory is structured like so.

    ~/.plans/
      .gitignore
      .cache
      proposed/
        plan_trip/
          thing.sh
          book_flight.sh
          call_bob.sh
        ...
      current/
        make_a_cardboard_comma/
          thing.sh
          buy_glue.sh
          find_cardboard.sh
        ...
      passed/
        ...
      done/
        ...

Now I explain that structure in more precision.

### Root level
The `~/.plans` directory contais four directories,
called `proposed`, `current`, `passed`, and `done`.
These directories are called *groups*, and each group
ctories contains a bunch of *things*.

The `~/.plans` directory contais two other files,
`.gitignore` and `.cache`. `.gitignore` just says
`.cache\n`, and `.cache` is a cache file.

### Things
Each thing directory contains a file called `thing.sh`.
This file has most of the thing-specific information.
All of the other files are tasks, named `${task_name}.sh`,
and they contain the task-specific information.

The one piece of information that is not encoded in the
thing directory is the group of which the thing is a part.
This information is instead encoded as the group directory
in which the thing directory is located.

Newly created things start in the `proposed` directory.
Running `plan edit thing -[bcp]` moves them between
directories. They can only be moved to the "done"
directory/group from the "current" group, and this happens
when all of the tasks have been marked "done", with
`plan edit thing task -d`.

### File format of the `.sh` files
All of the variables are set as shell scripts that are sourced.
(It [doesn't seem](http://wiki.bash-hackers.org/howto/conffile)
like there's a native shell configuration language or anything.)

Here's an example thing file.

    MESSAGE=$(cat<<EOF
    As someone who runs plan-things on barebones computers,
    I want plan-things to work with POSIX-compliant shell rather
    than just BASH so that I can use plan-things on all my computers.
    EOF)

And here's a task file.

    DAYS=0.25
    MESSAGE=$(cat<<EOF
    Set up the tests to run in multiple shells.
    EOF)

## Development
Run tests with urchin.

    npm install -g urchin
    urchin ./test


plots
------------------------------------------------------------------------------------------------
**Title**: plots

**Web address**: https://api.github.com/repos/tlevine/plots

**Date**: February 2013

**Description**: Plots for your shell
=========

## Histograms
Plot a histogram given a file or stdin.

    USAGE: hist [breaks] [file]

Plot a histogram.

    $ echo 1.3 1.2 3.4 4 2.8 3 6.2 | hist 2
    
    4   *
    3   *
    2 * *
    1 * * *
      2 4 6

If you don't specify the breaks, we use my tick algorithm to choose
about eight ticks. It would look like this, except this isn't the
actual algorithm output.

    $ echo 1.3 1.2 3.4 4 2.8 3 6.2 | hist
    
    4     *
    3     *
    2 *   *
    1 *   * *   *
      1 2 3 4 5 6

## Bar plot
Given a factor variable, plot a bar plot

    USAGE: bar [file]

    $ echo duck duck goose | bar

    2 -----
    1 ----- -----
      duck  goose


plots-landing-page
------------------------------------------------------------------------------------------------
**Title**: plots-landing-page

**Web address**: https://api.github.com/repos/tlevine/plots-landing-page

**Date**: January 2013

**Description**: 

plots-leaflet-viewer
------------------------------------------------------------------------------------------------
**Title**: plots-leaflet-viewer

**Web address**: https://api.github.com/repos/tlevine/plots-leaflet-viewer

**Date**: January 2013

**Description**: ##PLOTS LEAFLET VIEWER##

A leaflet-based TMS viewer which accepts a TMS URL as a GET parameter. Good for fullscreen map viewing and embedding.

It accests parameters for a TMS as well as others, following this syntax:

* tms - with trailing slahs, such as http://mapknitter.org/tms/lone-rock-pond/
* lon - center longitude, required
* lat - center latitude, required
* zoom - zoom level, default 18
* minZoom - minimum zoom level, default 15
* maxZoom - maximum zoom level, default 22

Use it to make embed codes:

    <iframe style="border:none;" width="500" height="375" src="http://archive.publiclaboratory.org/leaflet/?tms=http://mapknitter.org/tms/lone-rock-pond/&amp;lon=-73.0733&amp;lat=43.96055&amp;zoom=18"></iframe>



plots-organizing
------------------------------------------------------------------------------------------------
**Title**: plots-organizing

**Web address**: https://api.github.com/repos/tlevine/plots-organizing

**Date**: January 2013

**Description**: # How to organize a public lab chapter #

You want to organize a bunch of people to do some project.

You need to connect people in these groups.

* People with expertise who want to do something.
* People with environmental concern who want to do something.
* People who want gear who want something done with it.

Tack your project onto an existing group or project.
For example, if there is already a group who goes canoing,
suggest bringing kites along to take pictures. Or, if a
group is already concerned about an issue similar to yours.

Understand the area of interest before observing it.


postsecret
------------------------------------------------------------------------------------------------
**Title**: postsecret

**Web address**: https://api.github.com/repos/tlevine/postsecret

**Date**: April 2013

**Description**: PostSecret
====
Here I save PostSecret images weekly.

Index pages go in `downloads/index/:datestamp`, and images go in
`downloads/images/:url`.

Download things.

    ./download.py

View the images

    ./view.sh

Generate a crontab for downloading these images every week.

    ./crontab.sh

There's a similar script [on ScraperWiki](https://scraperwiki.com/scrapers/postsecret/),
but it doesn't have as much data because ScraperWiki isn't very reliable.
Regardless, it's worth converting some of that data here because it goes back further.


postsecret-downloads
------------------------------------------------------------------------------------------------
**Title**: postsecret-downloads

**Web address**: https://api.github.com/repos/tlevine/postsecret-downloads

**Date**: April 2013

**Description**: 

postsecret-downloads-sw
------------------------------------------------------------------------------------------------
**Title**: postsecret-downloads-sw

**Web address**: https://api.github.com/repos/tlevine/postsecret-downloads-sw

**Date**: April 2013

**Description**: ScraperWiki PostSecret Data Migration
===
The old script is `old/postsecret.py` and `new/postsecret.sqlite`.
The new filesystem is in `new`. Run like so:

    ./migrate.py


prezto
------------------------------------------------------------------------------------------------
**Title**: prezto

**Web address**: https://api.github.com/repos/tlevine/prezto

**Date**: October 2012

**Description**: Prezto a Instantly Awesome Zsh
==============================

Prezto is the configuration framework for [Zsh][1]; it enriches the command line
interface environment with sane defaults, aliases, functions, auto completion,
and prompt themes.

Installation
------------

Prezto will work with any recent release of Zsh, but the minimum recommended
version is 4.3.10.

  1. Launch Zsh:

        zsh

  2. Clone the repository:

        git clone --recursive https://github.com/sorin-ionescu/prezto.git "${ZDOTDIR:-$HOME}/.zprezto"

  3. Create a new Zsh configuration by copying the Zsh configuration files
     provided:

        setopt EXTENDED_GLOB
        for rcfile in "${ZDOTDIR:-$HOME}"/.zprezto/runcoms/^README.md(.N); do
          ln -s "$rcfile" "${ZDOTDIR:-$HOME}/.${rcfile:t}"
        done

  4. Set Zsh as your default shell:

        chsh -s /bin/zsh

  5. Open a new Zsh terminal window or tab.

### Mac OS X

If you have administrator privileges, you must fix an Apple-introduced problem
in Mac OS X 10.5 Leopard by executing the following command, or BASH and Zsh
will have the wrong `PATH` when executed non-interactively.

    sudo chmod ugo-x /usr/libexec/path_helper

`path_helper` is intended to make it easier for installers to add new paths to
the environment without having to edit shell configuration files by adding
a file with a path to the */etc/paths.d* directory.

Unfortunately, `path_helper` always reads paths from */etc/paths* set by Apple
then paths from */etc/paths.d* set by third party installers, and lastly paths
from the `PATH` environment variable set by the parent process, which
ultimately is set by the user with `export PATH=...` Thus, it reorders path
priorities, and user */bin* directories meant to override system */bin*
directories end up at the tail of the array.

### Troubleshooting

If you are not able to find certain commands after switching to *Prezto*,
modify the `PATH` variable in *~/.zshenv* then open a new Zsh terminal
window or tab.

Usage
-----

Prezto has many features disabled by default. Read the source code and
accompanying README files to learn of what is available.

### Modules

  1. Browse */modules* to see what is available.
  2. Load the modules you need in *~/.zpreztorc* then open a new Zsh terminal
     window or tab.

### Themes

  1. For a list of themes, type `prompt -l`.
  2. To preview a theme, type `prompt -p name`.
  3. Load the theme you like in *~/.zpreztorc* then open a new Zsh terminal
     window or tab.

     ![sorin theme][2]

Customization
-------------

The project is managed via [Git][3]. It is highly recommend that you commit
your changes and push them to [GitHub][4] to not lose them. If you do not know
how to use Git, follow this [tutorial][5] and bookmark this [reference][6].

Resources
---------

The [Zsh Reference Card][7] and the [zsh-lovers][8] man page are indispensable.

License
-------

(The MIT License)

Copyright (c) 2009-2012 Robby Russell, Sorin Ionescu, and contributors.

Permission is hereby granted, free of charge, to any person obtaining a copy of
this software and associated documentation files (the "Software"), to deal in
the Software without restriction, including without limitation the rights to
use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies
of the Software, and to permit persons to whom the Software is furnished to do
so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

[1]: http://www.zsh.org
[2]: http://i.imgur.com/nBEEZ.png "sorin theme"
[3]: http://git-scm.com
[4]: https://github.com
[5]: http://gitimmersion.com
[6]: http://gitref.org
[7]: http://www.bash2zsh.com/zsh_refcard/refcard.pdf
[8]: http://grml.org/zsh/zsh-lovers.html



printer
------------------------------------------------------------------------------------------------
**Title**: printer

**Web address**: https://api.github.com/repos/tlevine/printer

**Date**: July 2013

**Description**: Tom's Printer
===========
Tom has a [thermal printer](http://learn.adafruit.com/mini-thermal-receipt-printer/microcontroller)
connected to an Arduino and accessed over a local network.

When it's running, you can print something like so.

    curl --data foo=bar http://printer

where `printer` is the printer's network address.

You can also print something by plugging a USB cable into the arduino and sending text over serial.

    # Directions here

## Installation
This requires an arduino with an ethernet shield and thermal printer.
You also need Ladyada's thermal printer library.


prior-inventions
------------------------------------------------------------------------------------------------
**Title**: prior-inventions

**Web address**: https://api.github.com/repos/tlevine/prior-inventions

**Date**: October 2013

**Description**: 

project-open-data.github.io
------------------------------------------------------------------------------------------------
**Title**: project-open-data.github.io

**Web address**: https://api.github.com/repos/tlevine/project-open-data.github.io

**Date**: August 2013

**Description**: # Project Open Data

[![Build Status](https://travis-ci.org/project-open-data/project-open-data.github.io.png?branch=master)](https://travis-ci.org/project-open-data/project-open-data.github.io)

## Problem this Solves

Technology moves much faster than policy ever could.  Often when writing policy for technology, agencies are stuck w/ outdated methods as soon as they publish new policies.

## How this Project Solves this Problem

This Appendix is meant to be a living document so that collaboration in the open data ecosystem is fostered and the continual update of technology pieces that affect update can happen on a more rapid pace.

## Where You Come In

Help the United States Government make its Open Data policy better by collaborating.  Please suggest enhancements by editing the content here.   

## How to Contribute

This project constitutes a collaborative work ("open source"). Federal employees and members of the public are encouraged to improve the project by contributing.

For information on how to contribute, please see the [how to contribute](CONTRIBUTING.md)

## License

The project [as originally published](#) constitutes a work of the United States Government and is not subject to domestic copyright protection under 17 USC ASS 105. Subsequent contributions by members of the public, however, retain their original copyright.

In order to better facilitate collaboration, the content of this project is licensed under the [Creative Commons 3.0 license](http://creativecommons.org/licenses/by/3.0/us/deed.en_US), and the underlying source code used to format and display that content is licensed under the [MIT license](http://opensource.org/licenses/mit-license.php). 

## Privacy

Comments, pull requests and any other messages received through this repository may be subject to the [Presidential Records Act](http://www.archives.gov/about/laws/presidential-records.html) and may be archived. Learn more at http://WhiteHouse.gov/privacy


prosciutto
------------------------------------------------------------------------------------------------
**Title**: prosciutto

**Web address**: https://api.github.com/repos/tlevine/prosciutto

**Date**: February 2013

**Description**: Proscuitto
======

Proscuitto is a simple plotting library in Haskell. I'd rather it use the
grammar of graphics, but then we'd need to figure out what data structure to use.

    scatter :: Num a => [(a,a)] -> Something



pyufs2
------------------------------------------------------------------------------------------------
**Title**: pyufs2

**Web address**: https://api.github.com/repos/tlevine/pyufs2

**Date**: June 2013

**Description**: Microformats parser in Python
======
This is a microformats parser in Python.

# How to

    import microformats as ufs
    fp = open('index.html')
    data = ufs.parse(fp)

`data` now contains the microformats data.

# Running tests
From the root of the repository,

    nosetests

Specs come from [here](https://github.com/G5/microformats2).


quinoa-index
------------------------------------------------------------------------------------------------
**Title**: quinoa-index

**Web address**: https://api.github.com/repos/tlevine/quinoa-index

**Date**: March 2013

**Description**: Quinoa index
===========================

The Quinoa index combines strange properties of municipalities in order to find peculiar municipalities.

## Running the Yoga script
Start selenium.

    java -jar bin/selenium-server-standalone-2.21.0.jar

Run the downloader script.

    yoga/oh_just_use_selenium.py

This results in a `yoga/yoga.sqlite`. Copy this to `/tmp/yoga.sqlite`, then
parse the raw html downloads with

    parse_locations.py

Results will be in `/tmp/yoga.sqlite`; move that wherever you want to keep it.

## American FactFinder
I downloaded that stuff from [here](http://factfinder2.census.gov/faces/tableservices/jsf/pages/productview.xhtml?pid=DEC_10_SF1_GCTP2.ST09&prodType=table).

## More ideas

Inspiration

* Collect both hip things and non-hip things.
* Places
  * [Ikaria](http://www.nytimes.com/2012/10/28/magazine/the-island-where-people-forget-to-die.html)
  * Ashville, NC
  * Ithaca, NY
  * Williamsburg
* Transportation
  * Bicycle usage
  * Car usage
  * Speed limits
* Shapefiles
* Census data
* Presence of groups like PLoTS, CfA, Awesome, &c.

Specific ideas

* [Walkscore](http://www.walkscore.com/professional/research.php)
* County-level election data might say something; the cool places I know of vote democrat.
    This might be quite correlated with urbanness.
* Suburu dealerships
* Food
  * Cafes
  * McDonalds, Pizza Hut, &c.
  * Organic producers, food cooperatives, &c.
* University locations
* Water quality
* Air quality


realtimestream
------------------------------------------------------------------------------------------------
**Title**: realtimestream

**Web address**: https://api.github.com/repos/tlevine/realtimestream

**Date**: September 2012

**Description**: # Realtime Stream Processing: Computing Guide #

This repo contains all the files required for the August 13th, 2012 tutorial "Realtime Stream Processing".  Below is the computing guide which outlines the technical requirements required to follow along.

Rendered slides can be view at http://mynameisfiber.github.com/realtimestream/

## General Requirements ##

You will need to have the following installed:

  * [Python 2.7](http://python.org/)
      * [tornado](http://pypi.python.org/pypi/tornado/2.4)
      * [numpy](http://pypi.python.org/pypi/numpy/1.6.2)
      * [pysimplehttp >=0.2.1](http://pypi.python.org/pypi/pysimplehttp/0.2.1)
      * [host_pool](http://pypi.python.org/pypi/host_pool/0.2)
  * [simplehttp](https://github.com/bitly/simplehttp)
  * [redis](http://redis.io/)

Which require the following dependencies:

  * [json-c](https://github.com/jehiah/json-c)
  * [libevent 1.4](http://libevent.org/)
  * [openssl](http://www.openssl.org/)


## OSX guide ##

The easiest way to install the required programs is to first install [homebrew](https://github.com/mxcl/homebrew/) and then use `brew` and `pip` to download the required files.  Also, there is a great tutorial for steps 1 and 2 on this guide located at http://www.moncefbelyamani.com/how-to-install-xcode-homebrew-git-rvm-ruby-on-mac/ (follow steps 1-3 from that tutorial).

  1. Install GCC from XCode (see step 1-2 in [this tutorial](http://www.moncefbelyamani.com/how-to-install-xcode-homebrew-git-rvm-ruby-on-mac/))
  2. Follow the instructions on http://mxcl.github.com/homebrew/ to install homebrew (see step 3 in [this tutorial](http://www.moncefbelyamani.com/how-to-install-xcode-homebrew-git-rvm-ruby-on-mac/))
  3. `wget "https://github.com/downloads/mynameisfiber/realtimestream/brew_formulas.0.2.tar.gz"`
  4. `tar -xvzf "brew_formulas.0.2.tar.gz" -C /usr/local/Library/Formula/`
  5. `rm "brew_formulas.0.2.tar.gz"`
  6. `brew install simplequeue pubsub ps_to_http redis python`
  7. `brew test simplequeue pubsub`
  8. `pip install numpy "pysimplehttp>=0.2.1" redis ujson host_pool` (you may want to consider using `virtualenv`)
    * Note: If you get an error installing ujson, don't sweat it.  Nothing depends on it, but it is a really great and fast alternative to json/simplejson.  Simply run `sudo pip "pysimplehttp>=0.2.1" redis host_pool`

## Ubuntu guide ##

To get started in ubuntu, we first use `aptitude` to get dependencies, and then we use `pip` to download the python libraries and manually compile the requirements in `simplehttp`.

  1. `sudo apt-get install make gcc libevent1-dev libcurl4-openssl-dev redis-server`
  2. `sudo apt-get install ipython python-pip python-redis python-numpy git`
  3. `sudo pip install "pysimplehttp>=0.2.1" ujson host_pool`
    * Note: If you get an error installing ujson, don't sweat it.  Nothing depends on it, but it is a really great and fast alternative to json/simplejson.  Simply run `sudo pip install "pysimplehttp>=0.2.1" host_pool`
  4. `git clone https://github.com/bitly/simplehttp.git`
  5. `cd simplehttp/simplehttp`
  6. `make ; sudo make install`
  7. `cd ../pubsub/`
  8. `make ; sudo make install`
  9. `cd ../simplequeue`
  10. `make ; sudo make install`

## Arch Linux guide ##

To get started in ubuntu, we first use `pacman` to get dependencies, and then we use `pip2` to download the python libraries and manually compile the requirements in `simplehttp`.

  0. (Dunno what to put here; I already had most of the dependencies)
  1. `sudo pip2 install numpy "pysimplehttp>=0.2.1" host_pool`
  2. `sudo pip2 install ujson`
    * Note: If you get an error installing ujson, don't sweat it.  Nothing depends on it, but it is a really great and fast alternative to json/simplejson. 
  4. `git clone https://github.com/bitly/simplehttp.git`
  5. `cd simplehttp/simplehttp`

I'd prefer if the following steps were a package built with makepkg.

  6. `make ; sudo make install`
  7. `cd ../pubsub/`
  8. `make ; sudo make install`
  9. `cd ../simplequeue`
  10. `make ; sudo make install`

## Windows guide ##

We currently do not support windows.  If you have any success installing the required programs in windows, please tell us so we can update this section!  For the python requirements, [`pip`](http://pypi.python.org/pypi/pip) and [`enthought`](http://www.enthought.com/) will be useful.

### A note from enthought ###

Enthought, a firm that specializes in scientific applications using Python, is offering a one-year subscription to its Enthought Python Distribution (EPD) to tutorial attendees. For those unfamiliar with Python, EPD makes it simple to set up a complete Python environment. EPD offers a comprehensive array of libraries that include machine learning, statistical, and various data manipulation packages. Subscribers also have access to Enthoughtas private repository and package management tools, as well as a series of instructional webinars on various data analysis topics. EPD works just like a standard python environment, so you can still use pip and easy_install as you normally would.
If youad like a free one-year subscription to the EPD: 

1. Register for an Enthought account. 

2. Email datagotham@enthought.com and let us know you've registered as part of Datagotham. We will then activate your subscription and email further instructions.

Have fun at the tutorials!


rectangle-traversal
------------------------------------------------------------------------------------------------
**Title**: rectangle-traversal

**Web address**: https://api.github.com/repos/tlevine/rectangle-traversal

**Date**: July 2012

**Description**: Direction of traversal of rectangles by people as a function of sight lines
=======

## Introduction

For a while, I've been thinking that a shorter person is more likely to start
with the short leg when traversing a rectangle. After a further ponder, I
think relates to whether they can see their final location; somatic education
methods like Alexander Technique claim that vision is important in guiding
movement. This leads me to a slightly different hypothesis:

> Given two paths of equal distance to a goal, a person is more likely to
> choose the path where the goal is visible sooner.

In the case of the rectangle, this would mean that people would start
with the short leg if they can't see the target, as getting to the long
leg would help them see the target. It might also mean that they would
start with the long leg if they can see the target because starting with
the short leg would make them turn further away from the target at first,
making it harder to see.

Depending on how rectangular paths are designed, this potential relationship
could be confounded with effects of handedness; theories like right-hand
preference suggest that, other things being equal, people might be more likely
to start with the right side (more specifically, the side of their handedness).

## Methods

Participants were made to chose how to traverse a rectangular table in order
to retrieve beers from the opposite corner. The table arrangement and beer
location were varied, and route of travel was recorded.

### Room setup

A long table was placed in the middle of the room. Two beers were placed at one
corner, and an "X" was drawn on the floor at the opposite corner.

### Participants

Participants were recruited at a [frat party, picnic, &c.] in [place]; they were
offered a beer in return for participation in a one-minute experiment.
[Demographics based on study setting--general age, location, &c.].

### Procedure

Participants were asked to stand at the X and were given the following directions.

1. Do not start walking until I say "start".
2. There are two beers at the opposite corner of the table.
3. When I tell you, walk to the opposite corner of the table, and pick up the beers.
4. After you pick them up, bring them back here, and put either one on this "X".
5. You may keep the other beer.

After giving these directions, the experimentor asked "Do these directions make
sense?" and clarified any resulting misunderstandings. Once the directions were
clear, the experimentor told the participants, "Please start."

As the participant traversed the table, the experimentor drew two arrows on a
form (figure [form]) to indicate whether the participant turned left or right
at the start of each of the two legs of the trip (from the "X" to the beers
and from the beers to the "X").

[]
Figure [form]

After the participant returned the beer, the experimentor asked for the
participants handedness and then debriefed the participant.

### Variables

Before the participant entered the experiment room, it was configured into
one of four ways based on two two-level factors:

1. target location and
2. short leg.

At the end of the experiment, handedness was collected.

#### Target location
Target location was one of "floor" or "table". Beers were placed on the floor
in the floor condition and on the table in the table condition
(figure [target_locations]).

[]
Figure [target-locations]

They also had a specific placement in the horizontal axes. In both conditions,
the beers were placed within [four inches] of the corner point. In the table
condition, the beers were placed approximately along the diagonal of the
rectangular table that included the corner point. In the floor condition, the
beers were placed [four inches] from the corner in the opposite direction,
so they were not underneath the table.

Given the table dimensions and the beer placement, participants should have
been able to see the beers in the floor condition when they were at either
of the adjacent corners but not when they were at the opposite corner. In the
table condition, participants should have been able to see the beers from any
corner.

#### Short leg

A rectangle has four different vectors that connect opposite corners (figure
[vectors]). These vectors correspond to the routes that we could have asked
participants to traverse.

Participants were expected to walk around the table rather than over or under
the table. (We ignored any participants who walked through the table.)
In these expected paths, a participant would first turn one direction (left
or right), then traverse a leg of the table, then turn the other direction
and traverse the remaining leg. In order to control for affects of handedness,
we classified these four routes into two route types.

1. Short leg left, long leg right: If the participant turns left first,
    he traverses the short leg first.
2. Short leg right, long leg left: If the participant turns left first,
    he traverses the long leg first.

These are also diagramed in figure [path types].

[]
Figure [path types]

I randomly chose one leg from each of these two route types to use for the entire
study. Only one of these route types was used for each particular participant,
and the route type was randomly chosen for each trial. The beer and "X" were
placed according to this route type definition.

#### Handedness
Participants were asked for their handedness, which was recorded as "left",
"right" or "other". Data from participants with "other" handedness were
ignored.

### Route choice

Direction of each leg of travel was recorded on a form (section [whatever]) and
then translated into a "left", "right" or "other". If participants took routes
that did not fit neatly into "left" or "right", such as jumping over the table
or back-tracking, the direction was marked as "other", and the trial was ignored.

### Model

Data were transformed into four binomial variables:

1. Target location (floor or table)
2. First side of first leg of travel (left, right other)
3. First side of second leg of travel (short, long or other)
4. Short leg matches handedness (yes, no or other)

The first of these variables should be straightforward given the earlier
definition. The second variable corresponds to the route taken to retrieve
the beer. The third variable corresponds to the route taken to return the beer
to the "X".

The fourth variable relates two variables that were collected. One of these
variables is handedness. The other is the "short leg" variable discussed earlier.
That is, it's whether the table was arranged with the short leg to the left or
the right of the "X" from the point of view of the participant. When handedness
was "other", this variable was also "other".

If any of the variables was "other", the trial was ignored.

These four variables were modeled with bivariate analysis of variance with two
main effects and their interaction. Since all values of "other" were ignored,
all of the variables had exactly two levels.

## Results

[Some number of] people participated in the study. Here are some summary
statistics.

Regarding our interventions,

* [] trials had the short leg left and the target location floor
* [] trials had the short leg left and the target location table
* [] trials had the short leg right and the target location floor
* [] trials had the short leg right and the target location table

```
    Short leg   Target location | Number of trials
    ---------   --------------- | ----------------
    Left        Table           | 
    Left        Floor           | 
    Right       Table           | 
    Right       Floor           | 
```

Regarding handedness, [] people were left-handed, [] were right-handed and
[] were "other".

[]% of legs involved left turns first, and []% involved right turns first.
[]% of legs involved short sides first, and []% involved long sides first.

[]% of legs were something else, including [potential hilarity].

The aforementioned model had [reasonable fit statistics] and yielded [these
coefficients] (table [foo] and figure [bar]).

[Table with coefficients]

[Plot with observations and fitted values from the model]

When the target was on the table and the short leg did not match handedness,
people were [more likely to take the long leg?].
When the target was on the table and the short leg did match handedness,
people were [more likely to take the short leg?].
When the target was on the floor, [people were more likely to take the short
leg regardless of whether it matched handedness].

### Discussion

[It appears that both handedness and position of beer affect the choice of
route around the rectangular table, with the latter having a stronger effect.]

This relates to the relevance of vision in human movement.

More immediately, the study results can be used in the design of paths.
If people prefer to see the endpoint of their travel, landmarks can be
positioned or obscured in order to direct traffic to whichever path the
designer intends.

## Conclusion

[These results support the hypothesis?]

## Future research

* Alexander Technique
* Applications in parks, streets, &c.


redirect.thomaslevine.com
------------------------------------------------------------------------------------------------
**Title**: redirect.thomaslevine.com

**Web address**: https://api.github.com/repos/tlevine/redirect.thomaslevine.com

**Date**: November 2012

**Description**: Web redirects
=========================
## What do I mean by "web redirects"?
They're also called web forwards, HTTP redirects and URL redirects. Here's the
most common sort of HTTP redirect: I own
[thomaslevine.com](http://thomaslevine.com),
I host a website at
[www.thomaslevine.com](http://www.thomaslevine.com),
and I want
[thomaslevine.com](http://thomaslevine.com) and
[www.thomaslevine.com](http://www.thomaslevine.com)
to go to the same place, so I make
[thomaslevine.com](http://thomaslevine.com)
redirect to
[www.thomaslevine.com](http://www.thomaslevine.com).

Wikipedia has [further thoughts](http://en.wikipedia.org/wiki/URL_redirection).
More technically, this currently includes HTTP responses with codes of 301
and 303, but it could include more. The relevant part is that these are tiny
things that need a reliable server but don't need a big server.

## Why
I started needing these when I started hosting things on Amazon S3. Amason S3
requires you to point CNAME records to Amazon, so you can't use bare domains.
So I wanted to redirect the domains. I could do this with a proper server, but
I was sure some other service already did this. But I couldn't find any service
I liked. Problems with existing services:

* The good ones are [DNS](http://freedns.afraid.org/) [hosting](http://dyn.com)
    that costs like $50 per year. Redirects don't need to be so expensive.
* I use Gandi's web forwards for the domains I have registered on Gandi, but
    not all registrars provide HTTP redirects.
* Gandi is too complicated. HTTP redirects are stupidly simple to specify;
    I should be able to specify one with a form with one or two fields.
* Setting up my own server took half a day. It shouldn't take this long.

Given all these problems, I figured I'd make a service myself. For now, it's
free. If shit tons of people start using it and I need to get another server
or a bigger server, I'll need figure out how to make it sustainable, but it
would still certainly be cheap.

## How to use: Set up DNS
First, you need to add a record to your zone file with your DNS provider. If
you don't know what that means, it's probably the the site that you use to
manage all of your web hosting; a lot of hosts package domain registration,
domain name service and web hosting.

Let's say you want to redirect from [thomaslevine.com](http://thomaslevine.com) to [www.thomaslevine.com](http://www.thomaslevine.com).
In this case, you need to add an A record that points "[thomaslevine.com](http://thomaslevine.com)" to
"108.174.51.19". Depending on your provider, you might have to specify it in
one of a few days. Typically, the zone file editor assumes that you are
managing subdomains, so you might need to enter "" (nothing),
"[thomaslevine.com](http://thomaslevine.com)", or "@" in whatever form you're using. If you're editing the
zone file directly, this line should work.

    @ 10800 IN A 108.174.51.19

You'll have to wait some time, between a few minutes and a few days, for the
change to be propogated to the world's DNS servers. Once that happens, you can
go to the domain that you are redirecting from ([thomaslevine.com](http://thomaslevine.com) in this
example) and set up the redirection. Alternatively, you can go to
[redirect.thomaslevine.com](http://redirect.thomaslevine.com) before then and
set it up, but it won't work until the change gets propogated through the
world's caches. Either way, follow

## How to use: Set up the redirect
You can set up the redirect with a web page that works like you expect. If
you want something fancier or more automatic, use the the API. The form calls
the API, in case you're curious.

### Via the GUI
Go to [redirect.thomaslevine.com](http://redirect.thomaslevine.com), and fill
out the form. It contains two fields

1. **from** is the site that people type in to their URL bar; it's the thing
    you added an A record to the zone file (see above) for.
2. **to** is the place where they should land; it's where you are already
    hosting the site.

You will land at another page. **Save that page's URL**; The identifier at the
end is like a password, so you'll need it if you ever want to change the
redirect.

### Via the API
Choose an identifier for your redirect. This acts as a password, so you should
choose something that other people won't guess or choose accidentally. Bad
identifiers include "website" and "thomaslevine.com". Good ones include
"UDSkS9XH8w07Fq98Jue3aU" and "thomaslevine.com-dear badly pain blanket". You
can find more good identifiers
[here](http://preshing.com/20110811/xkcd-password-generator).
Now that you've chosen an identifier, this is the URL that should concern you

    http://redirect.thomaslevine.com/v1/<identifier>

You can **create**, **edit**, **read** and **delete** the redirect by making
HTTP requests to this URL. Examples follow.

#### Create
Create a redirect from
"[thomaslevine.com](http://thomaslevine.com)" to
"[www.thomaslevine.com](http://www.thomaslevine.com)"
like so.

    curl -X PUT\ 
    --data from=<from address, like "thomaslevine.com"> \ 
    --data to=<to address, like "www.thomaslevine.com"> \
    http://redirect.thomaslevine.com/v1/sho+ue8ohn,.n237fun

You may also specify an HTTP status code and email address for the redirect.

    curl -X PUT\ 
    --data from="thomaslevine.com" \ 
    --data to="www.thomaslevine.com" \
    --data status_code=301 \
    --data email=occurrence@example.com \
    http://redirect.thomaslevine.com/v1/sho+ue8ohn,.n237fun

Regardless of whether you provide an **email address**, I might contact you to
figure out how I can make this better. If you provide an email address, this
will be easier. Also, I'll be more able to inform you of outages and whatnot.

The default is **status code** is 303, which functions as a temporary redirect.
If 303 is specified, the redirect server will be called every time a browser
tries to go to your website. You may also specify 301, which is a permanent
redirect. In this cose, the redirect will be cached within the browser, so
the server will only be called once on each computer; this will make your site
ever-so-slightly faster, reduce my load ever-so-slightly, and make your site
ever-so-slightly more robust in case of outages of my server. It would be
reasonable to use 303 for testing and 301 once it seems to work. And tell me
if you want options for other status codes.

#### Edit
You can edit a redirect by making a PUT request just like you created it.

    # This would remove the email address if one was already listed.
    curl -X PUT \ 
    --data from="thomaslevine.com" \ 
    --data to="www.thomaslevine.com" \
    --data status_code=303 \
    http://redirect.thomaslevine.com/v1/sho+ue8ohn,.n237fun

If you would like to edit just one field, you can also use a POST instead.

    # This would not remove the email address if one was already listed.
    curl -X POST \ 
    --data from="thomaslevine.com" \ 
    --data to="www.thomaslevine.com" \
    --data status_code=303 \
    http://redirect.thomaslevine.com/v1/sho+ue8ohn,.n237fun

    # This is valid.
    curl -X POST \ 
    --data status_code=303 \
    http://redirect.thomaslevine.com/v1/sho+ue8ohn,.n237fun

    # This is not valid.
    curl -X PUT \ 
    --data status_code=303 \
    http://redirect.thomaslevine.com/v1/sho+ue8ohn,.n237fun

#### Read
Run something like this to read the redirect configuration of the redirect.

    curl http://redirect.thomaslevine.com/v1/sho+ue8ohn,.n237fun

You'll get something like this.

    { "from": "thomaslevine.com",
      "to": "www.thomaslevine.com",
      "email": "occurrence@example.com",
      "status_code": 303
    }

If you request a url that does not correspond to a redirect, you'll get
something like this

    { "error": "That redirect doesn't exist. But feel free to create it."
    }

**Delete** the redirect configuration like so.

    curl -X DELETE http://redirect.thomaslevine.com/v1/sho+ue8ohn,.n237fun

## Architecture
Let's divide it into two parts.

1. ReST API for editing nginx configuration files
2. Nginx

The ReST API (documented above) only reads and edits configuration files; it
doesn't serve redirects. If you **delete** the ReST API and **run** Nginx,
you will **not** be able to edit redirects, and the redirects **will** be served.
If you turn **on** the ReST API and **uninstall** Nginx, your API queries
**will** read and edit the nginx configuration files, and the redirects will
**not** be served.

This is very convenient; because Nginx handles all of the heavy traffic, I
don't need to write an application that can handle substantial load. Also,
if I notice a problem with the API, I can just disable it while I fix it.
(This aspect hasn't been relevant because my code is so awesome.)

### Front-end
Aside from sites that the ReST API creates, Nginx serves the front-end clicky
AJAX form for creating sites. The only interesting part about that is the
selection of identifiers. First, this is what I mean by the "identifier".

    http://redirect.thomaslevine.com/v1/:identifier

The API requires that you select the identifier. It is thus possible for them
to clash, so I just used them as passwords by suggesting that they be made
really long. The front-end does this for you without telling you; it generates
a UUID for the identifier. You can see it by clicking "Show advanced options."

### Scaling
It should be easy to adjust the nginx configuration to handle high load, but
problems could arise if lots of people are editing lots of redirects with the
ReST API. There isn't any particular thing that worries me much, but here are
some ideas of how to approach that.

1. If lots of people are accessing your redirects (not the API), just run
    multiple Nginx boxes with one API box; you can send the configuration file
    directory from the API box to the Nginx boxes every so often.
2. If you want to run multiple API boxes, consider why you need them, as this
    situation would be a bit strange. Naively, I could suggest storing the
    configuration files in GlusterFS and accessing this from all of the Nginx
    and API boxes. But you might be able to make some convenient assumptions
    in such a sitiation that would allow you to do something simpler.
3. Depending on what filesystem you're using, having lots of files could be
    problematic. I don't know whether it would noticeably impact Nginx. It
    shouldn't be a problem for the API because it always looks for one
    particular file. If you are using a stupid filesystem, you might hit
    the limit on the number of files in a directory.
4. If you are going to access the same API endpoint from many different
    computers at similar times, you might want to think about whether there
    are relevant race conditions in this API code. I don't think there are
    any, but I haven't used this API like that.

## Hosting
This runs on a tiny server from ChicagoVPS. The API is a uWSGI application that
runs inside of a tmux because I didn't feel like making a proper ademon. It
edits nginx configuration files; each redirect is a server entry.

### Installing the server
If we're lucky, the entire install process can be run without interactively
logging in to the redirect server; all of the commands below are supposed to be
run from any old computer, rather than the redirect server.

First, make an A record pointing to the server and install Debian on the server.
Then configure the ssh keys. 

    ssh-copy-id root@redirect.thomaslevine.com

Now build the dependencies and copy the files. There's a script for this.

    . activate
    build_redirect_server redirect.thomaslevine.com

All done. Test it out.

    curl http://redirect.thomaslevine.com
    curl -X PUT \ 
    --data from="thomaslevine.com" \ 
    --data to="www.thomaslevine.com" \
    --data status_code=303 \
    http://redirect.thomaslevine.com/v1/sho+ue8ohn,.n237fun

### Running the backups
Run `./bin/backup_redirect_server` to take backups. Set up a cron job to do
this daily or so.

### Integration tests
Run `./bin/runlocal` to run a server on [localhost:9002](http://localhost:9002).
Then you can run `nosetests2 integrationtests.py` from `./application` to run
integration tests locally.

Run `./bin/integration_tests` to run integration tests on the live site. You
could set up a cron job to do this daily or so.

## Wishlist
Here are some things I want.

* I want different views in the front-end should have own URLs
    (fragment identifiers) so that the back button can work and so that I can
    link to places.
* I want API should run in a real daemon, not a tmux, just because.
* I want the configuration files should be prefixed with something later than
    "1-", like maybe "50-", so that I can manually add other sites with higher
    priority.
* I want a system for backing up the nginx configuration with all of the
    redirects to another server so that I can restore the redirects in case of
    problems. A git repository in `/etc/nginx/conf.d/` should be fine for this.
* I sort of want the Javascript on the front end to be less hacky and ugly, but
    I haven't wanted to edit this at all, and it's been working just fine, so
    maybe it's not that important.


regulation-matrix
------------------------------------------------------------------------------------------------
**Title**: regulation-matrix

**Web address**: https://api.github.com/repos/tlevine/regulation-matrix

**Date**: December 2012

**Description**: 

reicalg
------------------------------------------------------------------------------------------------
**Title**: reicalg

**Web address**: https://api.github.com/repos/tlevine/reicalg

**Date**: August 2012

**Description**: 

reveal.js
------------------------------------------------------------------------------------------------
**Title**: reveal.js

**Web address**: https://api.github.com/repos/tlevine/reveal.js

**Date**: September 2013

**Description**: # reveal.js [![Build Status](https://travis-ci.org/hakimel/reveal.js.png?branch=master)](https://travis-ci.org/hakimel/reveal.js)

A framework for easily creating beautiful presentations using HTML. [Check out the live demo](http://lab.hakim.se/reveal-js/).

reveal.js comes with a broad range of features including [nested slides](https://github.com/hakimel/reveal.js#markup), [markdown contents](https://github.com/hakimel/reveal.js#markdown), [PDF export](https://github.com/hakimel/reveal.js#pdf-export), [speaker notes](https://github.com/hakimel/reveal.js#speaker-notes) and a [JavaScript API](https://github.com/hakimel/reveal.js#api). It's best viewed in a browser with support for CSS 3D transforms but [fallbacks](https://github.com/hakimel/reveal.js/wiki/Browser-Support) are available to make sure your presentation can still be viewed elsewhere.


#### More reading in the Wiki:
- [Changelog](https://github.com/hakimel/reveal.js/wiki/Changelog): Up-to-date version history.
- [Examples](https://github.com/hakimel/reveal.js/wiki/Example-Presentations): Presentations created with reveal.js, add your own!
- [Browser Support](https://github.com/hakimel/reveal.js/wiki/Browser-Support): Explanation of browser support and fallbacks.

## rvl.io

Slides are written using HTML or markdown but there's also an online editor for those of you who prefer a more traditional user interface. Give it a try at [www.rvl.io](http://www.rvl.io).


## Instructions

### Markup

Markup heirarchy needs to be ``<div class="reveal"> <div class="slides"> <section>`` where the ``<section>`` represents one slide and can be repeated indefinitely. If you place multiple ``<section>``'s inside of another ``<section>`` they will be shown as vertical slides. The first of the vertical slides is the "root" of the others (at the top), and it will be included in the horizontal sequence. For example:

```html
<div class="reveal">
	<div class="slides">
		<section>Single Horizontal Slide</section>
		<section>
			<section>Vertical Slide 1</section>
			<section>Vertical Slide 2</section>
		</section>
	</div>
</div>
```

### Markdown

It's possible to write your slides using Markdown. To enable Markdown, add the ```data-markdown``` attribute to your ```<section>``` elements and wrap the contents in a ```<script type="text/template">``` like the example below.

This is based on [data-markdown](https://gist.github.com/1343518) from [Paul Irish](https://github.com/paulirish) which in turn uses [showdown](https://github.com/coreyti/showdown/). Sensitive to indentation (avoid mixing tabs and spaces) and line breaks (avoid consecutive breaks).

```html
<section data-markdown>
	<script type="text/template">
		## Page title

		A paragraph with some text and a [link](http://hakim.se).
	</script>
</section>
```

#### External Markdown

You can write your content as a separate file and have reveal.js load it at runtime. Note the separator arguments which determine how slides are delimited in the external file.

```html
<section data-markdown="example.md" data-separator="^\n\n\n" data-vertical="^\n\n"></section>
```

### Configuration

At the end of your page you need to initialize reveal by running the following code. Note that all config values are optional and will default as specified below.

```javascript
Reveal.initialize({

	// Display controls in the bottom right corner
	controls: true,

	// Display a presentation progress bar
	progress: true,

	// Push each slide change to the browser history
	history: false,

	// Enable keyboard shortcuts for navigation
	keyboard: true,

	// Enable the slide overview mode
	overview: true,

	// Vertical centering of slides
	center: true,

	// Loop the presentation
	loop: false,

	// Change the presentation direction to be RTL
	rtl: false,

	// Number of milliseconds between automatically proceeding to the 
	// next slide, disabled when set to 0, this value can be overwritten
	// by using a data-autoslide attribute on your slides
	autoSlide: 0,

	// Enable slide navigation via mouse wheel
	mouseWheel: false,

	// Apply a 3D roll to links on hover
	rollingLinks: true,

	// Transition style
	transition: 'default' // default/cube/page/concave/zoom/linear/fade/none

});
```

Note that the new default vertical centering option will break compatibility with slides that were using transitions with backgrounds (`cube` and `page`). To restore the previous behavior, set `center` to `false`.


### Presentation Size

All presentations have a normal size, that is the resolution at which they are authored. The framework will automatically scale presentations uniformly based on this size to ensure that everything fits on any given display or viewport. 

See below for a list of configuration options related to sizing, including default values:

```javascript
Reveal.initialize({
	
	...
	
	// The "normal" size of the presentation, aspect ratio will be preserved
	// when the presentation is scaled to fit different resolutions. Can be
	// specified using percentage units.
	width: 960,
	height: 700,
	
	// Factor of the display size that should remain empty around the content
	margin: 0.1,
	
	// Bounds for smallest/largest possible scale to apply to content
	minScale: 0.2,
	maxScale: 1.0
	
});
```


### Dependencies

Reveal.js doesn't _rely_ on any third party scripts to work but a few optional libraries are included by default. These libraries are loaded as dependencies in the order they appear, for example:

```javascript
Reveal.initialize({
	dependencies: [
		// Cross-browser shim that fully implements classList - https://github.com/eligrey/classList.js/
		{ src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },

		// Interpret Markdown in <section> elements
		{ src: 'plugin/markdown/showdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
		{ src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },

		// Syntax highlight for <code> elements
		{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },

		// Zoom in and out with Alt+click
		{ src: 'plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },

		// Speaker notes
		{ src: 'plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } },

		// Remote control your reveal.js presentation using a touch device
		{ src: 'plugin/remotes/remotes.js', async: true, condition: function() { return !!document.body.classList; } }
	]
});
```

You can add your own extensions using the same syntax. The following properties are available for each dependency object:
- **src**: Path to the script to load
- **async**: [optional] Flags if the script should load after reveal.js has started, defaults to false
- **callback**: [optional] Function to execute when the script has loaded
- **condition**: [optional] Function which must return true for the script to be loaded


### API

The Reveal class provides a minimal JavaScript API for controlling navigation and reading state:

```javascript
// Navigation
Reveal.slide( indexh, indexv, indexf );
Reveal.left();
Reveal.right();
Reveal.up();
Reveal.down();
Reveal.prev();
Reveal.next();
Reveal.prevFragment();
Reveal.nextFragment();
Reveal.toggleOverview();

// Retrieves the previous and current slide elements
Reveal.getPreviousSlide();
Reveal.getCurrentSlide();

Reveal.getIndices(); // { h: 0, v: 0 } }
```

### States

If you set ``data-state="somestate"`` on a slide ``<section>``, "somestate" will be applied as a class on the document element when that slide is opened. This allows you to apply broad style changes to the page based on the active slide.

Furthermore you can also listen to these changes in state via JavaScript:

```javascript
Reveal.addEventListener( 'somestate', function() {
	// TODO: Sprinkle magic
}, false );
```

### Ready event

The 'ready' event is fired when reveal.js has loaded all (synchronous) dependencies and is ready to start navigating.

```javascript
Reveal.addEventListener( 'ready', function( event ) {
	// event.currentSlide, event.indexh, event.indexv
} );
```

### Slide change event

An 'slidechanged' event is fired each time the slide is changed (regardless of state). The event object holds the index values of the current slide as well as a reference to the previous and current slide HTML nodes.

Some libraries, like MathJax (see [#226](https://github.com/hakimel/reveal.js/issues/226#issuecomment-10261609)), get confused by the transforms and display states of slides. Often times, this can be fixed by calling their update or render function from this callback.

```javascript
Reveal.addEventListener( 'slidechanged', function( event ) {
	// event.previousSlide, event.currentSlide, event.indexh, event.indexv
} );
```

### Internal links

It's easy to link between slides. The first example below targets the index of another slide whereas the second targets a slide with an ID attribute (```<section id="some-slide">```):

```html
<a href="#/2/2">Link</a>
<a href="#/some-slide">Link</a>
```

You can also add relative navigation links, similar to the built in reveal.js controls, by appending one of the following classes on any element. Note that each element is automatically given an ```enabled``` class when it's a valid navigation route based on the current slide.

```html
<a href="#" class="navigate-left">
<a href="#" class="navigate-right">
<a href="#" class="navigate-up">
<a href="#" class="navigate-down">
<a href="#" class="navigate-prev"> <!-- Previous vertical or horizontal slide -->
<a href="#" class="navigate-next"> <!-- Next vertical or horizontal slide -->
```


### Fragments
Fragments are used to highlight individual elements on a slide. Every elmement with the class ```fragment``` will be stepped through before moving on to the next slide. Here's an example: http://lab.hakim.se/reveal-js/#/16

The default fragment style is to start out invisible and fade in. This style can be changed by appending a different class to the fragment:

```html
<section>
	<p class="fragment grow">grow</p>
	<p class="fragment shrink">shrink</p>
	<p class="fragment roll-in">roll-in</p>
	<p class="fragment fade-out">fade-out</p>
	<p class="fragment highlight-red">highlight-red</p>
	<p class="fragment highlight-green">highlight-green</p>
	<p class="fragment highlight-blue">highlight-blue</p>
</section>
```

Multiple fragments can be applied to the same element sequentially by wrapping it, this will fade in the text on the first step and fade it back out on the second.

```html
<section>
	<span class="fragment fade-in">
		<span class="fragment fade-out">I'll fade in, then out</span>
	</span>
</section>
```

The display order of fragments can be controlled using the ```data-fragment-index``` attribute.

```html
<section>
	<p class="fragment" data-fragment-index="3">Appears last</p>
	<p class="fragment" data-fragment-index="1">Appears first</p>
	<p class="fragment" data-fragment-index="2">Appears second</p>
</section>
```

### Fragment events

When a slide fragment is either shown or hidden reveal.js will dispatch an event.

```javascript
Reveal.addEventListener( 'fragmentshown', function( event ) {
	// event.fragment = the fragment DOM element
} );
Reveal.addEventListener( 'fragmenthidden', function( event ) {
	// event.fragment = the fragment DOM element
} );
```

### Code syntax higlighting

By default, Reveal is configured with [highlight.js](http://softwaremaniacs.org/soft/highlight/en/) for code syntax highlighting. Below is an example with clojure code that will be syntax highlighted:

```html
<section>
	<pre><code>
(def lazy-fib
  (concat
   [0 1]
   ((fn rfib [a b]
        (lazy-cons (+ a b) (rfib b (+ a b)))) 0 1)))
	</code></pre>
</section>
```


### Overview mode

Press "Esc" key to toggle the overview mode on and off. While you're in this mode, you can still navigate between slides,
as if you were at 1,000 feet above your presentation. The overview mode comes with a few API hooks:

```javascript
Reveal.addEventListener( 'overviewshown', function( event ) { /* ... */ } );
Reveal.addEventListener( 'overviewhidden', function( event ) { /* ... */ } );

// Toggle the overview mode programmatically
Reveal.toggleOverview();
```

### Fullscreen mode
Just press A>>FA<< on your keyboard to show your presentation in fullscreen mode. Press the A>>ESCA<< key to exit fullscreen mode.


## PDF Export

Presentations can be exported to PDF via a special print stylesheet. This feature requires that you use [Google Chrome](http://google.com/chrome). 
Here's an example of an exported presentation that's been uploaded to SlideShare: http://www.slideshare.net/hakimel/revealjs-13872948.

1. Open your presentation with [css/print/pdf.css](https://github.com/hakimel/reveal.js/blob/master/css/print/pdf.css) included on the page. The default index HTML lets you add *print-pdf* anywhere in the query to include the stylesheet, for example: [lab.hakim.se/reveal-js?print-pdf](http://lab.hakim.se/reveal-js?print-pdf).
2. Open the in-browser print dialog (CMD+P).
3. Change the **Destination** setting to **Save as PDF**.
4. Change the **Layout** to **Landscape**.
5. Change the **Margins** to **None**.
6. Click **Save**.

![Chrome Print Settings](https://s3.amazonaws.com/hakim-static/reveal-js/pdf-print-settings.png)


## Speaker Notes

reveal.js comes with a speaker notes plugin which can be used to present per-slide notes in a separate browser window. The notes window also gives you a preview of the next upcoming slide so it may be helpful even if you haven't written any notes. Append ```?notes``` to presentation URL or press the 's' key on your keyboard to open the notes window.

By default notes are written using standard HTML, see below, but you can add a ```data-markdown``` attribute to the ```<aside>``` to write them using Markdown.

```html
<section>
	<h2>Some Slide</h2>

	<aside class="notes">
		Oh hey, these are some notes. They'll be hidden in your presentation, but you can see them if you open the speaker notes window (hit 's' on your keyboard).
	</aside>
</section>
```

## Server Side Speaker Nodes

In some cases it can be desirable to run notes on a separate device from the one you're presenting on. The Node.js-based notes plugin lets you do this using the same note definitions as its client side counterpart. Include the requried scripts by adding the following dependencies:

```javascript
{ src: '/socket.io/socket.io.js', async: true },
{ src: 'plugin/notes-server/client.js', async: true }
```

Then:

1. Install [Node.js](http://nodejs.org/)
2. Run ```npm install```
3. Run ```node plugin/notes-server```


## Theming

The framework comes with a few different themes included:

- default: Gray background, white text, blue links
- beige: Beige background, dark text, brown links
- sky: Blue background, thin white text, blue links
- night: Black background, thick white text, orange links
- serif: Cappuccino background, gray text, brown links
- simple: White background, black text, blue links

Each theme is available as a separate stylesheet. To change theme you will need to replace **default** below with your desired theme name in index.html:

```html
<link rel="stylesheet" href="css/theme/default.css" id="theme">
```

If you want to add a theme of your own see the instructions here: [/css/theme/README.md](https://github.com/hakimel/reveal.js/blob/master/css/theme/README.md).


## Development Environment

reveal.js is built using the task-based command line build tool [grunt.js](http://gruntjs.com) ([installation instructions](https://github.com/gruntjs/grunt#installing-grunt)). With Node.js and grunt.js installed, you need to start by running ```npm install``` in the reveal.js root. When the dependencies have been installed you should run ```grunt watch``` to start monitoring files for changes.

If you want to customise reveal.js without running grunt.js you can alter the HTML to point to the uncompressed source files (css/reveal.css & js/reveal.js).

### Folder Structure
- **css/** Core styles without which the project does not function
- **js/** Like above but for JavaScript
- **plugin/** Components that have been developed as extensions to reveal.js
- **lib/** All other third party assets (JavaScript, CSS, fonts)


## License

MIT licensed

Copyright (C) 2013 Hakim El Hattab, http://hakim.se



rifidec
------------------------------------------------------------------------------------------------
**Title**: rifidec

**Web address**: https://api.github.com/repos/tlevine/rifidec

**Date**: October 2011

**Description**: 

risley-fb-post
------------------------------------------------------------------------------------------------
**Title**: risley-fb-post

**Web address**: https://api.github.com/repos/tlevine/risley-fb-post

**Date**: January 2013

**Description**: 

s3cmd
------------------------------------------------------------------------------------------------
**Title**: s3cmd

**Web address**: https://api.github.com/repos/tlevine/s3cmd

**Date**: April 2013

**Description**: S3cmd tool for Amazon Simple Storage Service (S3)
=================================================

Author:
    Michal Ludvig <michal@logix.cz>

S3tools / S3cmd project homepage:
    http://s3tools.org

S3tools / S3cmd mailing lists:
    * Announcements of new releases:
        s3tools-announce@lists.sourceforge.net

    * General questions and discussion about usage
        s3tools-general@lists.sourceforge.net

    * Bug reports
        s3tools-bugs@lists.sourceforge.net

Amazon S3 homepage:
    http://aws.amazon.com/s3

!!!
!!! Please consult INSTALL file for installation instructions!
!!!

What is Amazon S3
-----------------
Amazon S3 provides a managed internet-accessible storage 
service where anyone can store any amount of data and 
retrieve it later again. Maximum amount of data in one 
"object" is 5GB, maximum number of objects is not limited.

S3 is a paid service operated by the well known Amazon.com
internet book shop. Before storing anything into S3 you
must sign up for an "AWS" account (where AWS = Amazon Web 
Services) to obtain a pair of identifiers: Access Key and
Secret Key. You will need to give these keys to S3cmd. 
Think of them as if they were a username and password for
your S3 account.

Pricing explained
-----------------
At the time of this writing the costs of using S3 are (in USD):

$0.15 per GB per month of storage space used

plus

$0.10 per GB - all data uploaded

plus

$0.18 per GB - first 10 TB / month data downloaded
$0.16 per GB - next 40 TB / month data downloaded
$0.13 per GB - data downloaded / month over 50 TB

plus

$0.01 per 1,000 PUT or LIST requests
$0.01 per 10,000 GET and all other requests

If for instance on 1st of January you upload 2GB of 
photos in JPEG from your holiday in New Zealand, at the 
end of January you will be charged $0.30 for using 2GB of
storage space for a month, $0.20 for uploading 2GB
of data, and a few cents for requests. 
That comes to slightly over $0.50 for a complete backup 
of your precious holiday pictures.

In February you don't touch it. Your data are still on S3 
servers so you pay $0.30 for those two gigabytes, but not
a single cent will be charged for any transfer. That comes 
to $0.30 as an ongoing cost of your backup. Not too bad.

In March you allow anonymous read access to some of your
pictures and your friends download, say, 500MB of them. 
As the files are owned by you, you are responsible for the 
costs incurred. That means at the end of March you'll be 
charged $0.30 for storage plus $0.09 for the download traffic 
generated by your friends.

There is no minimum monthly contract or a setup fee. What 
you use is what you pay for. At the beginning my bill used
to be like US$0.03 or even nil.

That's the pricing model of Amazon S3 in a nutshell. Check
Amazon S3 homepage at http://aws.amazon.com/s3 for more 
details.

Needless to say that all these money are charged by Amazon 
itself, there is obviously no payment for using S3cmd :-)

Amazon S3 basics
----------------
Files stored in S3 are called "objects" and their names are
officially called "keys". Since this is sometimes confusing
for the users we often refer to the objects as "files" or
"remote files". Each object belongs to exactly one "bucket".

To describe objects in S3 storage we invented a URI-like
schema in the following form:

    s3://BUCKET
or
    s3://BUCKET/OBJECT

Buckets
-------
Buckets are sort of like directories or folders with some 
restrictions:
1) each user can only have 100 buckets at the most, 
2) bucket names must be unique amongst all users of S3, 
3) buckets can not be nested into a deeper hierarchy and 
4) a name of a bucket can only consist of basic alphanumeric 
   characters plus dot (.) and dash (-). No spaces, no accented
   or UTF-8 letters, etc. 

It is a good idea to use DNS-compatible bucket names. That
for instance means you should not use upper case characters.
While DNS compliance is not strictly required some features
described below are not available for DNS-incompatible named
buckets. One more step further is using a fully qualified
domain name (FQDN) for a bucket - that has even more benefits.

* For example "s3://--My-Bucket--" is not DNS compatible.
* On the other hand "s3://my-bucket" is DNS compatible but 
  is not FQDN.
* Finally "s3://my-bucket.s3tools.org" is DNS compatible 
  and FQDN provided you own the s3tools.org domain and can
  create the domain record for "my-bucket.s3tools.org".

Look for "Virtual Hosts" later in this text for more details 
regarding FQDN named buckets.

Objects (files stored in Amazon S3)
-----------------------------------
Unlike for buckets there are almost no restrictions on object 
names. These can be any UTF-8 strings of up to 1024 bytes long. 
Interestingly enough the object name can contain forward
slash character (/) thus a "my/funny/picture.jpg" is a valid
object name. Note that there are not directories nor
buckets called "my" and "funny" - it is really a single object 
name called "my/funny/picture.jpg" and S3 does not care at 
all that it _looks_ like a directory structure.

The full URI of such an image could be, for example:

    s3://my-bucket/my/funny/picture.jpg

Public vs Private files
-----------------------
The files stored in S3 can be either Private or Public. The 
Private ones are readable only by the user who uploaded them
while the Public ones can be read by anyone. Additionally the
Public files can be accessed using HTTP protocol, not only
using s3cmd or a similar tool.

The ACL (Access Control List) of a file can be set at the 
time of upload using --acl-public or --acl-private options 
with 's3cmd put' or 's3cmd sync' commands (see below).

Alternatively the ACL can be altered for existing remote files
with 's3cmd setacl --acl-public' (or --acl-private) command.

Simple s3cmd HowTo
------------------
1) Register for Amazon AWS / S3
   Go to http://aws.amazon.com/s3, click the "Sign up
   for web service" button in the right column and work 
   through the registration. You will have to supply 
   your Credit Card details in order to allow Amazon 
   charge you for S3 usage. 
   At the end you should have your Access and Secret Keys

2) Run "s3cmd --configure"
   You will be asked for the two keys - copy and paste 
   them from your confirmation email or from your Amazon 
   account page. Be careful when copying them! They are 
   case sensitive and must be entered accurately or you'll 
   keep getting errors about invalid signatures or similar.

3) Run "s3cmd ls" to list all your buckets.
   As you just started using S3 there are no buckets owned by 
   you as of now. So the output will be empty.

4) Make a bucket with "s3cmd mb s3://my-new-bucket-name"
   As mentioned above the bucket names must be unique amongst 
   _all_ users of S3. That means the simple names like "test" 
   or "asdf" are already taken and you must make up something 
   more original. To demonstrate as many features as possible
   let's create a FQDN-named bucket s3://public.s3tools.org:

   ~$ s3cmd mb s3://public.s3tools.org
   Bucket 's3://public.s3tools.org' created

5) List your buckets again with "s3cmd ls"
   Now you should see your freshly created bucket

   ~$ s3cmd ls
   2009-01-28 12:34  s3://public.s3tools.org

6) List the contents of the bucket

   ~$ s3cmd ls s3://public.s3tools.org
   ~$ 

   It's empty, indeed.

7) Upload a single file into the bucket:

   ~$ s3cmd put some-file.xml s3://public.s3tools.org/somefile.xml
   some-file.xml -> s3://public.s3tools.org/somefile.xml  [1 of 1]
    123456 of 123456   100% in    2s    51.75 kB/s  done

   Upload a two directory tree into the bucket's virtual 'directory':

   ~$ s3cmd put --recursive dir1 dir2 s3://public.s3tools.org/somewhere/
   File 'dir1/file1-1.txt' stored as 's3://public.s3tools.org/somewhere/dir1/file1-1.txt' [1 of 5]
   File 'dir1/file1-2.txt' stored as 's3://public.s3tools.org/somewhere/dir1/file1-2.txt' [2 of 5]
   File 'dir1/file1-3.log' stored as 's3://public.s3tools.org/somewhere/dir1/file1-3.log' [3 of 5]
   File 'dir2/file2-1.bin' stored as 's3://public.s3tools.org/somewhere/dir2/file2-1.bin' [4 of 5]
   File 'dir2/file2-2.txt' stored as 's3://public.s3tools.org/somewhere/dir2/file2-2.txt' [5 of 5]

   As you can see we didn't have to create the /somewhere
   'directory'. In fact it's only a filename prefix, not 
   a real directory and it doesn't have to be created in
   any way beforehand.

8) Now list the bucket contents again:

   ~$ s3cmd ls s3://public.s3tools.org
                          DIR   s3://public.s3tools.org/somewhere/
   2009-02-10 05:10    123456   s3://public.s3tools.org/somefile.xml

   Use --recursive (or -r) to list all the remote files:

   ~$ s3cmd ls s3://public.s3tools.org
   2009-02-10 05:10    123456   s3://public.s3tools.org/somefile.xml
   2009-02-10 05:13        18   s3://public.s3tools.org/somewhere/dir1/file1-1.txt
   2009-02-10 05:13         8   s3://public.s3tools.org/somewhere/dir1/file1-2.txt
   2009-02-10 05:13        16   s3://public.s3tools.org/somewhere/dir1/file1-3.log
   2009-02-10 05:13        11   s3://public.s3tools.org/somewhere/dir2/file2-1.bin
   2009-02-10 05:13         8   s3://public.s3tools.org/somewhere/dir2/file2-2.txt

9) Retrieve one of the files back and verify that it hasn't been 
   corrupted:

   ~$ s3cmd get s3://public.s3tools.org/somefile.xml some-file-2.xml
   s3://public.s3tools.org/somefile.xml -> some-file-2.xml  [1 of 1]
    123456 of 123456   100% in    3s    35.75 kB/s  done

   ~$ md5sum some-file.xml some-file-2.xml
   39bcb6992e461b269b95b3bda303addf  some-file.xml
   39bcb6992e461b269b95b3bda303addf  some-file-2.xml

   Checksums of the original file matches the one of the 
   retrieved one. Looks like it worked :-)

   To retrieve a whole 'directory tree' from S3 use recursive get:

   ~$ s3cmd get --recursive s3://public.s3tools.org/somewhere
   File s3://public.s3tools.org/somewhere/dir1/file1-1.txt saved as './somewhere/dir1/file1-1.txt'
   File s3://public.s3tools.org/somewhere/dir1/file1-2.txt saved as './somewhere/dir1/file1-2.txt'
   File s3://public.s3tools.org/somewhere/dir1/file1-3.log saved as './somewhere/dir1/file1-3.log'
   File s3://public.s3tools.org/somewhere/dir2/file2-1.bin saved as './somewhere/dir2/file2-1.bin'
   File s3://public.s3tools.org/somewhere/dir2/file2-2.txt saved as './somewhere/dir2/file2-2.txt'

   Since the destination directory wasn't specified s3cmd 
   saved the directory structure in a current working 
   directory ('.'). 

   There is an important difference between:
      get s3://public.s3tools.org/somewhere
   and
      get s3://public.s3tools.org/somewhere/
   (note the trailing slash)
   S3cmd always uses the last path part, ie the word
   after the last slash, for naming files.
 
   In the case of s3://.../somewhere the last path part 
   is 'somewhere' and therefore the recursive get names
   the local files as somewhere/dir1, somewhere/dir2, etc.

   On the other hand in s3://.../somewhere/ the last path
   part is empty and s3cmd will only create 'dir1' and 'dir2' 
   without the 'somewhere/' prefix:

   ~$ s3cmd get --recursive s3://public.s3tools.org/somewhere /tmp
   File s3://public.s3tools.org/somewhere/dir1/file1-1.txt saved as '/tmp/dir1/file1-1.txt'
   File s3://public.s3tools.org/somewhere/dir1/file1-2.txt saved as '/tmp/dir1/file1-2.txt'
   File s3://public.s3tools.org/somewhere/dir1/file1-3.log saved as '/tmp/dir1/file1-3.log'
   File s3://public.s3tools.org/somewhere/dir2/file2-1.bin saved as '/tmp/dir2/file2-1.bin'

   See? It's /tmp/dir1 and not /tmp/somewhere/dir1 as it 
   was in the previous example.

10) Clean up - delete the remote files and remove the bucket:

   Remove everything under s3://public.s3tools.org/somewhere/

   ~$ s3cmd del --recursive s3://public.s3tools.org/somewhere/
   File s3://public.s3tools.org/somewhere/dir1/file1-1.txt deleted
   File s3://public.s3tools.org/somewhere/dir1/file1-2.txt deleted
   ...

   Now try to remove the bucket:

   ~$ s3cmd rb s3://public.s3tools.org
   ERROR: S3 error: 409 (BucketNotEmpty): The bucket you tried to delete is not empty

   Ouch, we forgot about s3://public.s3tools.org/somefile.xml
   We can force the bucket removal anyway:

   ~$ s3cmd rb --force s3://public.s3tools.org/
   WARNING: Bucket is not empty. Removing all the objects from it first. This may take some time...
   File s3://public.s3tools.org/somefile.xml deleted
   Bucket 's3://public.s3tools.org/' removed

Hints
-----
The basic usage is as simple as described in the previous 
section.

You can increase the level of verbosity with -v option and 
if you're really keen to know what the program does under 
its bonet run it with -d to see all 'debugging' output.

After configuring it with --configure all available options
are spitted into your ~/.s3cfg file. It's a text file ready
to be modified in your favourite text editor.

For more information refer to:
* S3cmd / S3tools homepage at http://s3tools.org
* Amazon S3 homepage at http://aws.amazon.com/s3

Enjoy!

Michal Ludvig
* michal@logix.cz
* http://www.logix.cz/michal



salisbury
------------------------------------------------------------------------------------------------
**Title**: salisbury

**Web address**: https://api.github.com/repos/tlevine/salisbury

**Date**: November 2012

**Description**: Salisbury
=======

Salisbury sets up [mincemeat](https://github.com/michaelfairley/mincemeatpy)
workers and a mincemeat server.

## Install

### Master
Install mincemeat.

    pip install mincemeat

### Worker
Set up a worker by running `dependencies-user.sh` as the user on the worker.

## Run
You must start the master before the workers.

### Master
Run your mincemeat program. For example,

    ./example.py

### Several processes on one worker
As the user, activate and run `run.sh`. This will start as many processes
as you have cores.

### Several processes on several workers
Adjust `bin/workers` for your configuration, then activate and run `workers`.
This will ssh to all of the worker systems and run `run.sh`.

## References
Spawning multiple processes in shell
* http://kochanski.org/blog/?p=326
* http://elonen.iki.fi/code/misc-notes/core-split/index.html
* http://prll.sourceforge.net/shell_parallel.html



Scalable-Big-Data-Cloud-ROI-Agile-Fixie-API-with-HTML9-Responsive-Boilerstrap-JS
------------------------------------------------------------------------------------------------
**Title**: Scalable-Big-Data-Cloud-ROI-Agile-Fixie-API-with-HTML9-Responsive-Boilerstrap-JS

**Web address**: https://api.github.com/repos/tlevine/Scalable-Big-Data-Cloud-ROI-Agile-Fixie-API-with-HTML9-Responsive-Boilerstrap-JS

**Date**: November 2012

**Description**: #deck.js

A JavaScript library for building modern HTML presentations. deck.js is flexible enough to let advanced CSS and JavaScript authors craft highly customized decks, but also provides templates and themes for the HTML novice to build a standard slideshow.

## Quick Start

This repository includes a `boilerplate.html` as a starting point, with all the extensions included. Just [download it](https://github.com/imakewebthings/deck.js/zipball/stable), open `boilerplate.html`, and start editing your slides.

## Documentation

Check out the [documentation page](http://imakewebthings.github.com/deck.js/docs) for more information on the methods, events, and options available in core and all the included extensions.  A sample standard slide deck is included in the package under the `introduction` folder.  You can also [view that sample deck](http://imakewebthings.github.com/deck.js/introduction) online to play with the available style and transition themes.

## Extensions, Themes, and Related Projects

Take a look at [the wiki](https://github.com/imakewebthings/deck.js/wiki) for lists of extensions, themes, and other related goodies.  If you have a publicly available project of your own, feel free to add to the list.

## Dependencies (included in this repository)

- [jQuery](http://jquery.com)
- [Modernizr](http://modernizr.com)

## Tests & Support

Unit tests are written with [Jasmine](http://pivotal.github.com/jasmine/) and [jasmine-jquery](https://github.com/velesin/jasmine-jquery). You can [run them here](http://imakewebthings.github.com/deck.js/test).

deck.js has been tested with jQuery 1.6+ and works in IE7+, Chrome, FF, Safari, and Opera. The more capable browsers receive greater enhancements, but a basic cutaway slideshow will work for all browsers listed above. Please don't give your presentations in IE6.

For any questions or general discussion about deck.js please direct your attention to the [mailing list](http://groups.google.com/group/deckjs) (uses Google groups.)  If you would like to report a bug, please see the [issues page](https://github.com/imakewebthings/deck.js/issues).

## Known Bug(s)

There is an issue with certain builds of Chrome that result in a solid blue background and generally broken decks.  This is a bug in Chrome ([Issue 91518](http://code.google.com/p/chromium/issues/detail?id=91518)) that stems from hardware acceleration of 3d transforms.  Current workarounds:

- Use a different browser. This problem doesn't exist in Safari, FF, Opera.
- Disable hardware compositing by setting `--disable-accelerated-compositing` in the Chrome loading options
- Replace instances of `translate3d` with `translate` in the CSS of your decks (though this will slow down performance on iOS devices and Safari.)

Firefox contains a bug that allows users to scroll horizontally using the trackpad despite `overflow-x:hidden`. ([Bug 664275](https://bugzilla.mozilla.org/show_bug.cgi?id=664275) and [Bug 325942](https://bugzilla.mozilla.org/show_bug.cgi?id=325942).) If anyone knows of any workarounds to this issue please contact me.

## Printing

Core includes stripped down black and white print styles for the standard slide template that is suitable for handouts.

## Awesome Contributors

- [jbuck](https://github.com/jbuck)
- [cykod](https://github.com/cykod)
- [dougireton](https://github.com/dougireton)
- [awirick](https://github.com/awirick)
- Daniel Knittl-Frank
- [alexch](https://github.com/alexch)

If you would like to contribute a patch to deck.js please do as much as you can of the following:

- Add or amend Jasmine tests.
- Add inline documentation.
- If the standard snippet of an extension changes, please change it in both the introduction deck and the snippet html in the extension folder.
- If the API changes, it would be awesome to receive a parallel pull request to the gh-pages branch which updates the public-facing documentation.

## License

Copyright (c) 2011-2012 Caleb Troughton

Dual licensed under the [MIT license](https://github.com/imakewebthings/deck.js/blob/master/MIT-license.txt) and [GPL license](https://github.com/imakewebthings/deck.js/blob/master/GPL-license.txt).


scales-lightning-talk
------------------------------------------------------------------------------------------------
**Title**: scales-lightning-talk

**Web address**: https://api.github.com/repos/tlevine/scales-lightning-talk

**Date**: October 2013

**Description**: http://www.meetup.com/r-enthusiasts/events/140333872/


scarsdale-data
------------------------------------------------------------------------------------------------
**Title**: scarsdale-data

**Web address**: https://api.github.com/repos/tlevine/scarsdale-data

**Date**: May 2013

**Description**: Scarsdale Data-Journalism
===================

## Data sources

* [Online court records search](http://www.courtreference.com/Courts-Online.php?court_records=Scarsdale_Village_Court&court=12500)
* [Village contacts](http://www.scarsdale.com/ContactUs.aspx)
* [New York State FOIL documentation](http://www.dos.ny.gov/coog/right_to_know.html)

## Issues of popular concern

* [Scarsdale Inquirer headlines](http://www.scarsdalenews.com/Scarsdale_Inquirer/SCARSDALE_NEWS_Archives.html)
* The [year in review](http://www.scarsdalenews.com/Scarsdale_Inquirer/SCARSDALE_NEWS_123011.html)
    references a bunch of road and stormwater management projects
* I suspect schools to interest everyone.
* [Google search](https://www.google.com.ar/search?sugexp=chrome,mod=12&sourceid=chrome&ie=UTF-8&q=scarsdale#q=scarsdale&hl=en&prmd=imvns&source=univ&tbm=nws&tbo=u&sa=X&ei=2spQUMrVK4bs9ATvt4BY&ved=0CD8QqAI&bav=on.2,or.r_gc.r_pw.r_cp.r_qf.&fp=78bd13a7c0b676d2&biw=1280&bih=710) for Scarsdale News
* Scarsdale on [Patch](http://scarsdale.patch.com)

## Standard ideas

Tips that Beth photographed

![](ideas/basic-car-stories.jpg)

Tips from CSV from my notebook

?

## More ideas
I want a hyper-local indicator of inflation. I think prices at Lange's would
work pretty well.

## FOIL form
Scarsdale has a [form](http://www.scarsdale.com/Portals/0/Clerk/foil-form.pdf)
for FOI requests.


scarsdale-property-assessments
------------------------------------------------------------------------------------------------
**Title**: scarsdale-property-assessments

**Web address**: https://api.github.com/repos/tlevine/scarsdale-property-assessments

**Date**: November 2012

**Description**: 
# Backbone boilerplate
[http://backboneboilerplate.com](http://backboneboilerplate.com) is a community driven effort to help developers learn and rapidly deploy single page web applications.

## Philosophy
Coming soon

## Other resources

[http://backbonetutorials.com](http://backbonetutorials.com) - As single page apps and large scale javascript applications become more prominent on the web, useful resources for those developers who are jumping the ship are crucial.

## About the author

**Contact:**

*   [@neutralthoughts](http://twitter.com/neutralthoughts) on twitter
*   Github - https://github.com/thomasdavis
*   thomasalwyndavis@gmail.com

**Projects:**

*   Javascript Library CDN - http://cdnjs.com
*   Backbone.js Tutorials - http://backbonetutorials.com
*   Proposal Generation Start up - http://protosal.com
*   Technical Blog - http://thomasdavis.github.com
*   Quora - http://www.quora.com/Thomas-Davis
*   StackOverflow - http://stackoverflow.com/users/580675/thomas-davis

Love you mum!
<img alt="Clicky" width="1" height="1" src="//in.getclicky.com/66606907ns.gif" />

science-hack-day-resources
------------------------------------------------------------------------------------------------
**Title**: science-hack-day-resources

**Web address**: https://api.github.com/repos/tlevine/science-hack-day-resources

**Date**: September 2013

**Description**: We copy resources from disparate
[Science Hack Day wiki pages](http://sciencehackday.pbworks.com)
into one [book about citizen science](https://en.wikibooks.org/wiki/Citizen_Science).

Run this to return links in mediawiki syntax.

    ./science.py

Separate Twitter.

    ./science.py | grep twitter | sed 's+http://+https://+' | sort -u
    ./science.py | grep -v twitter


scott
------------------------------------------------------------------------------------------------
**Title**: scott

**Web address**: https://api.github.com/repos/tlevine/scott

**Date**: August 2013

**Description**: **Scott**'s goal is to automate the tedious parts of
[Scott](http://healthygulf.org/who-we-are/staff/)'s
job so that he has time to do the important parts.

## How to

Clone with submodules

    git clone git@github.com:tlevine/scott.git --recursive

Install dependencies

    # For the server
    (
      cd server
      npm install
    )

    # For the reader
    (
      cd reader
      sudo pip2 install -r requirements.txt
    )

    # In Arch
    sudo pacman -S python2-lxml python2-cssselect \
      tesseract-data-eng tesseract coffee-script \
      poppler imagemagick git

    # In Ubuntu 12.04
    echo Install node 0.8 from source. Then,
    sudo apt-get install python-lxml tesseract-ocr tesseract-ocr-eng \
      coffeescript git poppler-utils imagemagick

`server` contains a restify web server, and `client`
contains a Backbone application. The server also serves
the client files. Serve the site like so

    ./serve

During development, it expects a database to be in /tmp/wetlands.db.
This often ram, so you might need to put one there.

## Useful stuff

* [Automatic permit application data](http://wetlands.thomaslevine.com)
* [Parish coordinates](https://twitter.com/ian_villeda/status/267334042507169793)

## Network of servers
This is run across two computers.

1. One (`server`) serves the node application, the downloader/reader, the database
    and the primary documents
2. A second (`desk`) coordinates backups; it periodically pulls the documents and
    listings submodules and the logs directory and then pushes those to external
    git repository hosting services.

## Downloading old documents
You can see all the previous permit applications from Scott's
manual spreadsheet parsings
[here](https://github.com/tlevine/mvn-www2-backup)
(except for a couple that went missing).

If you want to integrate them with your csv file,
switch everything in the respective urls before the
final (sixth) slash for
"`https://raw.github.com/tlevine/mvn-www2-backup/master/documents`"

For example,
[`http://www2.mvn.usace.army.mil/ops/regulatory/pdf/document2011-09-09-103911.pdf`](http://www2.mvn.usace.army.mil/ops/regulatory/pdf/document2011-09-09-103911.pdf)
becomes
[`https://raw.github.com/tlevine/mvn-www2-backup/master/documents/document2011-09-09-103911.pdf`](https://raw.github.com/tlevine/mvn-www2-backup/master/documents/document2011-09-09-103911.pdf)

You can also download them all at once
[here](https://github.com/tlevine/mvn-www2-backup/archive/master.zip)

## contributors

* Thomas Levine
* Daniel Schneiderman


scott-documents
------------------------------------------------------------------------------------------------
**Title**: scott-documents

**Web address**: https://api.github.com/repos/tlevine/scott-documents

**Date**: July 2013

**Description**: 

scott-finalips
------------------------------------------------------------------------------------------------
**Title**: scott-finalips

**Web address**: https://api.github.com/repos/tlevine/scott-finalips

**Date**: June 2013

**Description**: scott-finalips
==============

ORM Permit Decisions for Final Individual Permits


scott-listings
------------------------------------------------------------------------------------------------
**Title**: scott-listings

**Web address**: https://api.github.com/repos/tlevine/scott-listings

**Date**: May 2013

**Description**: 

scott-listings-2
------------------------------------------------------------------------------------------------
**Title**: scott-listings-2

**Web address**: https://api.github.com/repos/tlevine/scott-listings-2

**Date**: May 2013

**Description**: These are copies of [this page](http://www.mvn.usace.army.mil/Missions/Regulatory/PublicNotices.aspx) and the associated [RSS feed](http://www.mvn.usace.army.mil/DesktopModules/DigArticle/RSS.ashx?portalid=56&moduleid=23919).


scott-logs
------------------------------------------------------------------------------------------------
**Title**: scott-logs

**Web address**: https://api.github.com/repos/tlevine/scott-logs

**Date**: August 2013

**Description**: 

scott-map-prototype
------------------------------------------------------------------------------------------------
**Title**: scott-map-prototype

**Web address**: https://api.github.com/repos/tlevine/scott-map-prototype

**Date**: August 2013

**Description**: Map prototype for Scott
====
[Scott](http://scott.thomaslevine.com) is collecting data on applications to
dredge or fill Louisiana wetlands, and we want to display the overall data.

In particular, Scott thinks a choropleth map showing the impacts by watershed
would be informative. Here we prototype such a concept.


Transformed the shape files into GeoJSON using this command: ogr2ogr -f "GeoJSON" -t_srs "WGS84" parishes.json Parishes.shp
##


scraperwiki-exp14
------------------------------------------------------------------------------------------------
**Title**: scraperwiki-exp14

**Web address**: https://api.github.com/repos/tlevine/scraperwiki-exp14

**Date**: July 2012

**Description**: Look at job postings for 10 companies that sell social media analysis. Do they all say Excel?

I went to a bunch of social media analytics companies and looked at all of the "analyst" jobs.


scraperwiki-export
------------------------------------------------------------------------------------------------
**Title**: scraperwiki-export

**Web address**: https://api.github.com/repos/tlevine/scraperwiki-export

**Date**: June 2013

**Description**: # scraperwiki-export

This tool provides an easy way for you to retrieve your data and code from ScraperWiki Classic.

The quickest way to run it, assuming you have the [go](http://golang.org/doc/install) compiler installed is to do:

  * git clone git@github.com:rossjones/scraperwiki-export.git
  * cd scraperwiki-export
  * export GOPATH=\`pwd\`
  * go run *.go --user YOUR_USERNAME --output ./data 
  * Go make a cup of tea.
  
  
If you don't have the go compiler installed, you'll need to wait for a binary.


## Still to-do

 * ~~Progress info~~
 * ~~Fetch a couple at a time~~
 * ~~See if SW supports range for re-starts~~
 * Implement fetching a substring match of the name
 * Better error checking
 * ~~See if we can get private scrapers~~


scraperwiki-kpi
------------------------------------------------------------------------------------------------
**Title**: scraperwiki-kpi

**Web address**: https://api.github.com/repos/tlevine/scraperwiki-kpi

**Date**: July 2012

**Description**: ScraperWiki Key Performance Indicators
======

Generate the table of interest on the server.
[This script](https://bitbucket.org/ScraperWiki/scraperwiki/src/a25f71204062/web/kpi/dump_user_table.sh) might help.

    SELECT username, last_login, date_joined FROM auth_user;

Download it.

    scp -P 7822 scraperdeploy@rush.scraperwiki.com:~/2012-04-23.csv .

Generate the plots.

    ./kpi.r

Look at them.

    evince coder_activity.pdf


scraperwiki-snippets
------------------------------------------------------------------------------------------------
**Title**: scraperwiki-snippets

**Web address**: https://api.github.com/repos/tlevine/scraperwiki-snippets

**Date**: September 2012

**Description**: 

scraperwiki_local
------------------------------------------------------------------------------------------------
**Title**: scraperwiki_local

**Web address**: https://api.github.com/repos/tlevine/scraperwiki_local

**Date**: September 2012

**Description**: The canonical repository is now
[here](https://github.com/scraperwiki/scraperwiki_local).


scraperwiki_local_dummy
------------------------------------------------------------------------------------------------
**Title**: scraperwiki_local_dummy

**Web address**: https://api.github.com/repos/tlevine/scraperwiki_local_dummy

**Date**: September 2012

**Description**: 

script_kiddie_bot_confuser
------------------------------------------------------------------------------------------------
**Title**: script_kiddie_bot_confuser

**Web address**: https://api.github.com/repos/tlevine/script_kiddie_bot_confuser

**Date**: November 2011

**Description**: Script kiddies crawl your site looking for security vulnerabilities.
Your site is secure because you're awesome, but you still find these
script kiddies annoying.

The script kiddy bot confuser responds in confusing ways to the requests
of these bots.

Since the bots are likely running in threads rather than event loops,
responding slowly will probably slow down their crawls. The default
behavior of the script kiddy bot confuser is not to respond at all
to the bots.

If you want to ban the users, you need to install django-ban and ipcalc
and then make some change in the settings file.

CHANGE GOES HERE


sed.js
------------------------------------------------------------------------------------------------
**Title**: sed.js

**Web address**: https://api.github.com/repos/tlevine/sed.js

**Date**: July 2013

**Description**: # sed.js

Unix sed for node.js

Intended to be POSIX compliant.

Any incorrect behaviour for a POSIX compliant script will be
considered a bug. Please report it at
https://github.com/drj11/sed.js/issues

# Source Code

github: https://github.com/drj11/sed.js
git: https://github.com/drj11/sed.js.git
tar: https://github.com/drj11/sed.js/archive/sed.js-0.0.0.tar.gz

# Current implementation status

Implemented
 * All POSIX options (`-e`, `-f`, and `-n`)
 * Implicit stdin and input from file arguments
 * All POSIX verbs (`#:=abcDdGgHhilNnpqrstwxy{`)
 * All (POSIX) addresses: numeric, `$`, context
 * Detection of `#n` at start of script
 * Empty REs in addresses and `s` pattern
 * Correct Exit Status

# Tests

    npm install # to install urchin
    npm test



settlers-timer
------------------------------------------------------------------------------------------------
**Title**: settlers-timer

**Web address**: https://api.github.com/repos/tlevine/settlers-timer

**Date**: January 2013

**Description**: 

sf-decoder
------------------------------------------------------------------------------------------------
**Title**: sf-decoder

**Web address**: https://api.github.com/repos/tlevine/sf-decoder

**Date**: August 2013

**Description**: Send this https://github.com/opengovfoundation/San-Francisco-Code

to this https://github.com/statedecoded/statedecoded


SHD_SF13
------------------------------------------------------------------------------------------------
**Title**: SHD_SF13

**Web address**: https://api.github.com/repos/tlevine/SHD_SF13

**Date**: September 2013

**Description**: # Convert a ROOT file to HDF5

Physicists use [ROOT](http://root.cern.ch/) because that's what they
know. The ROOT software stores its data in a ROOT file format that
no other software can read. This is fine if you're only using the ROOT
software, but physicists are the only people who use ROOT, and even
physicists sometimes want to use  something else. So wouldn't it be nice 
to convert ROOT files into a more standard format?

I wrote an example program for how to convert a ROOT file to HDF5,
a more standard file format. The program expects a very particular
dataset; it doesn't detect the schema of the dataset. Thus, it won't
automatically convert arbitrary files. Instead, the script serves as
a template for when you should need to do such a conversion.

## How to run

First, install the dependencies. You need

* ROOT
* Python 2.7
* `/usr/lib/root` in your `PYTHONPATH`
* `wget`

On Arch Linux, you can just run.

    make dep

Once you have the dependencies, this is how you download the datasets
and run the demo.

    make

The file that is running all of this is `root2hdf5.py`.


smart-recovery
------------------------------------------------------------------------------------------------
**Title**: smart-recovery

**Web address**: https://api.github.com/repos/tlevine/smart-recovery

**Date**: April 2013

**Description**: Smart Recovery Meetings
===
Run this like so.

    ./run.sh

Results go to `smart.csv`.

## In detail
Here are directions for running each step at a time.

Download the webpage.

    ./download.sh

Parse it to SQLite.

    ./parse.py

Save as CSV.

    sqlite3 -header -csv /tmp/smart.db 'SELECT * FROM smart;' > smart.csv


smtpcli
------------------------------------------------------------------------------------------------
**Title**: smtpcli

**Web address**: https://api.github.com/repos/tlevine/smtpcli

**Date**: July 2013

**Description**: ======================================
`smtpcli` is SMTP CLI tool.
======================================

This command line tool for SMTP client.

Requirements
------------

* Python 2.7 or later.


Setup
-----

Case: Github

::

   $ git clone https://github.com/kmn/smtpcli
   $ cd smtpcli
   $ sudo python setup.py install
   $ cp sample/smtpcli.conf.sample $HOME/.smtpcli.conf
 
   and edit $HOME/.smtpcli.conf

Case: Pypi

::
   $ sudo pip install smtpcli
   $ cp sample/smtpcli.conf.sample $HOME/.smtpcli.conf
 
   and edit $HOME/.smtpcli.conf

Usage
-----

Setting the configuration file ($HOME/.smtpcli.conf)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

An alternative method of command options that use the config file.
Copy examples/.smtpcli.conf.sample to `$HOME/.smtpcli.conf`. `password` key to set password in plain text.

::

   # 
   [smtpcli]
   smtp-server  = smtp.exmple.org
   port         = 25
   encoding     = ISO-2022-JP
   mailaddress  = user001@exmple.org
   password     = user001pw

Sending mail
~~~~~~~~~~~~~~~~~~~~

::
   $ echo "Hello, world!" > msg.txt
   $ smtpcli --to user007@example.org  --subject "from 001 to 007" \
     --file msg.txt

   To: user007@example.org
   Subject: from 001 to 007
   Body: Hello, world!

   Send this email? [y/N] 


History
-------

0.5 (2012-09-16)
~~~~~~~~~~~~~~~~
* fix minor bugs

0.4 (2012-09-16)
~~~~~~~~~~~~~~~~
* fix minor bugs

0.3 (2012-09-16)
~~~~~~~~~~~~~~~~
* fix minor bugs

0.2 (2012-09-16)
~~~~~~~~~~~~~~~~
* fix minor bugs

0.1 (2012-09-16)
~~~~~~~~~~~~~~~~
* modify setup.py

0.0 (2012-06)
~~~~~~~~~~~~~~~~
* first release


See also
--------

* `RFC 821  <http://tools.ietf.org/html/rfc821.html>`_
* `RFC 1896 <http://tools.ietf.org/html/rfc1869.html>`_


Smuglr
------------------------------------------------------------------------------------------------
**Title**: Smuglr

**Web address**: https://api.github.com/repos/tlevine/Smuglr

**Date**: April 2013

**Description**: Smuglr
======

Smuglr is a Python library that contains a lightweight (depends on Python standard lib only) API for retrieving photos and albums from SmugMug and a tool (smuglr.py) for syncing SmugMug albums to your computer. This is useful when you have family and friends that share photos on SmugMug but you'd like to download the photos and albums to your own computer.

If you are interested in this library and plan on signing up with SmugMug. Consider using my referral code: https://secure.smugmug.com/signup.mg?Coupon=5qLwf6TGUTmIk (5qLwf6TGUTmIk). You'll save $5 on your subscription and I'll receive a $10 coupon. 

Usage
----------

    # For now, you'll need to clone the git repo:
    git clone git://github.com/jzellman/smuglr.git
    cd smuglr
    # print help
    python smuglr.py -h
    # list albums located at someaccount.smugmug.com
    python smuglr.py albums -a 'someaccount'
    # list albums located at someaccount.smugmug.com which has a site password of password
    python smuglr.py albums -a 'someaccount' -p password
    # sync all albums at someaccount.smugmug.com to ~/Pictures/Smuglr/someaccount/
    python smuglr.py sync -a 'someaccount' -p password
    # sync album named Birthday Pictures for account someaccount.smugmug.com to ~/Pictures/Smuglr/someaccount/Birthday Pictures/
    python smuglr.py sync-album -a 'someaccount' -p password "Birthday Pictures"
    
Contributing
------------

Once you've made your commits:

1. [Fork](http://help.github.com/forking/) Smuglr
2. Create a topic branch - `git checkout -b my_branch`
3. Push to your branch - `git push origin my_branch`
4. Create a [Pull Request](http://help.github.com/pull-requests/) from your branch
5. Profit! 



smugmug-download
------------------------------------------------------------------------------------------------
**Title**: smugmug-download

**Web address**: https://api.github.com/repos/tlevine/smugmug-download

**Date**: May 2013

**Description**: SmugMug Download
=====
I'm ending my SmugMug account in July, and I'm gonna move the files somewhere
better, probably S3 or a VPS. As part of this, I need to download the files.

Someone wrote a [SmugMug uploader](http://braindump.dk/tech/2007/10/03/smugmug-uploader/),
and someone else [adapted](http://www.bootswithdefer.com/article.php?story=20071227102301195)
it for downloading, but I haven't found his his code.

Separately, someone made [smugdump](https://github.com/aarone/smugmug-to-flickr),
another person made [smugmug-sync](https://github.com/simoncoles/smugmug-sync),
and another person made [Smuglr](https://github.com/jzellman/Smuglr).

There's also [Smugmug-Get-Images](https://github.com/klinquist/Smugmug-Get-Images/blob/master/getimages.php),
but that seems less pleasant.


smugmug-sync
------------------------------------------------------------------------------------------------
**Title**: smugmug-sync

**Web address**: https://api.github.com/repos/tlevine/smugmug-sync

**Date**: June 2011

**Description**: = smugmug-sync

This is just a simple script that takes a copy of everything that's in your SmugMug account and puts it into a directory on the local file system. If you run it again with the same parameters it won't download files it already has, so you can stop the download and restart as needed, or re-run it to get new photos you have uploaded to SmugMug.

Note that this won't remove files which have been downloaded in a previous session which aren't in SmugMug any more (presumably because you deleted them).


== Usage
The following parameters are required:

username:: Your SmugMug username
password:: Password for your SmugMug account
destination:: Directory you want your files to go to

  ./download.rb --verbose --username prince --password charming --destination "My SmugMug"


== Status

This has had minimal testing - it works for me :-). Happy to take bug reports, questions etc. but I can't claim to know what I am doing, this was just a weekend hack.

This is a command line tool and I realise the audience is therefore restricted to those who can handle the Terminal and maybe a bit of Ruby. Which I feel bad about but I don't have the time to do anything else.


== Motivation 

I really like SmugMug, but I feel better having a copy of everything on a local disk too. SmugMug used to have a service which would email a DVD to you which was great, but that isn't currently available. You can download all the files for a particular album, but I just wanted something I could run and it would work.

== Note on Patches/Pull Requests
 
* Fork the project.
* Make your feature addition or bug fix.
* Add tests for it. This is important so I don't break it in a
  future version unintentionally.
* Commit, do not mess with rakefile, version, or history.
  (if you want to have your own version, that is fine but bump version in a commit by itself I can ignore when I pull)
* Send me a pull request. Bonus points for topic branches.

== Copyright

Copyright (c) 2010 Simon Coles. See LICENSE for details.


smugmug-to-flickr
------------------------------------------------------------------------------------------------
**Title**: smugmug-to-flickr

**Web address**: https://api.github.com/repos/tlevine/smugmug-to-flickr

**Date**: May 2013

**Description**: 
Smugmug => Flickr
==
This project contains python scripts for transferring images from smugmug to flickr:
 1. smugdump.py: download smugmug images to disk
 2. flickrupload.py : upload images output by 1 to flickr
 
The scripts attempt to keep photo metadata around for image filenames, titles,
and tags.  The scripts were created in a few hours to get my photos transferred to flickr so the quality and generality of the software is not all that good.

SmugMug albums are turned into flickr photosets.  I did not keep the 'key' or primary image for photosets/albums consistent across sites.

Requirements
==
 * smugmug-api : http://code.google.com/p/smugmug-api/
 * PyYAML : http://pyyaml.org/wiki/PyYAML
 * flickapi : http://stuvel.eu/projects/flickrapi

Caveats
==
I was too lazy to make the scripts take command line arguments.  See config.cfg for config options.


The error handling also sucks.  During uploads and downloads, any errors result in a message to the console and the upload continues.  I'm not looking for perfect parity between my SmugMug account and Flickr; some missing photos are not a big deal to me and the cost of implementing retry queues seemed out of scope.

If you have multiple SmugMug albums with the same name, the script tries to do something tricky and create unique photoset names when uploading photos to Flickr.  I didn't really test this out so beware.

Good luck and enjoy!

smugpy
------------------------------------------------------------------------------------------------
**Title**: smugpy

**Web address**: https://api.github.com/repos/tlevine/smugpy

**Date**: April 2013

**Description**: Smugpy [![Build Status](https://secure.travis-ci.org/chrishoffman/smugpy.png?branch=master)](http://travis-ci.org/chrishoffman/smugpy) [![Coverage Status](https://coveralls.io/repos/chrishoffman/smugpy/badge.png?branch=master)](https://coveralls.io/r/chrishoffman/smugpy)
======

Smugpy is an Python 2.x/3.x library for the [SmugMug](https://secure.smugmug.com/signup.mg?Coupon=2TqKwSOXw5HeU) API created by Chris Hoffman.  Smugpy supports all versions of the API and Oauth 1.0 for API versions 1.2.2+.  This library also works in [Google App Engine](http://code.google.com/appengine/).  For more information on the SmugMug API, see [SmugMug API Documentation](http://wiki.smugmug.net/display/API/).

Installation
------------

The latest **stable version** of smugpy can always be installed via [pip](http://www.pip-installer.org/en/latest/index.html):
    
    pip install -U smugpy

Or, you can install the **development version** directly from GitHub:

    pip install -U https://github.com/chrishoffman/smugpy/tarball/master

Or, download the package and install manually:

    python setup.py install

Usage
-----
Simple request (1.3.0+):

```python
from smugpy import SmugMug

API_KEY = "XXXXXXXXXXXXXXXXXXXXXXXXX"

smugmug = SmugMug(api_key=API_KEY, api_version="1.3.0", app_name="TestApp")
albums = smugmug.albums_get(NickName="williams")

for album in albums["Albums"]:
    print "%s, %s" % (album["id"], album["Title"])
```
For more examples, see the [examples](https://github.com/chrishoffman/smugpy/tree/master/examples) directory.

Helping Out
-----------
If you notice any problems, please report them to the GitHub issue tracker at [https://github.com/chrishoffman/smugpy/issues](http://github.com/chrishoffman/smugpy/issues). 

License
-------
Smugpy is released under the MIT license.


snippets
------------------------------------------------------------------------------------------------
**Title**: snippets

**Web address**: https://api.github.com/repos/tlevine/snippets

**Date**: January 2013

**Description**: 

snooze
------------------------------------------------------------------------------------------------
**Title**: snooze

**Web address**: https://api.github.com/repos/tlevine/snooze

**Date**: May 2012

**Description**: 

social-media-public-data
------------------------------------------------------------------------------------------------
**Title**: social-media-public-data

**Web address**: https://api.github.com/repos/tlevine/social-media-public-data

**Date**: September 2013

**Description**: # Computers are cheap

===

# ?

<!--
We study the world so we can make better decisions,
build better things and satisfy our curiosity.
-->

---

![](world-data-world.jpg)

<!-- This is how we in this room do it. -->

---

## World -> Data

```
Special Operations,2005,2006,2007,2008,2009,Total
Emergency Service,0%,0%,1%,1%,0%,2%
Harbor Unit,0%,0%,0%,0%,0%,0%
Aviation Unit,0%,0%,0%,0%,0%,0%
Taxi Unit,0%,0%,0%,0%,0%,0%
Canine Unit,0%,0%,0%,0%,0%,0%
Mounted Unit,0%,0%,0%,0%,0%,0%
Headquarters,0%,0%,0%,0%,0%,0%
Special Operations Division Total,0%,0%,1%,1%,0%,2%
Percent of All Subject Officers Against Whom Allegations were Substantiated,0%,0%,0.2%,0.3%,0%,0.1%
```

When I say "data", I'm talking about tables.

<!-- 
The world is complicated; we need to simplify it in
order to understand it. Representing the world as
tabular data is one way of doing that.
-->

---


## Data -> Data

![](1000px-The_Normal_Distribution.svg.png)


<!--
Sometimes the data are still too complicated, so we
simplify these data further with statistics.

http://upload.wikimedia.org/wikipedia/commons/thumb/2/25/The_Normal_Distribution.svg/1000px-The_Normal_Distribution.svg.png
-->


---



## Data -> World


![](sf-shapefile.png)


<!--

We simplify the data enough that we can understand it,
then we convert our data back into other things, like
papers and graphs.

-->


---


![](world-data-world.jpg)

===


## Computers are cheap. How does that change things?

---

![](computers.jpg)


(Remember, "data" are tables.)

<!--

Some things got cheap faster than other things did.

-->
---

### Tradeoffs

Lots of automatically collected data

* More power
* Simpler models
* Not the exact data we want
* More work to store and process
* Less external validity

---

![](world-data-world.jpg)

---

# World -> Data

<!--
Now it's super cheap to collect data, sometimes.
We collect the data practically for free by measuring things that are already happening.
We don't worry too much about designing a research plan. We wind up with
very large, often complete, historical data.
Here are some convenient/inexpensive data collection approaches.

-->

---

![](leif.jpg)

http://dontflush.me/420

---

![](highlighted-public-notice.png)

---

![](piwik.png)

---

# Data -> Data

<!--
We decided to collect more data because it was cheap.
The resulting data are far more complicated than the data
coming out of questionnaires, so we need to do more statistics
and data transformations.
-->


---

![](logfiles.png)

---

    text = '''
    Todo lo cual hace un total de $ 7.534,74, que deberA! ser abonado por la demandada a
    la actora en la forma y con mA!s los intereses dispuestos en la sentencia de grado.
    En virtud de las argumentaciones expuestas y con arreglo a lo
    establecido por el art. 279 del CPCCN, corresponde adecuar la imposiciA3n de costas y
    los honorarios al resultado del pleito que se ha dejado propuesto para resolver la
    apelaciA3n.
    '''
    money_raw = re.findall(r"\$ ?[.,0-9]+", text)
    money = [float(re.sub('[^0-9]$', '', filter(lambda c: c in ",1234567890", m).replace(",", "."))) for m in money_raw]
    print money
    # [7534.74]

Find dollar amounts in free-form text

---

![](zipcode.png)

---

# Data -> World

---

[![](baseball.png)](http://www.nytimes.com/interactive/2013/08/02/sports/baseball/bang-for-your-buck.html)

---

* Google
* Yelp
* Foursquare
* Facebook
* Twitter
* ...

---

[![](fms-symphony-preview.png)](http://fms.csvsoundsystem.com)

===

# Creating meaningful datasets

---

## A text search is not statistics

---

## Designing experiments with Social media data

<!--
Collecting everything is akin to turning on a
security camera; we get all of the information
about the place where the camera is pointing
over time. If we want to know about other locations
or about specific times, we have to process
the stuff we collect to fit our curiousity.
-->

![](interrupted-time-series.png)

http://snap.stanford.edu/soma2010/papers/soma2010_1.pdf

---

## Control group

![](topsy.png)

http://topsy.com/analytics?q1=bacon&q2=statistics&via=Topsy

---

## Be careful: Selection bias

![](pew.png)

http://pewinternet.org/Reports/2012/Twitter-Use-2012/Findings.aspx

---

![](gender.png)

http://www.informationisbeautiful.net/visualizations/chicks-rule/

---

![](brain.png)

People tweet about things they're comfortable sharing.

http://harmony-institute.org/therippleeffect/2012/10/12/1265/

---

## Predict other variables: Fancy post-stratification

---

[![](tweet-times.jpg)](https://hackpad.com/Measuring-Socioeconomic-Indicators-in-Arabic-Tweets-IZ5ByP2LvIt)

---

![](weekly_gender)

http://www.quora.com/Natural-Language-Processing/Are-there-any-text-corpora-labeled-with-gender-age-of-authors
http://clic.cimec.unitn.it/amac/twitter_ngram/

Gender

---

![](us_middle_names_over_time.png)

Age

http://thomaslevine.com/!/middle-names

---

Occupation/specialization

![](cornell_middle_names_by_school.png)

http://thomaslevine.com/!/middle-names

# One more thought

The power of linking

* Twitter user
* User profile
* Lots of tweets
* Links to websites

---

[![](opendatasites.png)](http://opendatasites.com/)

===

# Computers are cheap

* Computers are cheap,
* so we collect a lot more data,
* so we can do a lot more with it,
* but we have to process it more,
* and we still have to do good statistics.


SocialValue
------------------------------------------------------------------------------------------------
**Title**: SocialValue

**Web address**: https://api.github.com/repos/tlevine/SocialValue

**Date**: November 2011

**Description**: 

socrata-analysis
------------------------------------------------------------------------------------------------
**Title**: socrata-analysis

**Web address**: https://api.github.com/repos/tlevine/socrata-analysis

**Date**: August 2013

**Description**: Socrata Analysis
======
I [downloaded](https://github.com/tlevine/socrata-download) the metadata files
for all of the Socrata datasets on all of the Socrata portals as of
approximately June 23, 2013. Let's do something fun with them.

I don't like to distinguish between metadata and data, and I think this analysis
embodies that mindset. Anything can be turned into data, and data can be turned
into anything. You can use metadata to annotate graphs and stuff, but you can
also make graphs and models of the metadata.

Themes that are coming out
--------
There's a lot of awesome stuff going on in Socrata,
but I had no idea about it until I started looking
into it this deeply.

I'l always seen Socrata as this clumsy, unnecessarily
engineered website for serving files and searching
for them. And I figured it had import APIs, but that's
not special.

The special thing, I'm now learning, is that ordinary
people can make graphs and maps of the data. I live in
this bubble of people who know how to use computers.
A random person is unlikely to know even how to make
a bar graph. And if she does, she'll only know how to
do it in Excel, and she might not even know how to
export it as a PDF.


socrata-catalog
------------------------------------------------------------------------------------------------
**Title**: socrata-catalog

**Web address**: https://api.github.com/repos/tlevine/socrata-catalog

**Date**: October 2013

**Description**: Socrata catalogs
=====
Learn things from the data catalog JSON file on Socrata portals.

This file is at [`/data.json`](https://data.oregon.gov/data.json),
and it has [this format](http://project-open-data.github.io/schema/).

## Database
Download the files, and put them in an SQLite3 database.

```sh
make db
```

The files will be in `./catalogs`, and the SQLite3 database will be in
`/tmp/catalogs.db`. Here are some fun queries.

```sql
SELECT format, count(*) FROM catalog GROUP BY format ORDER BY count(*);
SELECT portal, format, count(*) FROM catalog GROUP BY portal, format ORDER BY count(*);
SELECT portal, count(*) FROM catalog WHERE format = 'application/vnd.ms-excel' GROUP BY portal ORDER BY portal;
SELECT 'https://' || portal || '/-/-/' || identifier AS url, title from catalog where format LIKE '%excel%';
SELECT 'https://' || portal || '/-/-/' || identifier AS url, title FROM catalog WHERE portal = 'data.sfgov.org' AND format = 'application/octet-stream';
SELECT portal, format, count(*) FROM catalog GROUP BY portal, format ORDER BY portal, format;
```

## Analyses
Run any of the various analyses.

* `make formats`: What file formats do source data come from?

<!-- * `make external`: What external files are linked? -->


socrata-defederate
------------------------------------------------------------------------------------------------
**Title**: socrata-defederate

**Web address**: https://api.github.com/repos/tlevine/socrata-defederate

**Date**: September 2013

**Description**: Find the federation links between portals,
and then make pretty network graphs.

http://bost.ocks.org/mike/miserables/
https://gist.github.com/mbostock/4062045


socrata-download
------------------------------------------------------------------------------------------------
**Title**: socrata-download

**Web address**: https://api.github.com/repos/tlevine/socrata-download

**Date**: September 2013

**Description**: Download Socrata Portals
======
I download the contents of Socrata portals to a filesystem and then upload them
to S3.

## How to run (multiple portals)

Set the parameters S3 bucket.

    export SOCRATA_URLS=( data.cityofnewyork.us 
    export SOCRATA_S3_BUCKET=socrata.appgen.me

If you are using [Proxy Rack](http://www.proxyrack.com/) to get around API limits,
set the wget proxy parameters.

    export http_proxy=

Then run the main script.

    ./run.sh

This runs `./portals.py` to get the list of all portals from socrata.com.
Then it runs `./run_all.sh` (See below.) for each of the portals.

API limits appear to apply across all of Socrata, not just within
data portal, so these different portals just get run in series.

## How to run (one portal)

Set the parameters.

    SOCRATA_URL=data.cityofnewyork.us
    SOCRATA_S3_BUCKET=socrata.appgen.me

Then run the main script.

    ./run_one.sh

The result will be the following file structure, both locally and in the bucket.

    data.cityofnewyork.us/
      searches/
        1
        2
        ...
      views/
        abcd-efgh
        ijkl-mnop
        ...
      rows/
        abcd-efgh
        ijkl-mnop

## Components of the run script.
When you run `./run_all.sh`, the following things happen in order.

1. `./search.sh` searches/browses through all of the datasets/maps/views/&c.
    and saves all of the files as `$SOCRATA_URL/searches/$page_number`.
2. `./viewids.py` returns all of the 4x4 Socrata view id codes from the
    files in `$SOCRATA_URL/searches`.
3. `./views.sh` downloads the metadata files for each of the viewids and
    and saves all of the files as `$SOCRATA_URL/views/$viewid`.
4. `./rows.sh` downloads the data files for each of the viewids as CSV
    and saves all of the files as `$SOCRATA_URL/rows/$viewid`.
5. `./builddb.py` makes a SQLite3 database with one row per dataset, using
    features from the view and row files. It contains one table, called
    `datasets`. One of the columns is named `socrata.url`, so unioning it
    with databases from other portals will be easy. It is named
    `$SOCRATA_URL/features.db`.
6. `./s3-upload.sh` uploads all of the downloaded view metadata files to
    an S3 bucket, compressing them first.
7. `./s3-download.sh` downloads and decompresses all of the files in the S3 bucket.

## data.json
`catalogs.sh` downloads the catalogs from the `/data.json` endpoints of each portal.


socrata-music
------------------------------------------------------------------------------------------------
**Title**: socrata-music

**Web address**: https://api.github.com/repos/tlevine/socrata-music

**Date**: August 2013

**Description**: Generate music from Socrata portals
====

I'd like to use the meta-dataset, but here are some other possibilities.

https://data.baltimorecity.gov/Government/Minority-and-Women-s-Business-Enterprises-Certific/us2p-bijb?
https://data.illinois.gov/Environment/IEPA-Leaking-Underground-Storage-Tank-Incident/2kz4-t22j?

I came across these with this query.

    subset(socrata,
      ncol > 20 & nrow > 1000 & ncol < 70 & ncol.date >= 2 &
      !grepl('Campaign',name) & displayType=='table'
    )[c('portal','id','name','viewCount')]


socrata-nominate
------------------------------------------------------------------------------------------------
**Title**: socrata-nominate

**Web address**: https://api.github.com/repos/tlevine/socrata-nominate

**Date**: September 2013

**Description**: Suggest that analytics opened on Socrata portals.

## Check for the portals with analytics already

    ./has_analytics.sh

## Nominate the analytics

### Settings
Specify your credentials.

```sh
export SOCRATA_EMAIL=tom@example.com
export SOCRATA_PASSWORD=hmkmovq223h89u,hr9on
```

### Run
Request that the analytics page be opened.

```sh
phantomjs nominate.js
```

This reads the description from `description.txt`.

### Limitations
This tool doesn't support the attachment field.


socrata-when-users-join
------------------------------------------------------------------------------------------------
**Title**: socrata-when-users-join

**Web address**: https://api.github.com/repos/tlevine/socrata-when-users-join

**Date**: September 2013

**Description**: Run analysis, caching downloads.

    ./analysis.py

You can also download a bunch of things at once.

    ./download.py


soda-scala
------------------------------------------------------------------------------------------------
**Title**: soda-scala

**Web address**: https://api.github.com/repos/tlevine/soda-scala

**Date**: August 2013

**Description**: # Soda-Scala

Scala bindings for the SODA2 API.

## Getting it

Soda-Scala is published to maven central.  There are two artifacts,
`soda-publisher-scala` which contains everything, and
`soda-consumer-scala` which contains only those features required for
read-access.  SBT configuration can be done by adding

```scala
libraryDependencies += "com.socrata" %% "soda-publisher-scala" % "1.0.0"
```

to your `.sbt` file, while for Maven, the pom snippet is:

```xml
<dependencies>
  <dependency>
    <groupId>com.socrata</groupId>
    <artifactId>soda-publisher-scala_${scala.version}</artifactId>
    <version>1.0.0</version>
  </dependency>
</dependencies>
```

Soda-scala is published for Scala versions 2.8.1, 2.8.2, 2.9.0,
2.9.0-1, 2.9.1, 2.9.1-1, and 2.9.2.

## Sample code

The soda-scala-sample subproject contains sample code for reading and
writing.

## Interop with databinder-dispatch

Version 0.9 and above of
[databinder-dispatch](https://github.com/dispatch/reboot/) uses
[async-http-client](https://github.com/sonatype/async-http-client) as
its underlying engine.  The default client can be retrieved using
`Http.client` and passed to the soda-scala `HttpConsumer` or
`HttpProducer`.

## Future work

* A query-building system richer than raw SoQL strings.
* More ways to upload data (e.g., from a CSV file).


sofa
------------------------------------------------------------------------------------------------
**Title**: sofa

**Web address**: https://api.github.com/repos/tlevine/sofa

**Date**: January 2012

**Description**: # Sofa: Standalone CouchDB Blog

Sofa showcases the [potential of pure CouchDB applications](http://jchris.mfdz.com/code/2008/10/standalone_applications_with_co). It should provide an easy way for people to put thier thoughts online, anywhere there's a running Couch. It's just HTML, JavaScript and the magic of CouchDB.

Currently supports authoring by anyone with the proper roles, and comments from anyone with a user account.

## Current News

Things are moving crazy fast around here right now as I bring this stuff up to ship-shape for the [CouchDB book](http://books.couchdb.org). I'll be renaming methods and stuff (if I find the time), any API feedback will be appreciated.

## Install CouchDB

You'll also need CouchDB (verion 0.11 or newer). Once you have that installed and the tests passing, you can install CouchApp
and the blog software. 

## Install CouchApp

CouchApp makes it easy to edit application that are hosted in CouchDB, by keeping a correspondence between a set of files, and a CouchDB design document. You'll use CouchApp to install Sofa in your CouchDB instance.

    sudo easy_install couchapp

CouchApp is a set of utilities for developing standalone CouchDB applications You can [learn more about the CouchApp project here](http://github.com/couchapp/couchapp/). Also, [`easy_install` has an unpleasant bug on OSX](http://mail.python.org/pipermail/pythonmac-sig/2008-October/020567.html), so you might end up having to work from git source.


### Setup Admin Access

If you are going to put your blog in public, you'll want to [set up an Admin account (screencast)](http://www.youtube.com/watch?v=oHKvV3Nh-CI).


## Install Sofa

    git clone git://github.com/jchris/sofa.git
    cd sofa
    couchapp push . http://user:pass@127.0.0.1:5984/myblogdb 
  
You'll want to edit the HTML and CSS to personalize your site. Don't worry, the markup is pretty basic, so it's easy to rework. Adding new features is just a few lines of JavaScript away.

Anytime you make edits to the on-disk version of Sofa, and want to see them in your browser, just run `couchapp push . http://127.0.0.1:5984/blogdb` again. **You probably want to setup your `.couchapprc` file.** You should read the CouchApp readme to learn about that.

You can customize the blog title and other stuff in the `blog.json` file.

# Relax

[Visit your new blog.](http://127.0.0.1:5984/blogdb/_design/sofa/_list/index/recent-posts?descending=true&limit=5)

## License

Licensed under Apache 2.0: http://www.apache.org/licenses/LICENSE-2.0



spd-middle-names
------------------------------------------------------------------------------------------------
**Title**: spd-middle-names

**Web address**: https://api.github.com/repos/tlevine/spd-middle-names

**Date**: November 2012

**Description**: How many Seattle Police Department employees have middle names?
==

I took the SPD roster from [a FOI request](https://www.muckrock.com/foi/seattle-69/seattle-police-department-staff-roster-2103/). In 2009, 1770 of 1953 employees (91%) had middle names.

![Bar plot titled "1770 of 1953 SPD employees have middle names"](https://raw.github.com/tlevine/spd-middle-names/master/spd_middle_names.png)


speaker-on-off-switch
------------------------------------------------------------------------------------------------
**Title**: speaker-on-off-switch

**Web address**: https://api.github.com/repos/tlevine/speaker-on-off-switch

**Date**: December 2012

**Description**: 

spheremusic
------------------------------------------------------------------------------------------------
**Title**: spheremusic

**Web address**: https://api.github.com/repos/tlevine/spheremusic

**Date**: September 2013

**Description**: 

spirit-gif
------------------------------------------------------------------------------------------------
**Title**: spirit-gif

**Web address**: https://api.github.com/repos/tlevine/spirit-gif

**Date**: September 2013

**Description**: 

spot
------------------------------------------------------------------------------------------------
**Title**: spot

**Web address**: https://api.github.com/repos/tlevine/spot

**Date**: October 2012

**Description**: 
## spot(1)

Tiny ack-style file search utility.

### Features

* Short & written in Bash: you can edit it easily to suit your liking.
* Fast. Just `find` + `grep`.
* Searches most things by default instead of some known predefined extensions.
* Ignores .git, .svn, devices and binary files.

### Usage

#### Smart phrases

All arguments constitute the search text. No need to wrap most searches
in double quotes.

![](http://f.cl.ly/items/1Z063i0o3O2m0y2n0Q0d/Image%202012.03.04%2012:26:39%20PM.png)

#### Smart case

`spot` is case-insensitive by default. However, if your search term
contains an uppercase letter, it becomes sensitive!

![](http://f.cl.ly/items/2N332F0V302x1X47042c/Image%202012.03.04%2012:35:22%20PM.png)

#### Smart targets

If the first argument contains a slash _and_ is a valid directory, the
search is constrained to that particular target.

![](http://f.cl.ly/items/2u3x3T3j0B0q3s0T310t/Image%202012.03.04%2012:40:08%20PM.png)

#### Wildcard matching

In `spot(1)` searches, the `.` character acts as the RegExp wildcard,
making it very easy to perform searches that match anything, and to avoid
escaping characters or including ones that are not necessary for your
search.

#### Options

`spot -h` to see them.

### Installation

Run this command:

```
curl https://raw.github.com/guille/spot/master/spot.sh -o ~/bin/spot && chmod +x ~/bin/spot
```

If you don't have `~/bin`, replace it with another directory in your
`$PATH`, like `/usr/local/bin`.

### License

(The MIT License)

Copyright (c) 2011 Guillermo Rauch &lt;guillermo@learnboost.com&gt;

Permission is hereby granted, free of charge, to any person obtaining
a copy of this software and associated documentation files (the
'Software'), to deal in the Software without restriction, including
without limitation the rights to use, copy, modify, merge, publish,
distribute, sublicense, and/or sell copies of the Software, and to
permit persons to whom the Software is furnished to do so, subject to
the following conditions:

The above copyright notice and this permission notice shall be
included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED 'AS IS', WITHOUT WARRANTY OF ANY KIND,
EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.


sql-statistics
------------------------------------------------------------------------------------------------
**Title**: sql-statistics

**Web address**: https://api.github.com/repos/tlevine/sql-statistics

**Date**: July 2013

**Description**: These are some simple statistics implemented in SQL as understood by SQLite3.

They're written as examples on top of the [treasury.io](http://treasury.io) data.
Download the database like so.

    wget http://api.treasury.io/cc7znvq/47d80ae900e04f2/http/treasury_data.db

And run things like so.

    sqlite3 treasury_data.db < covariance.sql


statistics-doodles
------------------------------------------------------------------------------------------------
**Title**: statistics-doodles

**Web address**: https://api.github.com/repos/tlevine/statistics-doodles

**Date**: April 2013

**Description**: 

status.thomaslevine.com
------------------------------------------------------------------------------------------------
**Title**: status.thomaslevine.com

**Web address**: https://api.github.com/repos/tlevine/status.thomaslevine.com

**Date**: July 2012

**Description**: 

stenotype
------------------------------------------------------------------------------------------------
**Title**: stenotype

**Web address**: https://api.github.com/repos/tlevine/stenotype

**Date**: December 2012

**Description**: 

strataeu-2012
------------------------------------------------------------------------------------------------
**Title**: strataeu-2012

**Web address**: https://api.github.com/repos/tlevine/strataeu-2012

**Date**: October 2012

**Description**: Tom's keynote for [this](hhba.info/?p=3677)


sudoroom-til-statistics-doodles
------------------------------------------------------------------------------------------------
**Title**: sudoroom-til-statistics-doodles

**Web address**: https://api.github.com/repos/tlevine/sudoroom-til-statistics-doodles

**Date**: September 2013

**Description**: ---
title: Today I learned about statistics through doodles
---
Here are some videos from the
[time I doodled about statistics](https://sudoroom.org/wiki/Today_I_Learned#July_20:_Statistics_through_doodles:_Geometric_computations_of_fundamental_statistical_concepts)
in [Sudoroom](http://sudoroom.org/).

## What is a statistic?
A statistic is a number that describes a lot of other numbers.
By reducing many numbers into one number, we make it easier to
figure out what the numbers mean; we wouldn't be able to fit all
of the original in our brain.

## Geometric computations
In this session, I geometrically computed four statistics
about the relationships between different variables.

1. Covariance
2. Variance
3. Correlation
4. Least-squares regression coefficients

Each computation is for the simplest version of the statistic,
that is the univariate or bivariate version, and the population
statistic rather than the sample statistic.

## Why we care about these statistics
The videos show the computations of these statistics, but they
don't really explain why we use these statistics. So I'll explain
that here.

### Linear relationships
A lot of relationships can be seen as linear relationships.
One such relationship is that between a person's height and weight;
taller people are heavier, and shorter people are lighter.

A relationship that isn't very linear might be 
[public transit ridership and time](/!/ridership-rachenitsa).
As time progresses, weekly public transit ridership stays the same.
However, it does change a lot within the week, with high ridership
on the weekdays, low ridership on Saturdays and lower ridership on Sundays.

These four statistics are ways of describing the strength of a
relationship, and they only make sense to use with linear relationships.

### Covariance
Covariance is a basic measure of how strong the relationship.
Is just a number that is zero if there is no relationship, really
big if the two variables tend to move in the same direction,
and really negative if the two variables tend to move in opposite
directions.

For example, the covariance between weight and height is a very positive
number, like maybe 9001, because taller people are heavier.

The covariance between number of times a person has eaten popcorn and her
shoe size is probably around zero, because I doubt that these are very
strongly related.

And the covariance between cholesterol level and lifespan is probably
a very negative number, like maybe -1337, because people with more
cholesterol tend to live less long.

We tend not to use the covariance very directly in practice because
it's hard to compare covariances directly to each other. The reasons
are explained in the video.

### Variance
Variance is a measure of how spread out one variable is. It is a
positive number that gets big when the variables are more spread out.

Let's say that two people are cutting wood to build a house. They cut
10 pieces of wood each, and each piece of wood is supposed to be exactly
10 feet long.

One person is very careful when he measures the wood, so his pieces come
out perfectly. They're not exactly 10 feet long because that's impossible,
but they're not off by more than a hair. The variance of the lengths of
his pieces of wood is very close to zero, like maybe 3.

The other person is drunk and stoned and thus not very careful, so the
lengths of his pieces are all over the place. They're still around 10
feet long on average, but some of them are 8 feet long, and others are
11 feet long. The variance of the lengths of this person's pieces is
very high; maybe it's 200.

We talk about variances a lot when we are estimating the average of a
variable. When we estimate an average, we want to know how precise our
estimate is, and the variance tells us that.

### Correlation
Think of the correlation as a standardized version of the covariance.
The correlation is a number between -1 and 1. Like for the covariance,
positive correlations mean that the variables move together, and negative
correlations mean that the variables move oppositely.

Like the covariance, the correlation tells us how strongly two variables
are related. The practical difference is that we can compare different
covariances to each other.

We can compute the correlation between two variables based on the covariance
between the two variables and the respective variances of the two variables.

### Least-squares regression
Maybe you want to be able to predict someone's height based on her weight.
Regression is one way of doing this.

To predict height from weight, we can use a simple regression that would
tell us two statistics (numbers). In order to calculate these numbers, we
first need to measure the heights and weights of a bunch of people.

Once we calculate these two numbers, we have a formula for predicting height;
you give the formula a weight, and it will give you back a predicted height.

## Statistics and math and doodles
Statistics lets us distill our complex observations of the world into simple
numbers that are easier to understand. Covariance, variance, correlation
and least-squares regression are some statistics that are commonly used. The
text explains why we use them, and the video explains how we calculate them.

The formulae for these statistics get a bit confusing when you write them
out as symbols, but math can always be drawn, and it usually makes more sense
that way.


swb-website
------------------------------------------------------------------------------------------------
**Title**: swb-website

**Web address**: https://api.github.com/repos/tlevine/swb-website

**Date**: August 2013

**Description**: 

t
------------------------------------------------------------------------------------------------
**Title**: t

**Web address**: https://api.github.com/repos/tlevine/t

**Date**: January 2013

**Description**: t(1)                             User Commands                            t(1)



NAME
       t - language translation service from Google

SYNOPSIS
       t source target string

DESCRIPTION
       Bash wrapper to Google Translate website.

LANGUAGE CODES
       http://en.wikipedia.org/wiki/List_of_ISO_639-1_codes

EXAMPLES
       t en pt "The World is Yours"
       t en zh "What's the Matrix?"

AUTHOR
       Written by Rainer Borene.

REPORTING BUGS
       Please file bugs at https://github.com/rainerborene/t/issues/

COPYRIGHT
       Google Translate is a free statistical machine translation service pro-
       vided by Google Inc.

SEE ALSO
       wget(1), python(1), sed(1), awk(1)



Version 0.2                       April 2012                              t(1)


taco
------------------------------------------------------------------------------------------------
**Title**: taco

**Web address**: https://api.github.com/repos/tlevine/taco

**Date**: September 2013

**Description**: Taco caravan
======
Turn data into tacos.

* Factor: asada/cabesa/carnitas/...
* Logical: Hot sauce
* Logical: Salsa
* Logical: Sour cream
* Logical: Guacamole

Number of tacos and arrangement on the plate could mean something too.




While data gastronomy is a promising approach for big data analysis,
it hasn't caught on. This is partly because of the difficulty of scaling
a data gastronomification analysis. Varying the ingredients for
individual dishes involves quite a bit of specialized work, and this
needs to be streamlined before data gastronomification will be practical
for industry use. Taco caravan is my attempt at solving this.

Taco caravan is a library for distributed data gastronomification in R.
It leverages the commodity food production establishments to build
robust, automated, scalabale, realtime data gastronomification workflows.

## Taco trucks
Taco caravan uses 
taco trucks, a commodity food production service ubiqitious
in Oakland

menu




tcamp13-tweets
------------------------------------------------------------------------------------------------
**Title**: tcamp13-tweets

**Web address**: https://api.github.com/repos/tlevine/tcamp13-tweets

**Date**: May 2013

**Description**: 

tcamp2013
------------------------------------------------------------------------------------------------
**Title**: tcamp2013

**Web address**: https://api.github.com/repos/tlevine/tcamp2013

**Date**: May 2013

**Description**: 

terminator-solarized
------------------------------------------------------------------------------------------------
**Title**: terminator-solarized

**Web address**: https://api.github.com/repos/tlevine/terminator-solarized

**Date**: November 2012

**Description**: # Solarized terminator colors

A color theme for [terminator](http://www.tenshu.net/terminator/) using Ethan Schoonoveras [Solarized color scheme](http://ethanschoonover.com/solarized).

## Repositories
  * This theme as a single repository: [/ghuntley/terminator-solarized](https://github.com/ghuntley/terminator-solarized)
  * The main solarized repository: [/altercation/solarized](https://github.com/altercation/solarized)

## Files
  * `config` -  solarized theme for terminator (by [ghuntley](https://github.com/ghuntley))

## Usage
Install the terminiator configuration file:

    # mkdir -p ~/.config/terminator/
    # cp config ~/.config/terminator/

Modify the defaults stanza within the terminator configuration file to select your default(s)

    # vi ~/.config/terminator/config

To configure the default scheme used for new windows/tabs to solarized-light; change:

    [[default]]
    # solarized-light
    #palette = "#073642:#d30102:#859900:#b58900:#6c71c4:#d33682:#2aa198:#839496:#586e75:#cb4b16:#859900:#b58900:#268bd2:#d33682:#2aa198:#93a1a1"
    #background_color = "#eee8d5"
    #cursor_color = "#002b36"
    #foreground_color = "#002b36"

To:

    [[default]]
    # solarized-light
    palette = "#073642:#d30102:#859900:#b58900:#6c71c4:#d33682:#2aa198:#839496:#586e75:#cb4b16:#859900:#b58900:#268bd2:#d33682:#2aa198:#93a1a1"
    background_color = "#eee8d5"
    cursor_color = "#002b36"
    foreground_color = "#002b36"

To configure the default scheme used upon launch; change:

    [[default]]
      [[[child1]]]
        type = Terminal
        parent = window0
        profile = default

To: 

    [[default]]
      [[[child1]]]
        type = Terminal
        parent = window0
        profile = solarized-light

## Screenshots

![solarized terminator](https://github.com/ghuntley/terminator-solarized/raw/master/screenshots/terminator-solarized.png)



the-new-field
------------------------------------------------------------------------------------------------
**Title**: the-new-field

**Web address**: https://api.github.com/repos/tlevine/the-new-field

**Date**: June 2013

**Description**: The New Field
=============

A collaborative story attempt. 

Members:  

 * Richard Littauer
 * Simon Vansintjan
 * _Add your name (Keep this line)_
 
Our commits tell stories; the links we follow trace trails in the web. We are makers, and we are movers, and we are doers. We work best when we help each other. Why shouldn't we then tell stories with each other?

Some groundrules:

 * All groundrules have to be agreed upon by everyone.
 * All rules previously established stand for new members until they are taken down by all of the members.
 * You can change anything. Use humility to know when this doesn't apply. 
 * If you have an idea, work it in. 
 * No using the wiki or pages or private notes. Bring it all to the table, here. 
 * Work collaboratively, share, talk, invite, spread, disseminate, take ownership.
 * We are bound by nothing written down, but only by what we can build up. 
 * Have fun!
 
Let's see what we do. 


thomaslevine.com
------------------------------------------------------------------------------------------------
**Title**: thomaslevine.com

**Web address**: https://api.github.com/repos/tlevine/thomaslevine.com

**Date**: June 2012

**Description**: 

thomaslevine.com-nanoc
------------------------------------------------------------------------------------------------
**Title**: thomaslevine.com-nanoc

**Web address**: https://api.github.com/repos/tlevine/thomaslevine.com-nanoc

**Date**: December 2012

**Description**: I didn't like this name; use https://github.com/tlevine/www.thomaslevine.com


thomaslevine.com2
------------------------------------------------------------------------------------------------
**Title**: thomaslevine.com2

**Web address**: https://api.github.com/repos/tlevine/thomaslevine.com2

**Date**: May 2012

**Description**: Client-side markdown parsing
* http://code.google.com/p/pagedown/source/browse/
* http://code.google.com/p/pagedown/source/browse/Markdown.Converter.js


tic-tac-toe
------------------------------------------------------------------------------------------------
**Title**: tic-tac-toe

**Web address**: https://api.github.com/repos/tlevine/tic-tac-toe

**Date**: May 2013

**Description**: 

tidyness
------------------------------------------------------------------------------------------------
**Title**: tidyness

**Web address**: https://api.github.com/repos/tlevine/tidyness

**Date**: September 2013

**Description**: Here I model our determination of how tidy a dataset is.
My goal in this is to determine whether a dataset is tidy
so that someone can know to fix it if it's not.

Hadley Wickham (1) presents five ways in which a dataset
might be untidy.

1. column headers are values, not variable names
2. multiple variables are stored in one colum
3. variables are stored in both rows and columns
4. multiple types of observational unit are stored in the same table
5. a single observational unit is stored in multiple tables

I try to detect all of these, in order because that's easiest.

## References

1. http://vita.had.co.nz/papers/tidy-data.pdf
2. http://stat405.had.co.nz/lectures/18-tidy-data.pdf


toilet-foi
------------------------------------------------------------------------------------------------
**Title**: toilet-foi

**Web address**: https://api.github.com/repos/tlevine/toilet-foi

**Date**: March 2013

**Description**: Here are some ideas on fee waivers.
http://www.foiadvocates.com/fees.html

GSA runs all the buildings. Here are the FOIA contacts.
http://www.gsa.gov/portal/content/105305


toilet-posture-release
------------------------------------------------------------------------------------------------
**Title**: toilet-posture-release

**Web address**: https://api.github.com/repos/tlevine/toilet-posture-release

**Date**: February 2013

**Description**: Postures of toilet use
===

This is a portion of the secret repository that I use for analysis of
toilet stuff. In order to make sure that I don't bury private in a
public git repository, I reviewed this section and created a new
repository for it.

The only vaguely potentially identifying variables are the school and
country of birth variables. These are probably fine, actually, but they
might be identifying for the smaller schools and countries. So I've
removed these variables from the data.

## Loading the data
First, convert the spreadsheet from Qualtrics into a SQLite database.

    python2 munge/qualtrics_to_sqlite.py munge/raw.csv toilet.db

Next, add the `expected.posture` table. This says things like "Society expects
that men sit while defecating."

    sqlite3 toilet.db < expected.posture.sql

## Analyzing the data
Once the `toilet.db` is prepared, you can run the analysis scripts in the
`analyze` directory. They use [ProjectTemplate](http://projecttemplate.net).


toilet-posture-toilet-hackers-presentation
------------------------------------------------------------------------------------------------
**Title**: toilet-posture-toilet-hackers-presentation

**Web address**: https://api.github.com/repos/tlevine/toilet-posture-toilet-hackers-presentation

**Date**: December 2012

**Description**: 

tompkins-trust-exporter
------------------------------------------------------------------------------------------------
**Title**: tompkins-trust-exporter

**Web address**: https://api.github.com/repos/tlevine/tompkins-trust-exporter

**Date**: November 2012

**Description**: Tompkins Trust History Exporter
============
The  Tompkins Trust History Exporter downloads your Tompkins Trust history in
OFX format.

## Dependencies

This needs Python 2 and Firefox.

## Configure

Put your account credentials in `~/.tompkins-trust.json`; see
`.tompkins-trust.json.sample` in this repository for the format.

## Run

Start selenium

    java -jar selenium-server-standalone-2.25.0.jar

Then run the exporter like so

    ./export.py

You'll need to click the save box manualy.


trailer-tracker
------------------------------------------------------------------------------------------------
**Title**: trailer-tracker

**Web address**: https://api.github.com/repos/tlevine/trailer-tracker

**Date**: April 2013

**Description**: Trailer Tracker
=====
This website collects information about peoples' trailers.

The homepage has the map.

You click something to add to the map. Within this view, you first are asked
for your trailer's information and, optionally, for your email address and
phone number. After this view, more questions appear.


## Outline
All of the dynamic stuff happens on the server; we only use Javascript for
making the map. Unless otherwise specified, all pages return HTML.

    GET    /
    GET    /login
    POST   /login
    GET    /logout
    POST   /logout
    GET    /me                   ->  View information about your user account.
    POST   /me                   ->  Edit information about your user account.

    GET    /questionnaires       ->  View a JSON list of all questionnaire data.
    GET    /questionnaire/:uuid  ->  View an questionnaire.
    PUT    /questionnaire/:uuid  ->  Answer more questionnaire questions.
    POST   /questionnaire/:uuid  ->  Create an questionnaire.

    GET    /observations         ->  View a JSON list of all observation data.
    GET    /observation/:uuid    ->  View an observation.
    PUT    /observation/:uuid    ->  Edit an observation.
    POST   /observation/:uuid    ->  Create an observation.

Some of these respond differently depending on whether you're logged in.
Also, it's not entirely set.

## Notes
text for the start page?
Welcome to Trailer Tracker! 
This website provides consumer information relating 
to the indoor air quality issues of mobile or modular homes.

The baseline data comes from the federal government
 but we need users like you to add  your own knowledge 
 for the benefit of the community. 

     Then below it, and flanked by two sets of thin parallel lines was the following text in slightly larger font:

     To get started:
      click on aTRACKERa in the menu 
      at the top of the screen.

## Integration tests to be written

* If I submit valid information to the trailer form,
  * the page submits
  * I see whatever I expect
  * I can go to the page again and see the information.
* If I submit invalid information to the trailer form, I get an appropriate error.
* The homepage loads leaflet and seems to make a map. (The test probably won't actually run the JavaScript though.)
* If I POST a new trailer, it is reflected in the /trailers map data.
* If I submit a form but the session is expired or otherwise bad, I am allowed to log in or register and then see the data again.
    (This situation might not come up because of how little you actually need to be authenticate for.)

## Ideas from talking with Nick on March 27
* Use IP address
  * Hmm maybe cookies
    * Consider cookie policies
    * Display the policy only in European Union
  * He worries about the chemical industry or resellers trying to fuck with us.
  * At least log everything.
* "Track" a trailer
  * After tracking, send to symptoms so there is no interruption
  * or another trailer
* Use the wide format with dummy variables for checkboxes (multiple columns).
* Show _your_ trailer's story _after_ you submit information about the trailer.
  * Make them do all of the trailer questions
  * Make them do some more?
* Link the symptom and trailer questionnaires for one respondent.
* But getting other peoples' trailers is interesting.
  * People are sort of "vigilante".
* Multiple people in the family
* The other data is a baseline for
  * starting the stories
  * starting the map
* Map filtering
  * Movements
  * Sale date
  * Depend on how users use the site?
    * Beta test on mailing lists before writing press releases
  * Highlight different segments
    * Types of sales: auctions, resales, &c.
    * Time
    * On indian reservations, oil fields, &c.
    * Whether people's health diminished
    * A single trailer, staging area, &c.
  * Refund program (duplicated VIN in spreadsheet)
  * Press release specific to New Orleans

* About variatons symptoms
  * On one sheet of plywood, there can be a three-fold difference in emissions depending on where you test.
  * Variability within components
  * Nobody's done it by plant + model + time
  * Jacking up with cinderblocks
  * Personal data

This is more than FEMA trailers.
  * Mobile homes are the fastest-growing housing market in America.

Immediate gratification
  * Track your trailer
    * Mitigation strategies
  * Symptoms
    * Is my trailer a FEMA trailer?
    * Consumer products
  * Nick offered free testing for FEMA trailers, and this is how his name got around.

Tracking anything serialized
  * Weapons
    * Bullets
    * Guns
    * Black market

People are worried about adding VIN to a list, so explain that it's already on a list.


treegit
------------------------------------------------------------------------------------------------
**Title**: treegit

**Web address**: https://api.github.com/repos/tlevine/treegit

**Date**: November 2012

**Description**: treegit
==================
**treegit** is an off-the-self cgit server with some helpers. It is intended to
be run on Debian 5.0 on a cheap VPS dedicated to treegit, but it could work in
other ways with some adjustment.

## Install/Update
Commands in this "Install" section get run on a computer other than the server
we're configuring. It's fine to run these commands on an existing installation,
(Configuration files will simply be replaced.) so that's a reasonable way to
update treegit.

First, choose your domain name.

    export TREEGIT_DOMAIN_NAME=git.thomaslevine.com

You'll have to run those lines again if you open a new shell.

Set up SSH keys if you want.

    scp-copy-id root@$TREEGIT_DOMAIN_NAME

Install dependencies

    ssh root@$TREEGIT_DOMAIN_NAME 'apt-get update && apt-get upgrade && apt-get install apache2 markdown git'

Unpack the current directory into `/`.

    scp -r etc usr var root@$TREEGIT_DOMAIN_NAME:/

### Building from source
If you want to build from source, follow the directions
in the cgit readme. This will produce `cgit.cgi`, `cgit.css`
and `cgit.png`. Move these to the places that they are in
in the current repository, then unpack the repository to `/`.

## Configure
SSH to the server

    ssh root@$TREEGIT_DOMAIN_NAME 

The rest of this "Configure" section will run on the server, inside of that SSH
session. First, set username, domain name and email address you would like to
use. The user does not have to have been created.

    export TREEGIT_USERNAME=tlevine
    export TREEGIT_DOMAIN_NAME=git.thomaslevine.com
    export TREEGIT_EMAIL_ADDRESS=tom@example.com
    treegit-configure

Disable any Apache sites that are enabled.

    rm /etc/apache2/sites-enabled/*

Create the user

    useradd $TREEGIT_USERNAME
    mkdir -p /home/$TREEGIT_USERNAME/.ssh
    cp ~/.ssh/authorized_keys /home/$TREEGIT_USERNAME/.ssh # optional
    chown -R $TREEGIT_USERNAME: /home/$TREEGIT_USERNAME

Then enable the cgit site. You get to choose between an SSL version and a
non-SSL version.

### Without SSL
This is all you need to run.

    a2ensite cgit
    service apache2 reload

### With SSL
Enable SSL.

    apt-get install libapache2-mod-gnutls ssl-cert
    a2enmod ssl

Then enable cgit.

    a2ensite cgit-ssl

This site is set up to perform identification of both the server and the client
over SSL. It expects a self-signed certificate stored in `/etc/ssl/certs/treegit.crt`,
with its key file in `/etc/ssl/private/treegit.key`. Furthermore, it expects
that the same certificate be used to sign the client key, which must be imported
in the web browser. Here is how you can generate all these files.

    # Generate
    openssl genrsa -des3 -out treegit.key 4096

    # Request
    openssl req -new -key treegit.key -out treegit.csr

    # Remove passphrase
    cp treegit.key treegit.key.org
    openssl rsa -in treegit.key.org -out treegit.key

    # Sign
    openssl x509 -req -days 2000 -in treegit.csr -signkey treegit.key -out treegit.crt

    # Convert to pkcs12 for client authentication.
    openssl pkcs12 -export -in treegit.crt -inkey treegit.key -out treegit.p12 -name TreeGit

First, upload the files for server identification.

    # Upload
    scp treegit.key root@$TREEGIT_DOMAIN_NAME:/etc/ssl/private/treegit.key
    scp treegit.crt root@$TREEGIT_DOMAIN_NAME:/etc/ssl/certs/treegit.crt

Second, import the `treegit.crt` file in your browser to identify the server to
your browser.

Third, import the `treegit.p12` file in your browser to identify your browser
to the server.

Finally, reload the apache configuration.

    service apache2 reload

## More configuration

If you would like to configure cgit further, edit `/etc/cgitrc.local` per the
instructions in the [man page](http://hjemli.net/git/cgit/tree/cgitrc.5.txt).
Configurations here are included at the end of `/etc/cgitrc`, so they will
override any settings specified there. Also, this file will not be overwritten
if you copy the treegit `etc` directory (the one in this repository) because
it does not contain the file `etc/cgitrc.local`.

## Use
The system should be ready for use. You can log out of the SSH and follow the
directions on the about page.

    echo http://$TREEGIT_DOMAIN_NAME/?p=about

You can test that it's working by running the tests.

    export TREEGIT_USERNAME=tlevine
    export TREEGIT_DOMAIN_NAME=git.thomaslevine.com
    ./tests


## To do

* Set up hooks so that the cgitrc and description files can be versioned inside the
    repository and automatically placed in the git directory on the server.
* Switch Apache for Nginx
* Make it easier to back up configuration of different sites but still apply updates.


twitter_countdown
------------------------------------------------------------------------------------------------
**Title**: twitter_countdown

**Web address**: https://api.github.com/repos/tlevine/twitter_countdown

**Date**: August 2012

**Description**: 

united-states-middlenames
------------------------------------------------------------------------------------------------
**Title**: united-states-middlenames

**Web address**: https://api.github.com/repos/tlevine/united-states-middlenames

**Date**: June 2012

**Description**: United States Middle Names
=================

I want to know what proportion of people have middle names
and how this varies by time and location? In considering "time",
I want to consider both raw time and time within cycles.

### Raw data
Get Social Security Death Master File for free

* [Original](http://ssdmf.info/)
* [Torrent](http://thepiratebay.se/torrent/7193029/)

`unpack.sh` unzips them and puts them in the `deathfile` directory.

You can see the format in the `deathfile.head` directory;
this contains the first ten lines of each file.

### Architecture
Data are stored inside a MongoDB, in a collection called `middlenames`.
ZeroMQ distributes jobs across computers on a local network, and results
are sent back to the MongoDB.

This particular network is composed of cheap computers connected by
100Mbps ethernet. The computer running Mongo has relatively a lot of RAM.

### Import
`load.py` puts the data into a `deathfile` collection in MongoDB,
which is indexed on social security number (SSN).

It doesn't try to be fast because it's loading from the same three
files; once they're in the database, everything will be faster.

### Features
Some features for each dead person and added to the document in Mongo.

Records are distributed across many workers via ZeroMQ.
Results are sent back into the database in a new collection.

The collection could be named based on the statistical unit of the features--whether
they are at the person level, year level, &c.; you can also think of this
as the field on which they are uniquely indexed.

#### Personal
Person-level features are extracted and stored them in the `person`
collection, which is indexed on SSN.

One of these documents looks like this

    {
      "_id" : "001010001",
      "died_dow" : null,
      "surname" : "MUZZEY",
      "forename" : "GRACE",
      "died_doy" : null,
      "born" : {
        "month" : 4,
        "day" : 16,
        "year" : 1902
      },
      "born_dow" : "Wed",
      "died_month" : "Deathec",
      "born_doy" : ISODate("2000-04-16T00:00:00Z"),
      "died_day" : null,
      "born_month" : "Apr",
      "died_date" : ISODate("1975-12-15T00:00:00Z"),
      "state" : "NH",
      "born_year" : 1902,
      "parse_errors" : [ ],
      "died_year" : 1975,
      "ssn" : "001010001",
      "died" : {
        "month" : 12,
        "year": 1975
      },
      "middles_count" : 0,
      "funny_names" : true,
      "middles" : [ ],
      "born_date" : ISODate("1902-04-16T00:00:00Z"),
      "born_day" : 16
    }

### Simple transformations

#### Name
Surname-middle-forename pairs are collected indexed by each other, so there could be a
surname collection that has counts of middle and forenames by surname, a
middlenames collection that has counts of sur and forenames by middlenames field
and a forename collection that has counts of surnames by forename.

#### State
[Here](http://www.ssa.gov/employer/stateweb.htm) is how states of registration (normally birth) are determined.

### Resulting table
This all leads to a table like this.

    ssn,       born.date,  died.date,  born.dow, died.dow, born.doy,   died.doy,   state, forename, surname, middles, middles.count
    123456789, 1930-02-10, 1978-03-07, mon,      tue,      2000-02-10, 2000-03-07, KY,    Mohommad, Lee,     N,       1,

"dow" refers to "day of week". "doy" refers to "day of year", but
it's just the date switched to a stardard leap year like so (in R).

    doy <- as.POSIXct(paste('2000', substring('1999-02-28', 5), sep=''))

"middles" refers to middle initials, and "middles.count" is how many
there are.

#### Missing dates
If the year is missing and the month and day are available, the dates
will be NAs.

    ssn,       born.date, died.date, born.dow, died.dow, born.doy,   died.doy,   state, forename, surname, middles, middles.count
    123456789, NA,        NA,        mon,      tue,      2000-02-10, 2000-03-07, KY,    Mohommad, Lee,     N,       1,

If only the day of the month is missing, the date will use the 15th
day of the month. This is equivalent to the middle day of the month
rounded down, except for February, where it is the day after the middle
day, rounded down. Also, the days of the week will be missing.

    ssn,       born.date,  died.date,  born.dow, died.dow, born.doy,   died.doy,   state, forename, surname, middles, middles.count, funny.name
    123456789, 1930-02-15, 1978-03-15, NA,       NA,       2000-02-15, 2000-03-15, KY,    Mohommad, Lee,     N,       1,             FALSE

"funny.name" is whether the name contains characters other than `[A-Z ',.]`


unwatermark
------------------------------------------------------------------------------------------------
**Title**: unwatermark

**Web address**: https://api.github.com/repos/tlevine/unwatermark

**Date**: August 2012

**Description**: Unwatermark
======
**erase.py** is a removes Yahoo SiteBuilder's watermarks
from [Fade Lee's website](http://www.fadelee.com).

Specify the SITE_ROOT in erase.py as the place where you
have the website. Then run erase.py to remove watermarks.
While it's at it, it adds Piwik tracking (you'll need to
change the snippet for different sites) and commits to a
git repository.

If you use the included crontab, erase.py will runs every
two minutes, but not when sftp is running. I do forsee
problems if it happens to run when sftp is running, but
those should be fixed by uploading the site again.


urchin
------------------------------------------------------------------------------------------------
**Title**: urchin

**Web address**: https://api.github.com/repos/tlevine/urchin

**Date**: October 2013

**Description**:                        __    _     
      __  ____________/ /_  (_)___ 
     / / / / ___/ ___/ __ \/ / __ \
    / /_/ / /  / /__/ / / / / / / /
    \__,_/_/   \___/_/ /_/_/_/ /_/ 

Urchin is a test framework for shell. It is implemented in
portable /bin/sh and should work on GNU/Linux, Mac OS X, and
other Unix platforms.

## Try it out
Urchin's tests are written in Urchin, so you can run them to see what Urchin
is like. Clone the repository

    git clone git://github.com/scraperwiki/urchin.git

Run the tests

    cd urchin
    ./urchin tests

The above command will run the tests in your systems default
shell, /bin/sh (on recent Ubuntu this is dash, but it could be
ksh or bash on other systems); to test cross-shell compatibility,
run this:

    cd urchin
    ./cross-shell-tests

## Globally
Download Urchin like so (as root) (or use npm, below):

    cd /usr/local/bin
    wget https://raw.github.com/scraperwiki/urchin/master/urchin
    chmod +x urchin

Can be installed with npm too:

    npm install -g urchin

Now you can run it.

    urchin <test directory>

## Writing tests
Make a root directory for your tests. Inside it, put executable files that
exit `0` on success and something else on fail. Non-executable files and hidden
files (dotfiles) are ignored, so you can store fixtures right next to your
tests. Run urchin from inside the tests directory.

Urchin only cares about the exit status, so you can actually write your tests
in any language, not just shell.

## More about writing tests
Tests are organized recursively in directories, where the names of the files
and directories have special meanings.

    tests/
      setup
      setup_dir
      bar/
        setup
        test_that_something_works
        teardown
      baz/
        jack-in-the-box/
          setup
          test_that_something_works
          teardown
        cat-in-the-box/
          fixtures/
            thingy.pdf
          test_thingy
      teardown

Directories are processed in a depth-first order. When a particular directory
is processed, `setup_dir` is run before everything else in the directory, including
subdirectories. `teardown_dir` is run after everything else in the directory.

A directory's `setup` file, if it exists, is run right before each test file
within the particular directory, and the `teardown` file is run right after.

Files are only run if they are executable, and files beginning with `.` are
ignored. Thus, fixtures and libraries can be included sloppily within the test
directory tree. The test passes if the file exits 0; otherwise, it fails.

## Alternatives to Urchin
Alternatives to Urchin are discussed in
[this blog post](https://blog.scraperwiki.com/2012/12/how-to-test-shell-scripts/).


usace-regulatory
------------------------------------------------------------------------------------------------
**Title**: usace-regulatory

**Web address**: https://api.github.com/repos/tlevine/usace-regulatory

**Date**: July 2013

**Description**: Download and parse US Army Corps of Engineers regulatory
public notices. Websites are like this,

    http://www.%s.usace.army.mil/Missions/Regulatory/PublicNotices.aspx

where `%s` is a district slug.


user-hats
------------------------------------------------------------------------------------------------
**Title**: user-hats

**Web address**: https://api.github.com/repos/tlevine/user-hats

**Date**: April 2013

**Description**: User Hats
======
At this NYU GovLab thingy, we're going to tell people to put on hats for different roles
and to think about things from different perspectives.

We want to show change-agents how to use a design process to consider different
stakeholders' interests in reacting to an observation.

We have five hats. Tom, the facilitator,  will wear something else to indicate
that he's facilitating. Up to five people will be given roles corresponding to
the hats. Other people may watch  but may not talk.

People are given these roles.

* __, a plumber
* __, a mother
* __, a high school principal
* __, an owner of a construction company
* __, ??

They are given this prompt (tentative).

>

> Put yourself in the stakeholder's shoes. The stakeholder probably won't know
> the same things that you do, so he or she will react differently than you would.

We write down problems by group on cards.

People are asked to keep their comments short, and Tom will stop them if they take
too long.

After two groups, we shift from thinking about considerations for each group to
building a policy that considers these.

After four minutes, a new group comes in.





Ideas

* Keep it simple because it's early in the morning.
* The people like to feel like they're smart and to be told that.
* Questions
  * Lower property taxes on houses 3 br+
  * Infrastructure
  * Public-private board


user-management-spike
------------------------------------------------------------------------------------------------
**Title**: user-management-spike

**Web address**: https://api.github.com/repos/tlevine/user-management-spike

**Date**: May 2013

**Description**: DataPad user management
=====

DataPad is going to have users in groups
with credentials for third-party services.
We need some way of managing that. The
present repository presents a story about
what we'll need to represent. Then it
discusses how to implement it with a few
different services.

## Services
Here are the services I looked at. This is
a pretty broad range, and it includes things
that are totally off the mark; I included
these so that we can quickly remember later
not to use them.

* http://singly.com/
* http://janrain.com/products/
* http://www.gigya.com/user-management/registration-as-a-service/
* http://aws.amazon.com/iam/
* https://www.dailycred.com/
* https://stormpath.com/

## Story
Thomas and Tomasina work at a marketing firm that is consulting for a tank
engine company. They are collaborating on a data analysis about the market
for tank engines, and they are using DataPad. They want to figure out which
cities they should tarket their marketing in.

They pull data from a few sources.

1. Internal spreadsheets
2. Stock market, via Yahoo
3. Twitter
4. Large private datasets stored on S3
5. Google Analytics

The last three of these services require special credentials (API keys).

Now let's focus on user accounts. This is not the only project that Thomas
and Tomasina work on, they don't always collaborate, and it's nice to see
who has done what. Thus, they want their own accounts, each with access to
this project.

They have a company-wide Twitter API key. For S3, they prefer to use separate
accounts for each project for security reasons. They're using the tank engine
company's Google Analytics credentials.

Thomas or Tomasina will specify these credentials, and it's fine, and perhaps
desirable, if the other of them can see the credential once it has been set.

They want to be able to share a read-only version of the analysis quite
publically within the two companies. In order that this analysis be safe to
share publically, people viewing version of the analysis should not be able
to see the third-party credentials.

## Architecture ideas
It seems like following GitHub's model would work pretty well.

People log in with their personal user account. Each account can be associated
with any number of organizations. Projects belong to either a user or an
organization. Credentials belong to a user, organization or project.

Handling collaborators on a personal project is a bit inelegant to me. It also
makes things more complicated to use. Maybe multi-person projects automatically
morph into organizations....

I'd like to think more about this, but for now, I'm going to try implementing this
architecture with the aforementioned services.


verbosity
------------------------------------------------------------------------------------------------
**Title**: verbosity

**Web address**: https://api.github.com/repos/tlevine/verbosity

**Date**: April 2013

**Description**: Verbosity is a word.


vimrc
------------------------------------------------------------------------------------------------
**Title**: vimrc

**Web address**: https://api.github.com/repos/tlevine/vimrc

**Date**: March 2013

**Description**: # The Ultimate vimrc

Over the last 8 years I have used and tweaked Vim. This is my Ultimate vimrc.

There are two versions:

* **Basic**: If you want something small just copy [basic.vim](https://github.com/amix/vimrc/blob/master/vimrcs/basic.vim) into your ~/.vimrc and you will have a great basic setup
* **Awesome**: This includes a ton of useful plugins, color schemes and configurations

I would of course recommend using the awesome version.


## How to install the Basic version?
The basic version is basically just one file and no plugins. You can check out [basic.vim](https://github.com/amix/vimrc/blob/master/vimrcs/basic.vim).

This is useful to install on remote servers where you don't need many plugins and you don't do many edits.

	git clone git://github.com/amix/vimrc.git ~/.vim_runtime
	sh ~/.vim_runtime/install_basic_vimrc.sh


## How to install the Awesome version?
The awesome version includes a lot of great plugins, configurations and color schemes that make Vim a lot better. To install it simply do following:

	git clone git://github.com/amix/vimrc.git ~/.vim_runtime
	sh ~/.vim_runtime/install_awesome_vimrc.sh


## How to install on Windows?

Use [msysgit](http://msysgit.github.com/) to checkout the repository and run the installation instructions above. No special instructions needed ;-)


## How to update to latest version?

Simply just do a git rebase!

    cd ~/.vim_runtime
    git pull --rebase


## Some screenshots

Colors when editing a Python file:
![Screenshot 1](http://files1.wedoist.com/e952fdb343b1e617b90d256e474d0370/as/screenshot_1.png)

Opening recently opened files [mru.vim](https://github.com/vim-scripts/mru.vim):
![Screenshot 2](http://files1.wedoist.com/1967b0e48af40e513d1a464e08196990/as/screenshot_2.png)

[NERD Tree](https://github.com/scrooloose/nerdtree) plugin in a terminal window:
![Screenshot 3](http://files1.wedoist.com/b1509d7ed9e9f357e8d04797f9fad67b/as/screenshot3.png)

This vimrc even works on Windows!
![Screenshot 4](http://files1.wedoist.com/4e85163d97b81422240c822c82022f2f/as/screenshot_4.png)


## What plugins are included?

I recommend reading the docs of these plugins to understand them better. Each of them provide a much better Vim experience!

* [pathogen.vim](https://github.com/tpope/vim-pathogen): Manages the runtime path of the plugins
* [YankRing](https://github.com/vim-scripts/YankRing.vim): Maintains a history of previous yanks, changes and deletes
* [snipMate.vim](https://github.com/garbas/vim-snipmate): snipMate.vim aims to be a concise vim script that implements some of TextMate's snippets features in Vim
* [bufexplorer.zip](https://github.com/vim-scripts/bufexplorer.zip): Buffer Explorer / Browser. This plugin can be opened with `<leader+o>`
* [NERD Tree](https://github.com/scrooloose/nerdtree): A tree explorer plugin for vim
* [ack.vim](github.com/mileszs/ack.vim): Vim plugin for the Perl module / CLI script 'ack'
* [vim-powerline](https://github.com/Lokaltog/vim-powerline): The ultimate vim statusline utility
* [ctrlp.vim](https://github.com/kien/ctrlp.vim): Fuzzy file, buffer, mru and tag finder. In my config it's mapped to `<Ctrl+F>`, because `<Ctrl+P>` is used by YankRing
* [mru.vim](https://github.com/vim-scripts/mru.vim): Plugin to manage Most Recently Used (MRU) files. Includes my own fork which adds syntax highlighting to MRU. This plugin can be opened with `<leader+f>`
* [open_file_under_cursor.vim](https://github.com/amix/open_file_under_cursor.vim): Open file under cursor when pressing `gf`


## What color schemes are included?

* [peaksea](https://github.com/vim-scripts/peaksea): My favorite!
* [vim-colors-solarized](https://github.com/altercation/vim-colors-solarized)
* [vim-irblack](https://github.com/wgibbs/vim-irblack)
* [mayansmoke](https://github.com/vim-scripts/mayansmoke)
* [vim-pyte](https://github.com/therubymug/vim-pyte)


## What modes are included?

* [vim-coffee-script](https://github.com/kchmck/vim-coffee-script)
* [vim-less](https://github.com/groenewege/vim-less)
* [vim-bundle-mako](https://github.com/sophacles/vim-bundle-mako)
* [vim-markdown](https://github.com/tpope/vim-markdown)


## How to include your own stuff?

After you have installed the setup you can create **~/.vim_runtime/my_configs.vim** to fill in any configurations that are important for you. For instance, my **my_configs.vim** looks like this:

	~/.vim_runtime (master)> cat my_configs.vim
	map <leader>ct :cd ~/Desktop/Todoist/todoist<cr>
	map <leader>cw :cd ~/Desktop/Wedoist/wedoist<cr> 

You can also install your own plugins, for instance, via pathogen we can install [vim-rails](https://github.com/tpope/vim-rails):

	cd ~/.vim_runtime
	git clone git://github.com/tpope/vim-rails.git sources_plugins/vim-rails
	
Now you have vim-rails installed ;-)


## Key Mappings

### Plugin related mappings

Open [bufexplorer](https://github.com/vim-scripts/bufexplorer.zip) and see and manage the current buffers:
    
    map <leader>o :BufExplorer<cr>

Open [MRU.vim](https://github.com/vim-scripts/mru.vim) and see the recently open files:

    map <leader>f :MRU<CR>

Open [ctrlp.vim](https://github.com/kien/ctrlp.vim) plugin:
    
    let g:ctrlp_map = '<c-f>'

Open [PeepOpen](https://peepcode.com/products/peepopen) plugin:

    map <leader>j :PeepOpen<cr>

Managing the [NERD Tree](https://github.com/scrooloose/nerdtree) plugin:

    map <leader>nn :NERDTreeToggle<cr>
    map <leader>nb :NERDTreeFromBookmark 
    map <leader>nf :NERDTreeFind<cr>

### Normal mode mappings
Fast saving of a buffer:

	nmap <leader>w :w!<cr>

Treat long lines as break lines (useful when moving around in them):

	map j gj
	map k gk
	
Map `<Space>` to `/` (search) and `<Ctrl>+<Space>` to `?` (backwards search):
	
	map <space> /
	map <c-space> ?
	map <silent> <leader><cr> :noh<cr>

Disable highlight when `<leader><cr>` is pressed:
	
	map <silent> <leader><cr> :noh<cr>

Smart way to move between windows:
	
	map <C-j> <C-W>j
	map <C-k> <C-W>k
	map <C-h> <C-W>h
	map <C-l> <C-W>l

Closing of current buffer(s):
	
	" Close current buffer
	map <leader>bd :Bclose<cr>
	
	" Close all buffers
	map <leader>ba :1,1000 bd!<cr>
	
Useful mappings for managing tabs:
	
	map <leader>tn :tabnew<cr>
	map <leader>to :tabonly<cr>
	map <leader>tc :tabclose<cr>
	map <leader>tm :tabmove 
	
	" Opens a new tab with the current buffer's path
	" Super useful when editing files in the same directory
	map <leader>te :tabedit <c-r>=expand("%:p:h")<cr>/
	
Switch CWD to the directory of the open buffer:
	
	map <leader>cd :cd %:p:h<cr>:pwd<cr>
	
Open vimgrep and put the cursor in the right position:
	
	map <leader>g :vimgrep // **/*.<left><left><left><left><left><left><left>

Vimgreps in the current file:
	
	map <leader><space> :vimgrep // <C-R>%<C-A><right><right><right><right><right><right><right><right><right>

Remove the Windows ^M - when the encodings gets messed up:
	
	noremap <leader>m mmHmt:%s/<C-V><cr>//ge<cr>'tzt'm
	
Quickly open a buffer for scripbble:
	
	map <leader>q :e ~/buffer<cr>

Toggle paste mode on and off:
	
	map <leader>pp :setlocal paste!<cr>


### Insert mode mappings

Quickly insert parenthesis/brackets/etc.:

    inoremap $1 ()<esc>i
    inoremap $2 []<esc>i
    inoremap $3 {}<esc>i
    inoremap $4 {<esc>o}<esc>O
    inoremap $q ''<esc>i
    inoremap $e ""<esc>i
    inoremap $t <><esc>i

Insert the current date and time (useful for timestamps):

    iab xdate <c-r>=strftime("%d/%m/%y %H:%M:%S")<cr>


### Visual mode mappings

Visual mode pressing `*` or `#` searches for the current selection:

	vnoremap <silent> * :call VisualSelection('f')<CR>
	vnoremap <silent> # :call VisualSelection('b')<CR>

When you press gv you vimgrep after the selected text:

	vnoremap <silent> gv :call VisualSelection('gv')<CR>

When you press `<leader>r` you can search and replace the selected text:

	vnoremap <silent> <leader>r :call VisualSelection('replace')<CR>

Surround the visual selection in parenthesis/brackets/etc.:

    vnoremap $1 <esc>`>a)<esc>`<i(<esc>
    vnoremap $2 <esc>`>a]<esc>`<i[<esc>
    vnoremap $3 <esc>`>a}<esc>`<i{<esc>
    vnoremap $$ <esc>`>a"<esc>`<i"<esc>
    vnoremap $q <esc>`>a'<esc>`<i'<esc>
    vnoremap $e <esc>`>a"<esc>`<i"<esc>
	

### Command line mappings

$q is super useful when browsing on the command line. It deletes everything until the last slash:

    cno $q <C-\>eDeleteTillSlash()<cr>

Bash like keys for the command line:

    cnoremap <C-A>		<Home>
    cnoremap <C-E>		<End>
    cnoremap <C-K>		<C-U>

    cnoremap <C-P> <Up>
    cnoremap <C-N> <Down>


### Spell checking
Pressing `<leader>ss` will toggle and untoggle spell checking

    map <leader>ss :setlocal spell!<cr>

Shortcuts using `<leader>` instead of special chars

    map <leader>sn ]s
    map <leader>sp [s
    map <leader>sa zg
    map <leader>s? z=

### Cope	
Do :help cope if you are unsure what cope is. It's super useful!

When you search with vimgrep, display your results in cope by doing:
`<leader>cc`

To go to the next search result do:
`<leader>n`

To go to the previous search results do:
`<leader>p`

Vimscript mappings:

    map <leader>cc :botright cope<cr>
    map <leader>co ggVGy:tabnew<cr>:set syntax=qf<cr>pgg
    map <leader>n :cn<cr>
    map <leader>p :cp<cr>


## Useful blog tips regarding my Vim setup

* [Vim: Annotate strings with gettext (the macro way)](http://amix.dk/blog/post/19678#Vim-Annotate-strings-with-gettext-the-macro-way)
* [vimgrep: Searching through multiple file extensions](http://amix.dk/blog/post/19672#vimgrep-Searching-through-multiple-file-extensions)
* [Filtering through vimgrep results using regular expressions](http://amix.dk/blog/post/19666#Filtering-through-vimgrep-results-using-regular-expressions)
* [PeepOpen - File auto completion for Mac editors](http://amix.dk/blog/post/19601#PeepOpen-File-auto-completion-for-Mac-editors)
* [Vim 7.3: Persistent undo and encryption!](http://amix.dk/blog/post/19548#Vim-7-3-Persistent-undo-and-encryption)
* [Vim tips: Visual Search](http://amix.dk/blog/post/19334#Vim-tips-Visual-Search)
* [Folding in Vim](http://amix.dk/blog/post/19132#Folding-in-Vim)


wetlands
------------------------------------------------------------------------------------------------
**Title**: wetlands

**Web address**: https://api.github.com/repos/tlevine/wetlands

**Date**: January 2013

**Description**: Army Corps 404 Website Scraper
=======

Prospectus
-------------
There are many, many permits issued to fill and/or dredge wetlands in the New Orleans District, perhaps the most vulnerable district to wetland destruction in the United States.  And many project reviewers.  There may be 30 standard permits a week, and few advocates, whose time is overcommitted.  There are probably more general permits issuedathese are never publicly noticed.  It takes a watchdog a minimum of 4 hours to review the 404 website for possible targets for comment, and 8 hours average for each comment.  Each FOIA takes an hour minimum.  Phone calls with regulators take 15 minutes to half an hour, usually. 

Once permits "fall off the page," a FOIA must be submitted to obtain documents that were once public.  Final Decision documents almost always require a FOIA.  It would be more productive to have to mail one FOIA.

Automated scrape would preserve these documents, and allow yearly review of impacts allowed by permitting. It would free up advocate time to investigate general permits hidden from public view, as well as follow up on decision documents and investigate permit violationsaall of which are needed for effective watchdogging.    It would assist independent review of cumulative impacts, as required of the Army Corps by rule.

Additional services like OCR and text searches within the Public Notice could streamline permit review even more.  The Mississippi river Collaborative has recently proposed commenting on all permits 10 acres or more. In the New Orleans District, this will increase the work load substantially.  Any time saved in permit review, finding these 10 acre permits, is time better spent fighting bad permits, and analyzing the patterns of Corps regulatory to strategically fight wetlands destruction.  

Analysing and preventing wetlands destruction from the 404 process in the New Orleans District presents a challenge beyond that in most districts.  But if this program is successful, it can be applied to other Army Corps Districts around the nation.

The Army Corps, by federal rule, is required  to analyse permits for aCumulativea impact of permitting, by using a aWatershed Approach.a

Holy Moly, but do they avoid doing this.  They do know how, but need encouragement.  Automation would assist them as well. 

Such a recordkeeping / databasing/ spreadsheet-making scraper software would aid in our pressure for the Corps to regard cumulative impacts of Permits.   It would also assist independent scientific review of impacts and mitigation. 

Thanks for whatever assistance you can give to our efforts to protect the wetlands that protect us.  Thanks for helping to keep us afloat. 

For a Healthy Gulf,

Scott

Stuff
------
A more detailed prospectus is in the prospectus directory. The program is in
the directories v*; you should use the highest version.


wetlands-listings
------------------------------------------------------------------------------------------------
**Title**: wetlands-listings

**Web address**: https://api.github.com/repos/tlevine/wetlands-listings

**Date**: October 2012

**Description**: 

wetlands-music
------------------------------------------------------------------------------------------------
**Title**: wetlands-music

**Web address**: https://api.github.com/repos/tlevine/wetlands-music

**Date**: June 2013

**Description**: Data-driven music about wetlands
=======

Ideas

* Each measure is a week.
* Each measure has four beats of data-driven intensity.
  1. The first beat is the number of impact applications that week.
  2. The second beat is the number of mitigation applications that week.
  3. The third beat is the number of restoration applications that week.
  4. The fourth beat is the number of other applications that week.
* The melodies are smoothed series of the mean longitude, latitude and
    acreage for the week; the values are smoothed between weeks such that
    the mean value occurs between the second and third beats, and then
    they are discritized to make four notes per measure.
* I want some melody that is opposite the melodies I mention above. Maybe
    weather works.


wetlands-v2-web
------------------------------------------------------------------------------------------------
**Title**: wetlands-v2-web

**Web address**: https://api.github.com/repos/tlevine/wetlands-v2-web

**Date**: November 2012

**Description**: 

when
------------------------------------------------------------------------------------------------
**Title**: when

**Web address**: https://api.github.com/repos/tlevine/when

**Date**: September 2012

**Description**: 

when-runevents-happen
------------------------------------------------------------------------------------------------
**Title**: when-runevents-happen

**Web address**: https://api.github.com/repos/tlevine/when-runevents-happen

**Date**: June 2012

**Description**: 

whitbygroup
------------------------------------------------------------------------------------------------
**Title**: whitbygroup

**Web address**: https://api.github.com/repos/tlevine/whitbygroup

**Date**: July 2012

**Description**: 

white-house-pif
------------------------------------------------------------------------------------------------
**Title**: white-house-pif

**Web address**: https://api.github.com/repos/tlevine/white-house-pif

**Date**: March 2013

**Description**: I downloaded the program description pages, pulled the
descriptions out and put them all in one page.


wrist-cracking
------------------------------------------------------------------------------------------------
**Title**: wrist-cracking

**Web address**: https://api.github.com/repos/tlevine/wrist-cracking

**Date**: January 2013

**Description**: Wrist cracking
====
This explains technical stuff. Read the `paper.md` for the point of all this.

Before you do anything else,

    . activate

Collecting data

    ?

Annotating data

    annotate

A demo annotation

    annotate-test


wsync
------------------------------------------------------------------------------------------------
**Title**: wsync

**Web address**: https://api.github.com/repos/tlevine/wsync

**Date**: January 2013

**Description**: **wsync** downloads and versions a url. It downloads the file, saves it with a
timestamp and links a current version to the timestamped version. If a
timestamped version already exists, wsync checks for differences between the new
file and the old file and does nothing if the new file is the same as the
previous version.


www.thomaslevine.com
------------------------------------------------------------------------------------------------
**Title**: www.thomaslevine.com

**Web address**: https://api.github.com/repos/tlevine/www.thomaslevine.com

**Date**: October 2013

**Description**: www.thomaslevine.com
===

## Authoring posts

### Syntax
Posts are written as subdirectories of `content/!`, with the main file having a basename
of `index`.

I can write posts in any of the supported markup languages. I tend to use markup or HTML.
Regardless of the language, you can put a block of YAML parameters at the top. In order
to be listed in the `/!` section, it needs at least the parameters `title`, `created_at`,
and `kind`. (`kind` is always `article` for this sort of page.)

    ---
    title: Analyze all the datasets
    created_at: 2013-07-07
    kind: article
    ---

There are some others that you can add.

tags
: If you tag something as `socrata` and configure the tag, it will show up in the list at
`/socrata`.

`main_class`
: This is the body class. You probably don't want to use it inside an article; it's more
    for layouts..

`tweet_link`
: The "Discuss" link sends people here.

`description`
: This goes in the metadata in the page.

These will be used for metadata elsewhere:

* `twitter_title` (The `title` is used if this is not set.)
* `twitter_image` (The apple touch icon is used if this is not set.)
* `twitter_description` (The `description` is used if this is not set.)
* `facebook_title` (The `title` is used if this is not set.)
* `facebook_image` (The apple touch icon is used if this is not set.)
* `facebook_description` (The `description` is used if this is not set.)

The following paramaters don't do anything, but I use them to store related information.

`tweet_text`
: The text that I'm going to use for the tweet.

`facebook_text`
: The text that I'm going to use for the Facebook status update.

`facebook_link`
: The URL of the relevant Facebook status update.

So the result might look like this.

    ---
    title: Analyze all the datasets
    created_at: 2013-07-07
    tags: ['socrata']
    kind: article
    image: figure/datasets_size_over_time2.png
    tweet_text: This one time, I Socrata's 80,000 datasets and analyzed them all.
    twitter_description: This one time, I Socrata's 80,000 datasets and analyzed them all.
    facebook_text: Guess how many datasets are in all of the Socrata portals.
    facebook_title: And which ones are the most popular?
    facebook_description: This one time, I all of Socrata's datasets and analyzed them.
    ---

### Deploying
Compile it locally.

    nanoc

View it locally.

    nanoc view

Run the checks, mainly the link validity check

    nanoc check external_links

Deploy by pushing the `output` directory to github.

    cd output
    git commit . -m compile
    git push

## Website styles

### Cards
Each page is composed of one `big-card`, several `little-card` boxes and
possibly an `article` box.
A `big-card` is 336 pixels by 168 pixels, with 28-pixel padding and 14-pixel margin.
a `little-card` is 84 pixels square, with 14-pixel padding and 14-pixel margin.
A `big-card` takes a full line, and three `little-card` boxes fit on one line.
Each line is centered to the page.

A `big-card` can be either a `business-card` or a `title-card`.
A `little-card` can be either `link` or `link-group`.

A big card contains either a business card or a title.
On the index page (`/`), the big card is a business card.
On the index page for a particular blog section (`/!/`, `/scarsdale/`, &c.),
the big card is the title of the section.
On a blog post, the big card is the title of the blog post.

### Parameters
`$square` is the base square for the layout in pixels.

`$calling-card-long` and `$calling-card-short` are the long and short sides,
respectively of a `big-card` in pixels.

`$topic-card-width` is the width of a `little-card` in squares, including
its half-square padding.

The parameters could be refactors.

### Order
The first row of the page is the `#nav`. It is always the following three
`little-card` boxes, in this order

1. "~", a link to `/`
2. "!", a link to `/!/`
3. "?", a link to `/!/about/`

On the blog category pages, a `#categories` row is included. It contains up to
three `little-card` boxes, one for each of three blog categories. Each one is
colored according to the section's color.

The next row is always the `big-card` box. It is also the `header`

On blog posts, the `big-card` is followed by an `article`. After the article are
two `little-card` boxes, with an empty space in the middle. One links to the
previous post, and the other links to the next post. These two `little-card`
boxes are the `#pagination`.

On other pages, the `big-card` is followed by a bunch of `little-card` boxes
that link to different places. These are the `#links`.

### Sections
The main blog is at `/!/` and contains a box for each post. Special sections
are at `/scarsdale/` and maybe two other places. Each section page has its own
color, and all references to this section use this color.
The scarsdale section is maroon.
All other pages use the color pink.

## To do

* After I compile, I want to make sure links are correct.
* When I click on the first article in the `/!` section, it should be clear that the change has changed.
* Prune the random email addresses so that only awesome ones come up.

## Quick Start
1\. Make sure you have [Ruby](http://www.ruby-lang.org/en/downloads/) and [bundler](http://gembundler.com/) installed.

    gem install bundler

2\. Clone the git repository or [download the zipball](https://github.com/jingoro/nanoc-html5boilerplate/zipball/master) and extract it.

    git clone git@github.com:jingoro/nanoc-html5boilerplate.git mysite

3\. Run bundler command to install the required gems.

    cd mysite
    bundle

4\. (Optional) [Install image compression binaries](https://github.com/toy/image_optim#binaries-installation) if your project will contain images.

5\. Compile the site.

    nanoc compile

You can view the skeleton site by browsing the generated `output/index.html` file.

6\. Validate the output

    nanoc check --all

## Development

### Composing
`master` should always have a presentable website state, including `created_at`
dates. This way, the site can be recompiled automatically every day, and queued
posts can be added automatically. Developing on master is okay as long as
articles in progress are not given a `kind` `article` and a `created_at` date.

There are a few helpful scripts for reviewing the various queued posts.
Run `./non-articles.sh` to find files named `content/!/*/index.md`
that are not marked as kind `article`. These are typically drafts.
Run `./posts-by-date.sh` to list these posts by `created_at` date. This
is helpful for planning when queued posts will post themselves.

## Rendering
Compile the site, and recompile when files are changed.

    guard exec nanoc

Serve the compiled files.

    nanoc view

### Deploying
To push new version of the site, check out master, compile the site, make sure that
all is well, then commit and push the submodule `output` directory, which has only
a `gh-pages` branch.

    git checkout master
    nanoc
    cd output
    git commit . -m compile
    git push
    cd ..
    git commit . -m update\ submodule
    git push

### Upgrading
To upgrade to a newer version of the boilerplate, checkout `nanoc-h5bp`, then
pull the new version.

    git checkout nanoc-h5bp
    git pull git://github.com/jingoro/nanoc-html5boilerplate.git master

Then checkout `upgrade`, which is an old version of the site, and merge the changes.

    git checkout upgrade
    git merge nanoc-h5bp
    echo Deal with merge crap.
    bundle # figure out how to set the directory; it uses the system directory, so i wound up installing stuff manually with gem
    nanoc
    echo Compile the site, and make sure it works.
    git commit -a

Now, merge with `master` and deal with more merge crap.

    git checkout upgrade
    git merge nanoc-h5bp
    echo Deal with merge crap.
    bundle
    nanoc
    echo Compile the site, and make sure it works.
    git commit -a

Finally, you can merge into master.

    git checkout master
    git merge upgrade

## References
This is based on jingoro's nanoc-html5boilerplate

    git://github.com/jingoro/nanoc-html5boilerplate.git

**NOTE: The boilerplate project is no longer being maintained after June 1, 2013. I should consider taking the project over.

## Novel articles included in this website
The following articles are included in this website.

* About Thomas Levine
* Ampersands
* Attendees decide between statistics talks
* Arrays
* Bare domain redirect server
* BeagleBone Power Supply
* Tom Life Best Practices
* BetterCoach and the merits of the inferior schedule-finder
* Big Data Ergonomics
* Bits
* Bookmarks
* Inspiration for questions
* Burqua
* Scalable Big Data Cloud ROI Agile Fixie API with HTML9 Responsive Boilerstrap JS
* CentOS cluster slaves
* On balancing checkbooks
* Design for varied ability
* Christmas Gifts
* Citizen Science Resources
* Couch-style data versioning in sqlite
* Covariance drawings
* Curriculum vitae
* Data business models
* Meeting nice people at DataKind events
* Another definition of "big data"
* The mindset of data science
* A new email address
* Local ScraperWiki Library
* DumpTruck Version 0.0.3
* Earmuffs
* Emergency Exit Only
* English and American
* Everyday Living for Girls
* Do all "analysts" use Excel?
* Foo
* Films I like
* Five stages of PhD
* FMS Symphony
* gastronomify
* Gullible
* Words that are hard to spell
* Hacks Hackers Buenos Aires Media Party
* Higher-power distance measures
* Hip 'Data' Terms
* The Hovering Cycle
* htmltable2matrix
* imapfilter
* My personal computer infrastructure
* Jaywalking
* Nude Jello Wrestling
* Where do people position computer keyboards?
* Klout jump on May 25
* Materials for leaning Angular
* Learn Computers Through ______
* Arrays
* On listening
* My appointment and termination at my high school newspaper
* Measuring Skin Temperature
* Prevalence of Middle Names
* Microfinance Data Scraping
* Moments of a function
* Effect of the number pad on mousing location
* Movies I like
* News Hack Day SF
* Addiction Recovery Meetings
* New York Pizza
* Open by default
* Open data licensing
* Open data possibilities
* OpenPrism
* The Orpheus Myth
* How I parse PDF files
* pbdq
* How principle component analysis works
* Industrial Remote-Controlled Peanut Butterers Parts & Supplies and Bagels Company
* Pirate Trends
* Preserving whitespace in haml
* Random Person of the Week
* R graphics tricks that you probably shouldn't use
* Real World Algorithms
* Reciprocity
* The Ridership Rachenitsa
* Unsanitary conditions in dormitory bathrooms
* R spells for data wizards
* cRowdsouRcing
* Capital projects
* Assessed property values
* Where does the "market value" on your tax bill come from?
* Scarsdale High School bus schedule weirdness
* Scarsdale Village tax rates and inflation
* Changes in appropriations to Scarsdale Village funds
* Scarsdale data journalism
* scp wrapper
* Handling exceptions in scrapers
* Installing Selenium
* Multisensory data experiences
* setxkbmap aliases
* Shakespeare on the internet
* Tests for your shell
* Soap-dispenser placement
* Open Calendars
* What's in a count?
* What file formats are on the data portals?
* Progeny of Ten Socrata Datasets
* How to use Socrata's site metrics API
* Analyze all the datasets
* Who uses Socrata's analysis tools?
* Statistics through doodles
* The Street Sign Protocol
* What's in a table?
* Teaching Data Science
* Tea Party
* Searching lots of inconveniently formatted files at once
* Terminal history
* Things to think about when you're building products on open data
* Things to read
* tmux aliases
* treasury.io
* Why does turning into a zombie seem unpleasant?
* Twinkle
* Twitter Scraper Library 
* Why don't people use this reading room?
* URL Encoding and Decoding
* Utlity of Cobalt
* My visit to Socrata, and data analysis about data analysis
* Things I want
* What I think people who want to learn programming
* World -> Data -> World
* Datestamped archives of a webpage
www.thomaslevine.com-compiled
------------------------------------------------------------------------------------------------
**Title**: www.thomaslevine.com-compiled

**Web address**: https://api.github.com/repos/tlevine/www.thomaslevine.com-compiled

**Date**: October 2013

**Description**: 

zipfianacademy.com
------------------------------------------------------------------------------------------------
**Title**: zipfianacademy.com

**Web address**: https://api.github.com/repos/tlevine/zipfianacademy.com

**Date**: July 2013

**Description**: 

zoetrope
------------------------------------------------------------------------------------------------
**Title**: zoetrope

**Web address**: https://api.github.com/repos/tlevine/zoetrope

**Date**: April 2013

**Description**: Zoetrope
=====
Produce video from a bunch of frames. It takes all of the png files from the
current directory.

    zoatrope [-bpm [bpm]] [-audio [audio file]] [out file]

For example, with bpm

    zoatrope -bpm 140 fms-symphony.avi

or with an audio file

    zoatrope -audio fms-symphony.mp3 fms-symphony.avi

If you pass an audio file, the frame rate is chosen such that the frames span the
audio file.

## Bit rate calculation
From [here](http://www.videohelp.com/calc.htm)

> The basic bitrate formula is
> 
> (Size - (Audio x Length )) / Length = Video bitrate
> L = Lenght of the whole movie in seconds
> S = Size you like to use in KB (note 700 MB x 1024Adeg = 716 800 KB) 
> A = Audio bitrate in KB/s (note 224 kbit/s = 224 / 8Adeg = 28 KB/s)
> V = Video bitrate in KB/s, to get kbit/s multiply with 8Adeg.
> 
> Adeg8 bit = 1 byte.
> Adeg1024 = 1 kilo in the computer world.
> 
> Example 
> 90 minutes video, L = 90 x 60 = 5 400 seconds
> 700 MB CD but be sure that if fits use a bit lower like 695 MB, S = 695 x 1024 = 711 680 KB
> Audio bitrate, A = 224 kbit/s / 8 = 28 KB/s
> 
> (711 680 - (5400 x 28) ) / 5400 = 104 KB/s x 8Adeg = 830 kbit/s. 


chorus
------------------------------------------------------------------------------------------------
**Title**: chorus

**Web address**: https://api.github.com/repos/csv/chorus

**Date**: March 2013

**Description**: # Chorus

Chorus is turns your mac into a chorus of [throat singers](http://www.youtube.com/watch?v=zwi6gF-0mP4) using Ruby threads and the mac `say` command.

## Installation

    $ gem install chorus

## Usage

	$ chorus "I just met you. This is crazy. Here's my number. Call me maybe?"
	$ chorus -h
		Usage: chorus [options] "your lyrics go here"
		    -v Voices,You,Want,To,Sing,      A comma-separated list of voices to use. Try multiple Voices with Fred*5,Kathy*2
		        --voices
		    -d, --delay SECONDSOFDELAY       The number of seconds in delay
		        --version                    Show version
		    -h, --help                       Show this message

	$ chorus -d .0125 -v Kathy,Princess "Hey I just met you, This is crazy. Here's my number. Call me maybe?"

	# Or Go Crazy....

	$ chorus -v Fred*25 -d .25 "King Tubby On The Sound System"

## Future Features

* Passing say commands in.
* Linux support
* Be Pipe Friendly

## Contributing

1. Fork it
2. Create your feature branch (`git checkout -b my-new-feature`)
3. Commit your changes (`git commit -am 'Add some feature'`)
4. Push to the branch (`git push origin my-new-feature`)
5. Create new Pull Request


codame-dataviz
------------------------------------------------------------------------------------------------
**Title**: codame-dataviz

**Web address**: https://api.github.com/repos/csv/codame-dataviz

**Date**: October 2013

**Description**: ## Plans

Already ready

1. Stop-question-frisk webpage
2. Gifts my true love gave to me on the 12 days of Christmas
3. FMS Symphony
4. Ridership rachenitsa

To do

1. Data-driven dance (below)
2. CSV shirts, with `,`, `\r`, and `\n` (Let's make a store, too!)
3. Big light-up comma
4. More songs (below)
5. Collecting bike accident data for an SF-related dataset. Almost done geocoding. Presentation could be similar to sqf-symphony or separate. [Info on what variables we have](https://www.baycitizen.org/data/bikes/bike-accident-tracker/). 

## Data-driven dance

In the part of `fms` when the debt ceiling is about to rise and the drums drop out, have everyone in the crowd yell 
"RAISE THE ROOF" and do this move:

![raise-the-roof](http://gifrific.com/wp-content/uploads/2013/05/Michael-Scott-and-Dwight-Schrute-Raise-the-Roof.gif)

## General inspirations

### Bring things in parts at a time
In the music and video, we can bring in things parts at a time.
A cool way of doing this is to use datasets that only start at
a particular time, but we could also do this by arbitrarily cutting
things off. Or we could cut things off at meaningful places.

### Live data feed
Something in the music and video changes based on a Wiimote or
whatever. We can hand that out to the audience

## Song ideas

### Ambient
Fit a model to some sparse and small dataset. Map the model
predictions to notes.

For example, we predict average daily bicycle ridership on
New York bridges from monthly data. We use some sort of nice
smoothing function so we get clean interpolations. Kernel
density should be fine.

Then we map predictions to frequencies. In the above example,
we only have one instrument, but we can have more instruments
for different dependent variables.

### Open data portal usage analytics
These data are a mess and are very random, so I don't think they'll be that interesting.
http://thomaslevine.com/!/socrata-metrics-api/

## Datasets

* http://thomaslevine.com/!/socrata-metrics-api/
* Nursing homes
* American Community Survey
* https://data.energystar.gov/

Silver

* http://datahub.io/dataset/goldsilverminesenergyuseghgemissions
* https://finances.worldbank.org/Procurement/Major-Contract-Awards/kdui-wcs3
* https://data.consumerfinance.gov/Government/Survey-of-Credit-Card-Plans/gme7-gkkr?
* http://www.usmint.gov/about_the_mint/index.cfm?action=PreciousMetals&type=bullion
* stock market or silver prices from yahoo
* gdp, smoothed

Towing

* https://data.cityofchicago.org/Transportation/Towed-Vehicles/ygr5-vcbg?
* https://data.baltimorecity.gov/Transportation/DOT-Towing/k78j-azhn?


conferences
------------------------------------------------------------------------------------------------
**Title**: conferences

**Web address**: https://api.github.com/repos/csv/conferences

**Date**: April 2013

**Description**: 

data-music-brainstorm
------------------------------------------------------------------------------------------------
**Title**: data-music-brainstorm

**Web address**: https://api.github.com/repos/csv/data-music-brainstorm

**Date**: August 2013

**Description**: Emails with ideas of data music stuff


ddpy
------------------------------------------------------------------------------------------------
**Title**: ddpy

**Web address**: https://api.github.com/repos/csv/ddpy

**Date**: October 2013

**Description**: 

ddr
------------------------------------------------------------------------------------------------
**Title**: ddr

**Web address**: https://api.github.com/repos/csv/ddr

**Date**: June 2013

**Description**: ```
#===========================#
         __     __        
     .--|  |.--|  |.----. 
     |  _  ||  _  ||   _| 
     |_____||_____||__| 

  data-driven rhythms in R
#===========================#
```

## Installation
`ddr` has ~ 1 GB of built-in instruments. It will take awhile to install from github.

```
library("devtools")
install_github("ddr", "csv")
library("ddr")
```

Computers without shit tons of RAM may crash if you try installing that way.
If that happens, try this, which uses less RAM.

```
git clone git://github.com/csv/ddr.git
cd ddr
Rscript -e 'library(devtools);install()'
```

You'll probably also want `seewave`, which indirectly depends on `BWidget`.

```
sudo pacman -S bwidge # In Arch Linux
Rscript -e 'install.packages("seewave")'
```

## Getting started
_NOTE: YOU MUST RUN `ddr_init` EVERYTIME YOU START UP `ddr`!_
<br/>
The way `ddr` makes noises is by creating temporary wave files and playing them through an audio player of your preference.  You can set your desired audio player as follows:
```
ddr_init(player="path_to_player")
```
By default, `ddr` is set to look for `QuickTime` on Mac OSX, eg:
```
ddr_init(player="/Applications/'QuickTime Player.app'/Contents/MacOS/'QuickTime Player'")
```
However, you might want to use `mplayer`, eg:
```
ddr_init(player="/usr/bin/env mplayer'")
```
One weird thing about using `QuickTime` is that, when it plays a temporary file, it will freeze the R console until you press `esc`. So, if you see this:
```
> play(piano$C3)

```
just press `esc` and `ddr` will move on to the next task.


## Built-in sounds

`ddr` comes with 5 instruments and 2 drum kits:

```
# Instruments:
blip -- a sinewave with a quick attack
piano -- a classic-sounding grand piano
rhodes -- a fender rhodes
sinewave -- a simple sinewave
sweeplow -- a cheesy synth

# Drums:
moog -- moog drum hits
roland -- a roland 707
```
Instruments and drum kits are simply R lists with each element being a separate wave file. For instance, you can select middle C on a piano as follows:
```
piano$C3
# or, alternatively:
piano[["C3"]]
```
If you want to see the names of the wave file in a given instrument, just type: `names(instrument)`, eg: `names(piano)` or `names(moog)`

## Sound manipulation

Slice 'em up!
```
chop(piano$C3, bpm=100, count=1/8)
```
Reverse too!
```
reverse(piano$C3)
```
Chop and screw!
```
chop(pitch(piano$C3, -36), bpm=100, count=2)
```
Loop!
```
loop(chop(piano$C3, bpm=100, count=1/8), 16)
```
Generate chords!
```
chord(C3, piano, "maj", bpm=100, count=4)
```

## Sound sequencing
`ddr` comes with a simple sound sequencing engine. This is best explained through an example:

```
# let's make a four-on-the-floor drum loop!

# first, let's put our drum sounds in a list:
wavs <- list(roland$HHO, roland$SD1, roland$BD1)

# now let's write a series of 1's and 0's indicating when we want each sound to play
# when we're done, let's also put these in a list:
hihat <- c(0,1,0,1)
kick <- c(1,0,1,0)
snare <- c(0,0,1,0)
seqs <- list(hihat, snare, kick)

# now lets put these lists into our sequence function and include a bpm and the count each note recieves
four_on_the_floor <- sequence(wavs, seqs, bpm=120, count=1/8)
play(loop(four_on_the_floor, 10))
```
Now lets take this logic and include chords to generate the chorus of _Call Me Maybe_

```
c1 <- chord(A4, sweeplow, "maj", bpm=119, count=1)
c2 <- chord(E4, sweeplow, "maj", bpm=119, count=1)
c3 <- chord(B4, sweeplow, "maj", bpm=119, count=1)
c4 <- chord(C.4, sweeplow, "min", bpm=119, count=1)
wavs <- list(c1, c2, c3, c4, roland$HHC, roland$TAM, roland$HHO, roland$BD1, roland$SD1)

A <- c(1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0)
E <- c(0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0)
B <- c(0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0)
C.m<-c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
H <- c(0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,1,1)
T <- c(0,0,1,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0)
O <- c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1)
K <- c(1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0)
S <- c(0,0,1,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,1,0)
seqs <- list(A, E, B, C.m, H, T, O, K, S)

callmemaybe <- sequence(wavs, seqs, bpm=59.5, count=1/16)
play(loop(callmemaybe, 4))
```
But wait, there's more! `ddr` can also generate sequences that include amplitude changes. Here, any number between 0 and 1 simply corresponds with the relative amplitude of the wave:
```
wavs <- list(roland$HHC)
seqs <- list(c(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8))

hihats <- sequence(wavs, seqs, bpm=59.5, count=1/16)
play(loop(hihats, 4))
```
BUT NOW IM REALLY GOING TO BLOW YOUR MIND. Since sequences are (mostly) binomial distributions, you can use built-in R functions to generate random music!
```
wavs <- list(roland$HHC, roland$TAM, roland$HHO, roland$BD1, roland$SD1)

H <- rnorm(32, mean=0.5, sd=0.1)
T <- rbinom(32, 1, prob=0.05)
O <- rbinom(32, 1, prob=0.075)
K <- rbinom(32, 1, prob=0.2)
S <- rbinom(32, 1, prob=0.3)
seqs <- list(H, T, O, K, S)

random_loop <- sequence(wavs, seqs, bpm=59.5, count=1/16)
play(loop(random_loop, 4))
```

## Data Sonification
Finally, `ddr` has a function for creating silly data sonifications. It's called `arpeggidata`. `arpeggidata` works by scaling a numeric vector onto a musical scale.  YOU HAVE TO HEAR IT TO BELIEVE IT!

```
# Let's use ChickWeight - Iris is so played out...
data('ChickWeight')
cw <- ChickWeight

chicks <- arpeggidata(sqrt(cw$weight),
                      blip,
                      scale="Emajor",
                      bpm=200,
                      count=1/32)
play(chicks)
```

### [FMS Symphony](http://fms.csvsoundsystem.com)
I used `ddr` to make the music in [FMS Symphony](http://fms.csvsoundsystem.com).  Here's the code (fms_data is built-in to `ddr`):

```
bpm <- 280
ct <- 1/4

rate <- arpeggidata(fms_data$rate,
            sinewave,
            low_note="",
            high_note="",
            descending = FALSE,
            scale="Cmajor",
            remove=NULL,
            bpm=bpm,
            count=ct)
writeWave(rate, "rate.wav")

ceil <- arpeggidata(fms_data$dist_to_ceiling,
            sinewave,
            low_note="",
            high_note="",
            descending = TRUE,
            scale="Emajor",
            remove=NULL,
            bpm=bpm,
            count=ct)
writeWave(ceil, "ceiling.wav")

gen_chords <- function(z) {
    if (z < 0) {
        if (z <= -0.5) {
            c <- chord(A3, sinewave,
                       "min", bpm=bpm,
                       count=ct)
        } else {
            c <- chord(A4, sinewave,
                       "min", bpm=bpm,
                       count=ct)
        }
    } else {
        if (z >= 0.5) {
            c <- chord(C4, sinewave,
                       "maj", bpm=bpm,
                       count=ct)
        } else {
            c <- chord(C3, sinewave,
                       "maj", bpm=bpm,
                       count=ct)
        }
    }
    return(c)
}

chords <- llply(fms_data$z_change, gen_chords, .progress="text")
bind_list_of_waves <- function(x, y) {
    bind(x, y)
}

reduce_waves <- function(list_of_waves) {
    Reduce(bind_list_of_waves, list_of_waves)
}
chords <- reduce_waves(chords)
writeWave(chords, "chords.wav")
```


ddr_nyhackr
------------------------------------------------------------------------------------------------
**Title**: ddr_nyhackr

**Web address**: https://api.github.com/repos/csv/ddr_nyhackr

**Date**: April 2013

**Description**: 

dds
------------------------------------------------------------------------------------------------
**Title**: dds

**Web address**: https://api.github.com/repos/csv/dds

**Date**: September 2013

**Description**: 

dumptruck-web
------------------------------------------------------------------------------------------------
**Title**: dumptruck-web

**Web address**: https://api.github.com/repos/csv/dumptruck-web

**Date**: May 2013

**Description**: Foo bar
==============

## Running on Nginx

### CGI
Here's an Nginx FastCGI configuration for Ubuntu based on the
[Arch Linux Wiki](https://wiki.archlinux.org/index.php/Nginx#FastCGI).

Install.

    apt-get install fcgiwrap nginx

Configure the nginx site. (Try `/etc/nginx/sites-enabled/default`.)

    location / {                                               
      fastcgi_param DOCUMENT_ROOT /var/www/dumptruck-web/;
      fastcgi_param SCRIPT_NAME dumptruck_web.py;
      fastcgi_param SCRIPT_FILENAME /var/www/dumptruck-web/dumptruck_web.py;
      fastcgi_pass unix:/var/run/fcgiwrap.socket;  
      
      # Fill in the gaps. This does not overwrite previous settings,
      # so it goes last
      include /etc/nginx/fastcgi_params;
     }

This depends on `/var/www/dumptruck_web.py` being a cgi script file that www-data
can execute.
 
If you're installing this as part of cobalt, the configuration is 

    rewrite  ^\/([^\s/]+)\/sqlite\/?$  /$1/sqlite?box=$1;

    location ~ ^\/([^\s/]+)\/sqlite\/?$ {
        fastcgi_param DOCUMENT_ROOT /var/www/dumptruck-web/;
        fastcgi_param SCRIPT_NAME dumptruck_web.py;
        fastcgi_param SCRIPT_FILENAME /var/www/dumptruck-web/dumptruck_web.py;

        # Fill in the gaps. This does not overwrite previous settings,
        # so it goes last
        include /etc/nginx/fastcgi_params;
        fastcgi_pass unix:/var/run/fcgiwrap.socket;
    }


Specify some high number of processes in `/etc/init.d/fcgiwrap` like so.

    FCGI_CHILDREN="9001"

You could also try something less extreme.

Then (re)start the daemons.

    service fcgiwrap restart
    service nginx restart

If this doesn't work, read `/etc/init.d/fcgiwrap`.

An example (simple) script would be

    #!/usr/bin/env python
    
    print '''HTTP/1.1 200
    Content-Type: text/plain
    
    Hello world
    '''

An API call looks like this,

    /jack-in-the/sqlite?q=SELECT+foo+FROM+baz

but the CGI script expects this,

    /sqlite?q=SELECT+foo+FROM+baz&box=made-of-ticky-tacky

so the Nginx needs to rewrite the URL.

### uWSGI
Here's a configuration based on the
[uWSGI quickstart](http://projects.unbit.it/uwsgi/wiki/Quickstart) 

Also see [uWSGI on nginx page](http://projects.unbit.it/uwsgi/wiki/RunOnNginx),
for reference but note that this explains a configuration that may be
unnecessarily complicated.

Install these.

    apt-get install uwsgi nginx uwsgi-plugin-{http,python}  

Run this (preferably as a daemon).

    uwsgi \
      --plugins http,python \
      --wsgi-file foobar.py \
      --socket 127.0.0.1:3031 \
      --callable application \
      --processes 20

Use some high number of processes because they block.

We'll have to adjust the api script so that it works with uWSGI;
once we do, add this to the nginx site. (Try `/etc/nginx/sites-enabled/default`.)

    location /path/to/sqlite {
        include uwsgi_params;
        uwsgi_pass 127.0.0.1:3031;
    }

Restart nginx.

    service nginx restart

Test

    curl localhost/path/to/sqlite?q=SELECT+42+FROM+sqlite_master&boxname=jack-in-the

An example (simple) script would be

    def application(env, start_response):
        start_response('200 OK', [('Content-Type','text/html')])
        return "Hello World"

## Add later
Gzip responses.

## SQLite errors
The SQLite errors are normally pretty good, so an api call with that raises a
SQLite error normally displays the error messages. This includes

* Locked databases
* Ungrammatical SQL

Some of these errors aren't great, like

* Database that the user doesn't have permission to read

We also treat some things as errors that SQLite doesn't:

* Databases that don't exist


events
------------------------------------------------------------------------------------------------
**Title**: events

**Web address**: https://api.github.com/repos/csv/events

**Date**: June 2013

**Description**: 

fitbite
------------------------------------------------------------------------------------------------
**Title**: fitbite

**Web address**: https://api.github.com/repos/csv/fitbite

**Date**: July 2013

**Description**: ```
           .-""""-.
          /' .  '. \
         (`-..:...-')
          ; FITBITE ;
           '-------'
           ___/(  /(
          /--/ \\//
      __ )/ /\/ \/
     `-.\  //\\
        \\//  \\
         \/    \\
                \\
                '--`
```
fitbite
-----------------------------------------------------------------------------------------
_Order one of your favorite meals from Seamless based on your Fitbit activity for the day._
## Dependencies
First, you'll need to set the following 6 Environmental Variables.  The easy way is to just add these lines to `
~/.bash_profile` or `~/.zshrc`, depending on which shell you use.

```
export SEAMLESS_EMAIL='name@email.com'
export SEAMLESS_PASSWORD='mehungry'
export FITBIT_CONSUMER_KEY='wwwwwwwwwwwwwwwwwwwwww'
export FITBIT_CONSUMER_SECRET='xxxxxxxxxxxxxxxxxxxx'
export FITBIT_USER_KEY='yyyyyyyyyyyyyyyyyy'
export FITBIT_USER_SECRET='zzzzzzzzzzzzzzzzz'
```

Then you'll need to install [`python-fitbit`](https://github.com/orcasgit/python-fitbit) like so:

```
git clone https://github.com/orcasgit/python-fitbit.git
cd python-fitbit
sudo python setup.py install
```

You'll also need to make sure you have `selenium` for python installed:

```
sudo pip install selenium
```
And have downloaded Firefox from [here]((http://www.mozilla.org/en-US/firefox/new/)

## Assumptions
1.  Your CC info is saved on Seamless
2.  You're at your home (default) address on Seamless.

## Setup
`fitbite` works by querying your fitbit activity for the today, comparing it to set of arbitrary tags and step levels in `fitbite.yaml` and then ordering one of your favorite meals from Seamless which have been named with the same tags.

See an example list of my favorite meals:
![fav_meals](imgs/fav_meals.png)

The names of each meal match the tagged calorie levels set in [`fitbite.yaml`](fitbite.yaml). These can be as simple or specific as you'd like, just so long as they are ordered from lowest caloric content to highest caloric content and match the tags you set on Seamless:
```
levels:
  - LOW_CALORIES: 5000 # Calorie Level, Step Level
  - MEDIUM_CALORIES: 10000 # Calorie Level, Step Level
  - HIGH_CALORIES: 15000 # Calorie Level, Step Level
```

## Runtime
When you step into your door at night and you're hungry, just type `python fitbite.py`. That's it. Your food will soon be on its way!




gastronomify
------------------------------------------------------------------------------------------------
**Title**: gastronomify

**Web address**: https://api.github.com/repos/csv/gastronomify

**Date**: June 2013

**Description**: To do
* Find some recipes that are easy to mess up by measuring incorrectly.
    (Outliers will be more noticeable with these recipes.)
* Integrate taskrabbit.
* Each recipe should have an ingredient list and a prose explanation of
    how to assemble the dish.


json
------------------------------------------------------------------------------------------------
**Title**: json

**Web address**: https://api.github.com/repos/csv/json

**Date**: June 2013

**Description**: 

mc-api
------------------------------------------------------------------------------------------------
**Title**: mc-api

**Web address**: https://api.github.com/repos/csv/mc-api

**Date**: October 2013

**Description**: Missed Connections API
======================
a crawler, redis store, and flask api for missed connections

## the url endpoint
currently:
```
http://54.205.111.185:5000/
```

## the database
`redis` is not a typical database, it's a key/value store.
to keep the api as simple as possible, the keys are unique combinations 
of city and sexual orientation (m4w, w4m, m4m, m4w, etc), and the values
are lists of json blobs, with each blob corresponding to a missed connection. 

## how do I query the database?
Yes, you may be thinking this:
<br></br>
![img](https://pbs.twimg.com/media/BOIlOviCEAEZXTj.png:medium)

But `mc-api` is pretty easy to query.  the url format is this
```
/?city=<city>&orientation=<orientation>&start=<start_ts>&end=<end_ts>&order=<order>
```
For example:
<br/>
[http://54.205.111.185:5000/?city=orange-county&orientation=m4w&start=0&end=10000000000&order=desc](http://54.205.111.185:5000/?city=orange-county&orientation=m4w&start=0&end=10000000000&order=desc)

If you specify `orientation=all` the api will return all missed connections in a given city.

You can also retrieve a random missed connection at:
<br/>
[http://54.205.111.185:5000/random](http://54.205.111.185:5000/random)

## Data Dictionary
#### `gender` / `target`:
- `m` = man
- `w` = woman
- `t` = transexual

#### `orientation`:
possible_values:
```
None
t4t
t4tt
t4m
t4w
t4mm
t4ww
t4wm
t4mw
tt4t
tt4tt
tt4m
tt4w
tt4mm
tt4ww
tt4wm
tt4mw
m4t
m4tt
m4m
m4w
m4mm
m4ww
m4wm
m4mw
w4t
w4tt
w4m
w4w
w4mm
w4ww
w4wm
w4mw
mm4t
mm4tt
mm4m
mm4w
mm4mm
mm4ww
mm4wm
mm4mw
ww4t
ww4tt
ww4m
ww4w
ww4mm
ww4ww
ww4wm
ww4mw
wm4t
wm4tt
wm4m
wm4w
wm4mm
wm4ww
wm4wm
wm4mw
mw4t
mw4tt
mw4m
mw4w
mw4mm
mw4ww
mw4wm
mw4mw
```

#### `city` (with links to rss feeds)
You can also find a csv of these [here](https://github.com/csv/mc-api/blob/master/crawler/feeds/all_rss_feeds.csv)

----------------------------------------------------

[auburn](http://auburn.craigslist.org/mis/index.rss)<br/>
[birmingham](http://bham.craigslist.org/mis/index.rss)<br/>
[dothan](http://dothan.craigslist.org/mis/index.rss)<br/>
[florence-muscle-shoals](http://shoals.craigslist.org/mis/index.rss)<br/>
[gadsden-anniston](http://gadsden.craigslist.org/mis/index.rss)<br/>
[huntsville-decatur](http://huntsville.craigslist.org/mis/index.rss)<br/>
[mobile](http://mobile.craigslist.org/mis/index.rss)<br/>
[montgomery](http://montgomery.craigslist.org/mis/index.rss)<br/>
[tuscaloosa](http://tuscaloosa.craigslist.org/mis/index.rss)<br/>
[anchorage-mat-su](http://anchorage.craigslist.org/mis/index.rss)<br/>
[fairbanks](http://fairbanks.craigslist.org/mis/index.rss)<br/>
[kenai-peninsula](http://kenai.craigslist.org/mis/index.rss)<br/>
[southeast-alaska](http://juneau.craigslist.org/mis/index.rss)<br/>
[flagstaff-sedona](http://flagstaff.craigslist.org/mis/index.rss)<br/>
[mohave-county](http://mohave.craigslist.org/mis/index.rss)<br/>
[phoenix](http://phoenix.craigslist.org/mis/index.rss)<br/>
[prescott](http://prescott.craigslist.org/mis/index.rss)<br/>
[show-low](http://showlow.craigslist.org/mis/index.rss)<br/>
[sierra-vista](http://sierravista.craigslist.org/mis/index.rss)<br/>
[tucson](http://tucson.craigslist.org/mis/index.rss)<br/>
[yuma](http://yuma.craigslist.org/mis/index.rss)<br/>
[fayetteville](http://fayar.craigslist.org/mis/index.rss)<br/>
[fort-smith](http://fortsmith.craigslist.org/mis/index.rss)<br/>
[jonesboro](http://jonesboro.craigslist.org/mis/index.rss)<br/>
[little-rock](http://littlerock.craigslist.org/mis/index.rss)<br/>
[texarkana](http://texarkana.craigslist.org/mis/index.rss)<br/>
[bakersfield](http://bakersfield.craigslist.org/mis/index.rss)<br/>
[chico](http://chico.craigslist.org/mis/index.rss)<br/>
[fresno-madera](http://fresno.craigslist.org/mis/index.rss)<br/>
[gold-country](http://goldcountry.craigslist.org/mis/index.rss)<br/>
[hanford-corcoran](http://hanford.craigslist.org/mis/index.rss)<br/>
[humboldt-county](http://humboldt.craigslist.org/mis/index.rss)<br/>
[imperial-county](http://imperial.craigslist.org/mis/index.rss)<br/>
[inland-empire](http://inlandempire.craigslist.org/mis/index.rss)<br/>
[los-angeles](http://losangeles.craigslist.org/mis/index.rss)<br/>
[mendocino-county](http://mendocino.craigslist.org/mis/index.rss)<br/>
[merced](http://merced.craigslist.org/mis/index.rss)<br/>
[modesto](http://modesto.craigslist.org/mis/index.rss)<br/>
[monterey-bay](http://monterey.craigslist.org/mis/index.rss)<br/>
[orange-county](http://orangecounty.craigslist.org/mis/index.rss)<br/>
[palm-springs](http://palmsprings.craigslist.org/mis/index.rss)<br/>
[redding](http://redding.craigslist.org/mis/index.rss)<br/>
[sacramento](http://sacramento.craigslist.org/mis/index.rss)<br/>
[san-diego](http://sandiego.craigslist.org/mis/index.rss)<br/>
[san-francisco-bay-area](http://sfbay.craigslist.org/mis/index.rss)<br/>
[san-luis-obispo](http://slo.craigslist.org/mis/index.rss)<br/>
[santa-barbara](http://santabarbara.craigslist.org/mis/index.rss)<br/>
[santa-maria](http://santamaria.craigslist.org/mis/index.rss)<br/>
[siskiyou-county](http://siskiyou.craigslist.org/mis/index.rss)<br/>
[stockton](http://stockton.craigslist.org/mis/index.rss)<br/>
[susanville](http://susanville.craigslist.org/mis/index.rss)<br/>
[ventura-county](http://ventura.craigslist.org/mis/index.rss)<br/>
[visalia-tulare](http://visalia.craigslist.org/mis/index.rss)<br/>
[yuba-sutter](http://yubasutter.craigslist.org/mis/index.rss)<br/>
[boulder](http://boulder.craigslist.org/mis/index.rss)<br/>
[colorado-springs](http://cosprings.craigslist.org/mis/index.rss)<br/>
[denver](http://denver.craigslist.org/mis/index.rss)<br/>
[eastern-co](http://eastco.craigslist.org/mis/index.rss)<br/>
[fort-collins-north-co](http://fortcollins.craigslist.org/mis/index.rss)<br/>
[high-rockies](http://rockies.craigslist.org/mis/index.rss)<br/>
[pueblo](http://pueblo.craigslist.org/mis/index.rss)<br/>
[western-slope](http://westslope.craigslist.org/mis/index.rss)<br/>
[eastern-ct](http://newlondon.craigslist.org/mis/index.rss)<br/>
[hartford](http://hartford.craigslist.org/mis/index.rss)<br/>
[new-haven](http://newhaven.craigslist.org/mis/index.rss)<br/>
[northwest-ct](http://nwct.craigslist.org/mis/index.rss)<br/>
[delaware](http://delaware.craigslist.org/mis/index.rss)<br/>
[washington](http://washingtondc.craigslist.org/mis/index.rss)<br/>
[daytona-beach](http://daytona.craigslist.org/mis/index.rss)<br/>
[florida-keys](http://keys.craigslist.org/mis/index.rss)<br/>
[fort-lauderdale](http://fortlauderdale.craigslist.org/mis/index.rss)<br/>
[ft-myers-sw-florida](http://fortmyers.craigslist.org/mis/index.rss)<br/>
[gainesville](http://gainesville.craigslist.org/mis/index.rss)<br/>
[heartland-florida](http://cfl.craigslist.org/mis/index.rss)<br/>
[jacksonville](http://jacksonville.craigslist.org/mis/index.rss)<br/>
[lakeland](http://lakeland.craigslist.org/mis/index.rss)<br/>
[north-central-fl](http://lakecity.craigslist.org/mis/index.rss)<br/>
[ocala](http://ocala.craigslist.org/mis/index.rss)<br/>
[okaloosa-walton](http://okaloosa.craigslist.org/mis/index.rss)<br/>
[orlando](http://orlando.craigslist.org/mis/index.rss)<br/>
[panama-city](http://panamacity.craigslist.org/mis/index.rss)<br/>
[pensacola](http://pensacola.craigslist.org/mis/index.rss)<br/>
[sarasota-bradenton](http://sarasota.craigslist.org/mis/index.rss)<br/>
[south-florida](http://miami.craigslist.org/mis/index.rss)<br/>
[space-coast](http://spacecoast.craigslist.org/mis/index.rss)<br/>
[st-augustine](http://staugustine.craigslist.org/mis/index.rss)<br/>
[tallahassee](http://tallahassee.craigslist.org/mis/index.rss)<br/>
[tampa-bay-area](http://tampa.craigslist.org/mis/index.rss)<br/>
[treasure-coast](http://treasure.craigslist.org/mis/index.rss)<br/>
[west-palm-beach](http://westpalmbeach.craigslist.org/mis/index.rss)<br/>
[albany](http://albanyga.craigslist.org/mis/index.rss)<br/>
[athens](http://athensga.craigslist.org/mis/index.rss)<br/>
[atlanta](http://atlanta.craigslist.org/mis/index.rss)<br/>
[augusta](http://augusta.craigslist.org/mis/index.rss)<br/>
[brunswick](http://brunswick.craigslist.org/mis/index.rss)<br/>
[columbus](http://columbusga.craigslist.org/mis/index.rss)<br/>
[macon-warner-robins](http://macon.craigslist.org/mis/index.rss)<br/>
[northwest-ga](http://nwga.craigslist.org/mis/index.rss)<br/>
[savannah-hinesville](http://savannah.craigslist.org/mis/index.rss)<br/>
[statesboro](http://statesboro.craigslist.org/mis/index.rss)<br/>
[valdosta](http://valdosta.craigslist.org/mis/index.rss)<br/>
[hawaii](http://honolulu.craigslist.org/mis/index.rss)<br/>
[boise](http://boise.craigslist.org/mis/index.rss)<br/>
[east-idaho](http://eastidaho.craigslist.org/mis/index.rss)<br/>
[lewiston-clarkston](http://lewiston.craigslist.org/mis/index.rss)<br/>
[twin-falls](http://twinfalls.craigslist.org/mis/index.rss)<br/>
[bloomington-normal](http://bn.craigslist.org/mis/index.rss)<br/>
[champaign-urbana](http://chambana.craigslist.org/mis/index.rss)<br/>
[chicago](http://chicago.craigslist.org/mis/index.rss)<br/>
[decatur](http://decatur.craigslist.org/mis/index.rss)<br/>
[la-salle-co](http://lasalle.craigslist.org/mis/index.rss)<br/>
[mattoon-charleston](http://mattoon.craigslist.org/mis/index.rss)<br/>
[peoria](http://peoria.craigslist.org/mis/index.rss)<br/>
[rockford](http://rockford.craigslist.org/mis/index.rss)<br/>
[southern-illinois](http://carbondale.craigslist.org/mis/index.rss)<br/>
[springfield](http://springfieldil.craigslist.org/mis/index.rss)<br/>
[western-il](http://quincy.craigslist.org/mis/index.rss)<br/>
[bloomington](http://bloomington.craigslist.org/mis/index.rss)<br/>
[evansville](http://evansville.craigslist.org/mis/index.rss)<br/>
[fort-wayne](http://fortwayne.craigslist.org/mis/index.rss)<br/>
[indianapolis](http://indianapolis.craigslist.org/mis/index.rss)<br/>
[kokomo](http://kokomo.craigslist.org/mis/index.rss)<br/>
[lafayette-west-lafayette](http://tippecanoe.craigslist.org/mis/index.rss)<br/>
[muncie-anderson](http://muncie.craigslist.org/mis/index.rss)<br/>
[richmond](http://richmondin.craigslist.org/mis/index.rss)<br/>
[south-bend-michiana](http://southbend.craigslist.org/mis/index.rss)<br/>
[terre-haute](http://terrehaute.craigslist.org/mis/index.rss)<br/>
[ames](http://ames.craigslist.org/mis/index.rss)<br/>
[cedar-rapids](http://cedarrapids.craigslist.org/mis/index.rss)<br/>
[des-moines](http://desmoines.craigslist.org/mis/index.rss)<br/>
[dubuque](http://dubuque.craigslist.org/mis/index.rss)<br/>
[fort-dodge](http://fortdodge.craigslist.org/mis/index.rss)<br/>
[iowa-city](http://iowacity.craigslist.org/mis/index.rss)<br/>
[mason-city](http://masoncity.craigslist.org/mis/index.rss)<br/>
[quad-cities](http://quadcities.craigslist.org/mis/index.rss)<br/>
[sioux-city](http://siouxcity.craigslist.org/mis/index.rss)<br/>
[southeast-ia](http://ottumwa.craigslist.org/mis/index.rss)<br/>
[waterloo-cedar-falls](http://waterloo.craigslist.org/mis/index.rss)<br/>
[lawrence](http://lawrence.craigslist.org/mis/index.rss)<br/>
[manhattan](http://ksu.craigslist.org/mis/index.rss)<br/>
[northwest-ks](http://nwks.craigslist.org/mis/index.rss)<br/>
[salina](http://salina.craigslist.org/mis/index.rss)<br/>
[southeast-ks](http://seks.craigslist.org/mis/index.rss)<br/>
[southwest-ks](http://swks.craigslist.org/mis/index.rss)<br/>
[topeka](http://topeka.craigslist.org/mis/index.rss)<br/>
[wichita](http://wichita.craigslist.org/mis/index.rss)<br/>
[bowling-green](http://bgky.craigslist.org/mis/index.rss)<br/>
[eastern-kentucky](http://eastky.craigslist.org/mis/index.rss)<br/>
[lexington](http://lexington.craigslist.org/mis/index.rss)<br/>
[louisville](http://louisville.craigslist.org/mis/index.rss)<br/>
[owensboro](http://owensboro.craigslist.org/mis/index.rss)<br/>
[western-ky](http://westky.craigslist.org/mis/index.rss)<br/>
[baton-rouge](http://batonrouge.craigslist.org/mis/index.rss)<br/>
[central-louisiana](http://cenla.craigslist.org/mis/index.rss)<br/>
[houma](http://houma.craigslist.org/mis/index.rss)<br/>
[lafayette](http://lafayette.craigslist.org/mis/index.rss)<br/>
[lake-charles](http://lakecharles.craigslist.org/mis/index.rss)<br/>
[monroe](http://monroe.craigslist.org/mis/index.rss)<br/>
[new-orleans](http://neworleans.craigslist.org/mis/index.rss)<br/>
[shreveport](http://shreveport.craigslist.org/mis/index.rss)<br/>
[maine](http://maine.craigslist.org/mis/index.rss)<br/>
[annapolis](http://annapolis.craigslist.org/mis/index.rss)<br/>
[baltimore](http://baltimore.craigslist.org/mis/index.rss)<br/>
[eastern-shore](http://easternshore.craigslist.org/mis/index.rss)<br/>
[frederick](http://frederick.craigslist.org/mis/index.rss)<br/>
[southern-maryland](http://smd.craigslist.org/mis/index.rss)<br/>
[western-maryland](http://westmd.craigslist.org/mis/index.rss)<br/>
[boston](http://boston.craigslist.org/mis/index.rss)<br/>
[cape-cod-islands](http://capecod.craigslist.org/mis/index.rss)<br/>
[south-coast](http://southcoast.craigslist.org/mis/index.rss)<br/>
[western-massachusetts](http://westernmass.craigslist.org/mis/index.rss)<br/>
[worcester-central-ma](http://worcester.craigslist.org/mis/index.rss)<br/>
[ann-arbor](http://annarbor.craigslist.org/mis/index.rss)<br/>
[battle-creek](http://battlecreek.craigslist.org/mis/index.rss)<br/>
[central-michigan](http://centralmich.craigslist.org/mis/index.rss)<br/>
[detroit-metro](http://detroit.craigslist.org/mis/index.rss)<br/>
[flint](http://flint.craigslist.org/mis/index.rss)<br/>
[grand-rapids](http://grandrapids.craigslist.org/mis/index.rss)<br/>
[holland](http://holland.craigslist.org/mis/index.rss)<br/>
[jackson](http://jxn.craigslist.org/mis/index.rss)<br/>
[kalamazoo](http://kalamazoo.craigslist.org/mis/index.rss)<br/>
[lansing](http://lansing.craigslist.org/mis/index.rss)<br/>
[monroe](http://monroemi.craigslist.org/mis/index.rss)<br/>
[muskegon](http://muskegon.craigslist.org/mis/index.rss)<br/>
[northern-michigan](http://nmi.craigslist.org/mis/index.rss)<br/>
[port-huron](http://porthuron.craigslist.org/mis/index.rss)<br/>
[saginaw-midland-baycity](http://saginaw.craigslist.org/mis/index.rss)<br/>
[southwest-michigan](http://swmi.craigslist.org/mis/index.rss)<br/>
[the-thumb](http://thumb.craigslist.org/mis/index.rss)<br/>
[upper-peninsula](http://up.craigslist.org/mis/index.rss)<br/>
[bemidji](http://bemidji.craigslist.org/mis/index.rss)<br/>
[brainerd](http://brainerd.craigslist.org/mis/index.rss)<br/>
[duluth-superior](http://duluth.craigslist.org/mis/index.rss)<br/>
[mankato](http://mankato.craigslist.org/mis/index.rss)<br/>
[minneapolis-st-paul](http://minneapolis.craigslist.org/mis/index.rss)<br/>
[rochester](http://rmn.craigslist.org/mis/index.rss)<br/>
[southwest-mn](http://marshall.craigslist.org/mis/index.rss)<br/>
[st-cloud](http://stcloud.craigslist.org/mis/index.rss)<br/>
[gulfport-biloxi](http://gulfport.craigslist.org/mis/index.rss)<br/>
[hattiesburg](http://hattiesburg.craigslist.org/mis/index.rss)<br/>
[jackson](http://jackson.craigslist.org/mis/index.rss)<br/>
[meridian](http://meridian.craigslist.org/mis/index.rss)<br/>
[north-mississippi](http://northmiss.craigslist.org/mis/index.rss)<br/>
[southwest-ms](http://natchez.craigslist.org/mis/index.rss)<br/>
[columbia-jeff-city](http://columbiamo.craigslist.org/mis/index.rss)<br/>
[joplin](http://joplin.craigslist.org/mis/index.rss)<br/>
[kansas-city](http://kansascity.craigslist.org/mis/index.rss)<br/>
[kirksville](http://kirksville.craigslist.org/mis/index.rss)<br/>
[lake-of-the-ozarks](http://loz.craigslist.org/mis/index.rss)<br/>
[southeast-missouri](http://semo.craigslist.org/mis/index.rss)<br/>
[springfield](http://springfield.craigslist.org/mis/index.rss)<br/>
[st-joseph](http://stjoseph.craigslist.org/mis/index.rss)<br/>
[st-louis](http://stlouis.craigslist.org/mis/index.rss)<br/>
[billings](http://billings.craigslist.org/mis/index.rss)<br/>
[bozeman](http://bozeman.craigslist.org/mis/index.rss)<br/>
[butte](http://butte.craigslist.org/mis/index.rss)<br/>
[great-falls](http://greatfalls.craigslist.org/mis/index.rss)<br/>
[helena](http://helena.craigslist.org/mis/index.rss)<br/>
[kalispell](http://kalispell.craigslist.org/mis/index.rss)<br/>
[missoula](http://missoula.craigslist.org/mis/index.rss)<br/>
[montana-(old)](http://montana.craigslist.org/mis/index.rss)<br/>
[grand-island](http://grandisland.craigslist.org/mis/index.rss)<br/>
[lincoln](http://lincoln.craigslist.org/mis/index.rss)<br/>
[north-platte](http://northplatte.craigslist.org/mis/index.rss)<br/>
[omaha-council-bluffs](http://omaha.craigslist.org/mis/index.rss)<br/>
[scottsbluff-panhandle](http://scottsbluff.craigslist.org/mis/index.rss)<br/>
[elko](http://elko.craigslist.org/mis/index.rss)<br/>
[las-vegas](http://lasvegas.craigslist.org/mis/index.rss)<br/>
[reno-tahoe](http://reno.craigslist.org/mis/index.rss)<br/>
[new-hampshire](http://nh.craigslist.org/mis/index.rss)<br/>
[central-nj](http://cnj.craigslist.org/mis/index.rss)<br/>
[jersey-shore](http://jerseyshore.craigslist.org/mis/index.rss)<br/>
[north-jersey](http://newjersey.craigslist.org/mis/index.rss)<br/>
[south-jersey](http://southjersey.craigslist.org/mis/index.rss)<br/>
[albuquerque](http://albuquerque.craigslist.org/mis/index.rss)<br/>
[clovis-portales](http://clovis.craigslist.org/mis/index.rss)<br/>
[farmington](http://farmington.craigslist.org/mis/index.rss)<br/>
[las-cruces](http://lascruces.craigslist.org/mis/index.rss)<br/>
[roswell-carlsbad](http://roswell.craigslist.org/mis/index.rss)<br/>
[santa-fe-taos](http://santafe.craigslist.org/mis/index.rss)<br/>
[albany](http://albany.craigslist.org/mis/index.rss)<br/>
[binghamton](http://binghamton.craigslist.org/mis/index.rss)<br/>
[buffalo](http://buffalo.craigslist.org/mis/index.rss)<br/>
[catskills](http://catskills.craigslist.org/mis/index.rss)<br/>
[chautauqua](http://chautauqua.craigslist.org/mis/index.rss)<br/>
[elmira-corning](http://elmira.craigslist.org/mis/index.rss)<br/>
[finger-lakes](http://fingerlakes.craigslist.org/mis/index.rss)<br/>
[glens-falls](http://glensfalls.craigslist.org/mis/index.rss)<br/>
[hudson-valley](http://hudsonvalley.craigslist.org/mis/index.rss)<br/>
[ithaca](http://ithaca.craigslist.org/mis/index.rss)<br/>
[long-island](http://longisland.craigslist.org/mis/index.rss)<br/>
[new-york-city](http://newyork.craigslist.org/mis/index.rss)<br/>
[oneonta](http://oneonta.craigslist.org/mis/index.rss)<br/>
[plattsburgh-adirondacks](http://plattsburgh.craigslist.org/mis/index.rss)<br/>
[potsdam-canton-massena](http://potsdam.craigslist.org/mis/index.rss)<br/>
[rochester](http://rochester.craigslist.org/mis/index.rss)<br/>
[syracuse](http://syracuse.craigslist.org/mis/index.rss)<br/>
[twin-tiers-ny-pa](http://twintiers.craigslist.org/mis/index.rss)<br/>
[utica-rome-oneida](http://utica.craigslist.org/mis/index.rss)<br/>
[watertown](http://watertown.craigslist.org/mis/index.rss)<br/>
[asheville](http://asheville.craigslist.org/mis/index.rss)<br/>
[boone](http://boone.craigslist.org/mis/index.rss)<br/>
[charlotte](http://charlotte.craigslist.org/mis/index.rss)<br/>
[eastern-nc](http://eastnc.craigslist.org/mis/index.rss)<br/>
[fayetteville](http://fayetteville.craigslist.org/mis/index.rss)<br/>
[greensboro](http://greensboro.craigslist.org/mis/index.rss)<br/>
[hickory-lenoir](http://hickory.craigslist.org/mis/index.rss)<br/>
[jacksonville](http://onslow.craigslist.org/mis/index.rss)<br/>
[outer-banks](http://outerbanks.craigslist.org/mis/index.rss)<br/>
[raleigh-durham-ch](http://raleigh.craigslist.org/mis/index.rss)<br/>
[wilmington](http://wilmington.craigslist.org/mis/index.rss)<br/>
[winston-salem](http://winstonsalem.craigslist.org/mis/index.rss)<br/>
[bismarck](http://bismarck.craigslist.org/mis/index.rss)<br/>
[fargo-moorhead](http://fargo.craigslist.org/mis/index.rss)<br/>
[grand-forks](http://grandforks.craigslist.org/mis/index.rss)<br/>
[north-dakota](http://nd.craigslist.org/mis/index.rss)<br/>
[akron-canton](http://akroncanton.craigslist.org/mis/index.rss)<br/>
[ashtabula](http://ashtabula.craigslist.org/mis/index.rss)<br/>
[athens](http://athensohio.craigslist.org/mis/index.rss)<br/>
[chillicothe](http://chillicothe.craigslist.org/mis/index.rss)<br/>
[cincinnati](http://cincinnati.craigslist.org/mis/index.rss)<br/>
[cleveland](http://cleveland.craigslist.org/mis/index.rss)<br/>
[columbus](http://columbus.craigslist.org/mis/index.rss)<br/>
[dayton-springfield](http://dayton.craigslist.org/mis/index.rss)<br/>
[lima-findlay](http://limaohio.craigslist.org/mis/index.rss)<br/>
[mansfield](http://mansfield.craigslist.org/mis/index.rss)<br/>
[sandusky](http://sandusky.craigslist.org/mis/index.rss)<br/>
[toledo](http://toledo.craigslist.org/mis/index.rss)<br/>
[tuscarawas-co](http://tuscarawas.craigslist.org/mis/index.rss)<br/>
[youngstown](http://youngstown.craigslist.org/mis/index.rss)<br/>
[zanesville-cambridge](http://zanesville.craigslist.org/mis/index.rss)<br/>
[lawton](http://lawton.craigslist.org/mis/index.rss)<br/>
[northwest-ok](http://enid.craigslist.org/mis/index.rss)<br/>
[oklahoma-city](http://oklahomacity.craigslist.org/mis/index.rss)<br/>
[stillwater](http://stillwater.craigslist.org/mis/index.rss)<br/>
[tulsa](http://tulsa.craigslist.org/mis/index.rss)<br/>
[bend](http://bend.craigslist.org/mis/index.rss)<br/>
[corvallis-albany](http://corvallis.craigslist.org/mis/index.rss)<br/>
[east-oregon](http://eastoregon.craigslist.org/mis/index.rss)<br/>
[eugene](http://eugene.craigslist.org/mis/index.rss)<br/>
[klamath-falls](http://klamath.craigslist.org/mis/index.rss)<br/>
[medford-ashland](http://medford.craigslist.org/mis/index.rss)<br/>
[oregon-coast](http://oregoncoast.craigslist.org/mis/index.rss)<br/>
[portland](http://portland.craigslist.org/mis/index.rss)<br/>
[roseburg](http://roseburg.craigslist.org/mis/index.rss)<br/>
[salem](http://salem.craigslist.org/mis/index.rss)<br/>
[altoona-johnstown](http://altoona.craigslist.org/mis/index.rss)<br/>
[cumberland-valley](http://chambersburg.craigslist.org/mis/index.rss)<br/>
[erie](http://erie.craigslist.org/mis/index.rss)<br/>
[harrisburg](http://harrisburg.craigslist.org/mis/index.rss)<br/>
[lancaster](http://lancaster.craigslist.org/mis/index.rss)<br/>
[lehigh-valley](http://allentown.craigslist.org/mis/index.rss)<br/>
[meadville](http://meadville.craigslist.org/mis/index.rss)<br/>
[philadelphia](http://philadelphia.craigslist.org/mis/index.rss)<br/>
[pittsburgh](http://pittsburgh.craigslist.org/mis/index.rss)<br/>
[poconos](http://poconos.craigslist.org/mis/index.rss)<br/>
[reading](http://reading.craigslist.org/mis/index.rss)<br/>
[scranton-wilkes-barre](http://scranton.craigslist.org/mis/index.rss)<br/>
[state-college](http://pennstate.craigslist.org/mis/index.rss)<br/>
[williamsport](http://williamsport.craigslist.org/mis/index.rss)<br/>
[york](http://york.craigslist.org/mis/index.rss)<br/>
[rhode-island](http://providence.craigslist.org/mis/index.rss)<br/>
[charleston](http://charleston.craigslist.org/mis/index.rss)<br/>
[columbia](http://columbia.craigslist.org/mis/index.rss)<br/>
[florence](http://florencesc.craigslist.org/mis/index.rss)<br/>
[greenville-upstate](http://greenville.craigslist.org/mis/index.rss)<br/>
[hilton-head](http://hiltonhead.craigslist.org/mis/index.rss)<br/>
[myrtle-beach](http://myrtlebeach.craigslist.org/mis/index.rss)<br/>
[northeast-sd](http://nesd.craigslist.org/mis/index.rss)<br/>
[pierre-central-sd](http://csd.craigslist.org/mis/index.rss)<br/>
[rapid-city-west-sd](http://rapidcity.craigslist.org/mis/index.rss)<br/>
[sioux-falls-se-sd](http://siouxfalls.craigslist.org/mis/index.rss)<br/>
[south-dakota](http://sd.craigslist.org/mis/index.rss)<br/>
[chattanooga](http://chattanooga.craigslist.org/mis/index.rss)<br/>
[clarksville](http://clarksville.craigslist.org/mis/index.rss)<br/>
[cookeville](http://cookeville.craigslist.org/mis/index.rss)<br/>
[jackson](http://jacksontn.craigslist.org/mis/index.rss)<br/>
[knoxville](http://knoxville.craigslist.org/mis/index.rss)<br/>
[memphis](http://memphis.craigslist.org/mis/index.rss)<br/>
[nashville](http://nashville.craigslist.org/mis/index.rss)<br/>
[tri-cities](http://tricities.craigslist.org/mis/index.rss)<br/>
[abilene](http://abilene.craigslist.org/mis/index.rss)<br/>
[amarillo](http://amarillo.craigslist.org/mis/index.rss)<br/>
[austin](http://austin.craigslist.org/mis/index.rss)<br/>
[beaumont-port-arthur](http://beaumont.craigslist.org/mis/index.rss)<br/>
[brownsville](http://brownsville.craigslist.org/mis/index.rss)<br/>
[college-station](http://collegestation.craigslist.org/mis/index.rss)<br/>
[corpus-christi](http://corpuschristi.craigslist.org/mis/index.rss)<br/>
[dallas-fort-worth](http://dallas.craigslist.org/mis/index.rss)<br/>
[deep-east-texas](http://nacogdoches.craigslist.org/mis/index.rss)<br/>
[del-rio-eagle-pass](http://delrio.craigslist.org/mis/index.rss)<br/>
[el-paso](http://elpaso.craigslist.org/mis/index.rss)<br/>
[galveston](http://galveston.craigslist.org/mis/index.rss)<br/>
[houston](http://houston.craigslist.org/mis/index.rss)<br/>
[killeen-temple-ft-hood](http://killeen.craigslist.org/mis/index.rss)<br/>
[laredo](http://laredo.craigslist.org/mis/index.rss)<br/>
[lubbock](http://lubbock.craigslist.org/mis/index.rss)<br/>
[mcallen-edinburg](http://mcallen.craigslist.org/mis/index.rss)<br/>
[odessa-midland](http://odessa.craigslist.org/mis/index.rss)<br/>
[san-angelo](http://sanangelo.craigslist.org/mis/index.rss)<br/>
[san-antonio](http://sanantonio.craigslist.org/mis/index.rss)<br/>
[san-marcos](http://sanmarcos.craigslist.org/mis/index.rss)<br/>
[southwest-tx](http://bigbend.craigslist.org/mis/index.rss)<br/>
[texoma](http://texoma.craigslist.org/mis/index.rss)<br/>
[tyler-east-tx](http://easttexas.craigslist.org/mis/index.rss)<br/>
[victoria](http://victoriatx.craigslist.org/mis/index.rss)<br/>
[waco](http://waco.craigslist.org/mis/index.rss)<br/>
[wichita-falls](http://wichitafalls.craigslist.org/mis/index.rss)<br/>
[logan](http://logan.craigslist.org/mis/index.rss)<br/>
[ogden-clearfield](http://ogden.craigslist.org/mis/index.rss)<br/>
[provo-orem](http://provo.craigslist.org/mis/index.rss)<br/>
[salt-lake-city](http://saltlakecity.craigslist.org/mis/index.rss)<br/>
[st-george](http://stgeorge.craigslist.org/mis/index.rss)<br/>
[vermont](http://burlington.craigslist.org/mis/index.rss)<br/>
[charlottesville](http://charlottesville.craigslist.org/mis/index.rss)<br/>
[danville](http://danville.craigslist.org/mis/index.rss)<br/>
[fredericksburg](http://fredericksburg.craigslist.org/mis/index.rss)<br/>
[hampton-roads](http://norfolk.craigslist.org/mis/index.rss)<br/>
[harrisonburg](http://harrisonburg.craigslist.org/mis/index.rss)<br/>
[lynchburg](http://lynchburg.craigslist.org/mis/index.rss)<br/>
[new-river-valley](http://blacksburg.craigslist.org/mis/index.rss)<br/>
[richmond](http://richmond.craigslist.org/mis/index.rss)<br/>
[roanoke](http://roanoke.craigslist.org/mis/index.rss)<br/>
[southwest-va](http://swva.craigslist.org/mis/index.rss)<br/>
[winchester](http://winchester.craigslist.org/mis/index.rss)<br/>
[bellingham](http://bellingham.craigslist.org/mis/index.rss)<br/>
[kennewick-pasco-richland](http://kpr.craigslist.org/mis/index.rss)<br/>
[moses-lake](http://moseslake.craigslist.org/mis/index.rss)<br/>
[olympic-peninsula](http://olympic.craigslist.org/mis/index.rss)<br/>
[pullman-moscow](http://pullman.craigslist.org/mis/index.rss)<br/>
[seattle-tacoma](http://seattle.craigslist.org/mis/index.rss)<br/>
[skagit-island-sji](http://skagit.craigslist.org/mis/index.rss)<br/>
[spokane-coeur-d'alene](http://spokane.craigslist.org/mis/index.rss)<br/>
[wenatchee](http://wenatchee.craigslist.org/mis/index.rss)<br/>
[yakima](http://yakima.craigslist.org/mis/index.rss)<br/>
[charleston](http://charlestonwv.craigslist.org/mis/index.rss)<br/>
[eastern-panhandle](http://martinsburg.craigslist.org/mis/index.rss)<br/>
[huntington-ashland](http://huntington.craigslist.org/mis/index.rss)<br/>
[morgantown](http://morgantown.craigslist.org/mis/index.rss)<br/>
[northern-panhandle](http://wheeling.craigslist.org/mis/index.rss)<br/>
[parkersburg-marietta](http://parkersburg.craigslist.org/mis/index.rss)<br/>
[southern-wv](http://swv.craigslist.org/mis/index.rss)<br/>
[west-virginia-(old)](http://wv.craigslist.org/mis/index.rss)<br/>
[appleton-oshkosh-fdl](http://appleton.craigslist.org/mis/index.rss)<br/>
[eau-claire](http://eauclaire.craigslist.org/mis/index.rss)<br/>
[green-bay](http://greenbay.craigslist.org/mis/index.rss)<br/>
[janesville](http://janesville.craigslist.org/mis/index.rss)<br/>
[kenosha-racine](http://racine.craigslist.org/mis/index.rss)<br/>
[la-crosse](http://lacrosse.craigslist.org/mis/index.rss)<br/>
[madison](http://madison.craigslist.org/mis/index.rss)<br/>
[milwaukee](http://milwaukee.craigslist.org/mis/index.rss)<br/>
[northern-wi](http://northernwi.craigslist.org/mis/index.rss)<br/>
[sheboygan](http://sheboygan.craigslist.org/mis/index.rss)<br/>
[wausau](http://wausau.craigslist.org/mis/index.rss)<br/>
[wyoming](http://wyoming.craigslist.org/mis/index.rss)<br/>
[guam-micronesia](http://micronesia.craigslist.org/mis/index.rss)<br/>
[puerto-rico](http://puertorico.craigslist.org/mis/index.rss)<br/>
[u-s-virgin-islands](http://virgin.craigslist.org/mis/index.rss)<br/>
[calgary](http://calgary.craigslist.ca/mis/index.rss)<br/>
[edmonton](http://edmonton.craigslist.ca/mis/index.rss)<br/>
[ft-mcmurray](http://ftmcmurray.craigslist.ca/mis/index.rss)<br/>
[lethbridge](http://lethbridge.craigslist.ca/mis/index.rss)<br/>
[medicine-hat](http://hat.craigslist.ca/mis/index.rss)<br/>
[peace-river-country](http://peace.craigslist.ca/mis/index.rss)<br/>
[red-deer](http://reddeer.craigslist.ca/mis/index.rss)<br/>
[cariboo](http://cariboo.craigslist.ca/mis/index.rss)<br/>
[comox-valley](http://comoxvalley.craigslist.ca/mis/index.rss)<br/>
[fraser-valley](http://abbotsford.craigslist.ca/mis/index.rss)<br/>
[kamloops](http://kamloops.craigslist.ca/mis/index.rss)<br/>
[kelowna-okanagan](http://kelowna.craigslist.ca/mis/index.rss)<br/>
[kootenays](http://cranbrook.craigslist.ca/mis/index.rss)<br/>
[nanaimo](http://nanaimo.craigslist.ca/mis/index.rss)<br/>
[prince-george](http://princegeorge.craigslist.ca/mis/index.rss)<br/>
[skeena-bulkley](http://skeena.craigslist.ca/mis/index.rss)<br/>
[sunshine-coast](http://sunshine.craigslist.ca/mis/index.rss)<br/>
[vancouver](http://vancouver.craigslist.ca/mis/index.rss)<br/>
[victoria](http://victoria.craigslist.ca/mis/index.rss)<br/>
[whistler](http://whistler.craigslist.ca/mis/index.rss)<br/>
[winnipeg](http://winnipeg.craigslist.ca/mis/index.rss)<br/>
[new-brunswick](http://newbrunswick.craigslist.ca/mis/index.rss)<br/>
[st-john's](http://newfoundland.craigslist.ca/mis/index.rss)<br/>
[territories](http://territories.craigslist.ca/mis/index.rss)<br/>
[yellowknife](http://yellowknife.craigslist.ca/mis/index.rss)<br/>
[halifax](http://halifax.craigslist.ca/mis/index.rss)<br/>
[barrie](http://barrie.craigslist.ca/mis/index.rss)<br/>
[belleville](http://belleville.craigslist.ca/mis/index.rss)<br/>
[brantford-woodstock](http://brantford.craigslist.ca/mis/index.rss)<br/>
[chatham-kent](http://chatham.craigslist.ca/mis/index.rss)<br/>
[cornwall](http://cornwall.craigslist.ca/mis/index.rss)<br/>
[guelph](http://guelph.craigslist.ca/mis/index.rss)<br/>
[hamilton-burlington](http://hamilton.craigslist.ca/mis/index.rss)<br/>
[kingston](http://kingston.craigslist.ca/mis/index.rss)<br/>
[kitchener-waterloo-cambridge](http://kitchener.craigslist.ca/mis/index.rss)<br/>
[london](http://londonon.craigslist.ca/mis/index.rss)<br/>
[niagara-region](http://niagara.craigslist.ca/mis/index.rss)<br/>
[ottawa-hull-gatineau](http://ottawa.craigslist.ca/mis/index.rss)<br/>
[owen-sound](http://owensound.craigslist.ca/mis/index.rss)<br/>
[peterborough](http://peterborough.craigslist.ca/mis/index.rss)<br/>
[sarnia](http://sarnia.craigslist.ca/mis/index.rss)<br/>
[sault-ste-marie](http://soo.craigslist.ca/mis/index.rss)<br/>
[sudbury](http://sudbury.craigslist.ca/mis/index.rss)<br/>
[thunder-bay](http://thunderbay.craigslist.ca/mis/index.rss)<br/>
[toronto](http://toronto.craigslist.ca/mis/index.rss)<br/>
[windsor](http://windsor.craigslist.ca/mis/index.rss)<br/>
[prince-edward-island](http://pei.craigslist.ca/mis/index.rss)<br/>
[montreal](http://montreal.craigslist.ca/mis/index.rss)<br/>
[quebec-city](http://quebec.craigslist.ca/mis/index.rss)<br/>
[saguenay](http://saguenay.craigslist.ca/mis/index.rss)<br/>
[sherbrooke](http://sherbrooke.craigslist.ca/mis/index.rss)<br/>
[trois-rivieres](http://troisrivieres.craigslist.ca/mis/index.rss)<br/>
[regina](http://regina.craigslist.ca/mis/index.rss)<br/>
[saskatoon](http://saskatoon.craigslist.ca/mis/index.rss)<br/>
[whitehorse](http://whitehorse.craigslist.ca/mis/index.rss)<br/>
[vienna](http://vienna.craigslist.at/mis/index.rss)<br/>
[belgium](http://brussels.craigslist.org/mis/index.rss)<br/>
[bulgaria](http://bulgaria.craigslist.org/mis/index.rss)<br/>
[croatia](http://zagreb.craigslist.org/mis/index.rss)<br/>
[prague](http://prague.craigslist.cz/mis/index.rss)<br/>
[copenhagen](http://copenhagen.craigslist.org/mis/index.rss)<br/>
[finland](http://helsinki.craigslist.fi/mis/index.rss)<br/>
[bordeaux](http://bordeaux.craigslist.org/mis/index.rss)<br/>
[brittany](http://rennes.craigslist.org/mis/index.rss)<br/>
[grenoble](http://grenoble.craigslist.org/mis/index.rss)<br/>
[lille](http://lille.craigslist.org/mis/index.rss)<br/>
[loire-valley](http://loire.craigslist.org/mis/index.rss)<br/>
[lyon](http://lyon.craigslist.org/mis/index.rss)<br/>
[marseille](http://marseilles.craigslist.org/mis/index.rss)<br/>
[montpellier](http://montpellier.craigslist.org/mis/index.rss)<br/>
[nice-cote-d'azur](http://cotedazur.craigslist.org/mis/index.rss)<br/>
[normandy](http://rouen.craigslist.org/mis/index.rss)<br/>
[paris](http://paris.craigslist.org/mis/index.rss)<br/>
[strasbourg](http://strasbourg.craigslist.org/mis/index.rss)<br/>
[toulouse](http://toulouse.craigslist.org/mis/index.rss)<br/>
[berlin](http://berlin.craigslist.de/mis/index.rss)<br/>
[bremen](http://bremen.craigslist.de/mis/index.rss)<br/>
[cologne](http://cologne.craigslist.de/mis/index.rss)<br/>
[dresden](http://dresden.craigslist.de/mis/index.rss)<br/>
[dusseldorf](http://dusseldorf.craigslist.de/mis/index.rss)<br/>
[essen-ruhr](http://essen.craigslist.de/mis/index.rss)<br/>
[frankfurt](http://frankfurt.craigslist.de/mis/index.rss)<br/>
[hamburg](http://hamburg.craigslist.de/mis/index.rss)<br/>
[hannover](http://hannover.craigslist.de/mis/index.rss)<br/>
[heidelberg](http://heidelberg.craigslist.de/mis/index.rss)<br/>
[kaiserslautern](http://kaiserslautern.craigslist.de/mis/index.rss)<br/>
[leipzig](http://leipzig.craigslist.de/mis/index.rss)<br/>
[munich](http://munich.craigslist.de/mis/index.rss)<br/>
[nuremberg](http://nuremberg.craigslist.de/mis/index.rss)<br/>
[stuttgart](http://stuttgart.craigslist.de/mis/index.rss)<br/>
[greece](http://athens.craigslist.gr/mis/index.rss)<br/>
[budapest](http://budapest.craigslist.org/mis/index.rss)<br/>
[reykjavik](http://reykjavik.craigslist.org/mis/index.rss)<br/>
[dublin](http://dublin.craigslist.org/mis/index.rss)<br/>
[bologna](http://bologna.craigslist.it/mis/index.rss)<br/>
[florence-tuscany](http://florence.craigslist.it/mis/index.rss)<br/>
[genoa](http://genoa.craigslist.it/mis/index.rss)<br/>
[milan](http://milan.craigslist.it/mis/index.rss)<br/>
[napoli-campania](http://naples.craigslist.it/mis/index.rss)<br/>
[perugia](http://perugia.craigslist.it/mis/index.rss)<br/>
[rome](http://rome.craigslist.it/mis/index.rss)<br/>
[sardinia](http://sardinia.craigslist.it/mis/index.rss)<br/>
[sicilia](http://sicily.craigslist.it/mis/index.rss)<br/>
[torino](http://torino.craigslist.it/mis/index.rss)<br/>
[venice-veneto](http://venice.craigslist.it/mis/index.rss)<br/>
[luxembourg](http://luxembourg.craigslist.org/mis/index.rss)<br/>
[amsterdam-randstad](http://amsterdam.craigslist.org/mis/index.rss)<br/>
[norway](http://oslo.craigslist.org/mis/index.rss)<br/>
[poland](http://warsaw.craigslist.pl/mis/index.rss)<br/>
[faro-algarve](http://faro.craigslist.pt/mis/index.rss)<br/>
[lisbon](http://lisbon.craigslist.pt/mis/index.rss)<br/>
[porto](http://porto.craigslist.pt/mis/index.rss)<br/>
[romania](http://bucharest.craigslist.org/mis/index.rss)<br/>
[moscow](http://moscow.craigslist.org/mis/index.rss)<br/>
[st-petersburg](http://stpetersburg.craigslist.org/mis/index.rss)<br/>
[alicante](http://alicante.craigslist.es/mis/index.rss)<br/>
[baleares](http://baleares.craigslist.es/mis/index.rss)<br/>
[barcelona](http://barcelona.craigslist.es/mis/index.rss)<br/>
[bilbao](http://bilbao.craigslist.es/mis/index.rss)<br/>
[cadiz](http://cadiz.craigslist.es/mis/index.rss)<br/>
[canarias](http://canarias.craigslist.es/mis/index.rss)<br/>
[granada](http://granada.craigslist.es/mis/index.rss)<br/>
[madrid](http://madrid.craigslist.es/mis/index.rss)<br/>
[malaga](http://malaga.craigslist.es/mis/index.rss)<br/>
[sevilla](http://sevilla.craigslist.es/mis/index.rss)<br/>
[valencia](http://valencia.craigslist.es/mis/index.rss)<br/>
[sweden](http://stockholm.craigslist.se/mis/index.rss)<br/>
[basel](http://basel.craigslist.ch/mis/index.rss)<br/>
[bern](http://bern.craigslist.ch/mis/index.rss)<br/>
[geneva](http://geneva.craigslist.ch/mis/index.rss)<br/>
[lausanne](http://lausanne.craigslist.ch/mis/index.rss)<br/>
[zurich](http://zurich.craigslist.ch/mis/index.rss)<br/>
[turkey](http://istanbul.craigslist.com.tr/mis/index.rss)<br/>
[ukraine](http://ukraine.craigslist.org/mis/index.rss)<br/>
[aberdeen](http://aberdeen.craigslist.co.uk/mis/index.rss)<br/>
[bath](http://bath.craigslist.co.uk/mis/index.rss)<br/>
[belfast](http://belfast.craigslist.co.uk/mis/index.rss)<br/>
[birmingham-west-mids](http://birmingham.craigslist.co.uk/mis/index.rss)<br/>
[brighton](http://brighton.craigslist.co.uk/mis/index.rss)<br/>
[bristol](http://bristol.craigslist.co.uk/mis/index.rss)<br/>
["cambridge](http://cambridge.craigslist.co.uk/mis/index.rss)<br/>
[cardiff-wales](http://cardiff.craigslist.co.uk/mis/index.rss)<br/>
[coventry](http://coventry.craigslist.co.uk/mis/index.rss)<br/>
[derby](http://derby.craigslist.co.uk/mis/index.rss)<br/>
[devon-&-cornwall](http://devon.craigslist.co.uk/mis/index.rss)<br/>
[dundee](http://dundee.craigslist.co.uk/mis/index.rss)<br/>
[east-anglia](http://norwich.craigslist.co.uk/mis/index.rss)<br/>
[east-midlands](http://eastmids.craigslist.co.uk/mis/index.rss)<br/>
[edinburgh](http://edinburgh.craigslist.co.uk/mis/index.rss)<br/>
[essex](http://essex.craigslist.co.uk/mis/index.rss)<br/>
[glasgow](http://glasgow.craigslist.co.uk/mis/index.rss)<br/>
[hampshire](http://hampshire.craigslist.co.uk/mis/index.rss)<br/>
[kent](http://kent.craigslist.co.uk/mis/index.rss)<br/>
[leeds](http://leeds.craigslist.co.uk/mis/index.rss)<br/>
[liverpool](http://liverpool.craigslist.co.uk/mis/index.rss)<br/>
[london](http://london.craigslist.co.uk/mis/index.rss)<br/>
[manchester](http://manchester.craigslist.co.uk/mis/index.rss)<br/>
[newcastle-ne-england](http://newcastle.craigslist.co.uk/mis/index.rss)<br/>
[nottingham](http://nottingham.craigslist.co.uk/mis/index.rss)<br/>
[oxford](http://oxford.craigslist.co.uk/mis/index.rss)<br/>
[sheffield](http://sheffield.craigslist.co.uk/mis/index.rss)<br/>
[bangladesh](http://bangladesh.craigslist.org/mis/index.rss)<br/>
[beijing](http://beijing.craigslist.com.cn/mis/index.rss)<br/>
[chengdu](http://chengdu.craigslist.com.cn/mis/index.rss)<br/>
[chongqing](http://chongqing.craigslist.com.cn/mis/index.rss)<br/>
[dalian](http://dalian.craigslist.com.cn/mis/index.rss)<br/>
[guangzhou](http://guangzhou.craigslist.com.cn/mis/index.rss)<br/>
[hangzhou](http://hangzhou.craigslist.com.cn/mis/index.rss)<br/>
[nanjing](http://nanjing.craigslist.com.cn/mis/index.rss)<br/>
[shanghai](http://shanghai.craigslist.com.cn/mis/index.rss)<br/>
[shenyang](http://shenyang.craigslist.com.cn/mis/index.rss)<br/>
[shenzhen](http://shenzhen.craigslist.com.cn/mis/index.rss)<br/>
[wuhan](http://wuhan.craigslist.com.cn/mis/index.rss)<br/>
[xi'an](http://xian.craigslist.com.cn/mis/index.rss)<br/>
[hong-kong](http://hongkong.craigslist.hk/mis/index.rss)<br/>
[ahmedabad](http://ahmedabad.craigslist.co.in/mis/index.rss)<br/>
[bangalore](http://bangalore.craigslist.co.in/mis/index.rss)<br/>
[bhubaneswar](http://bhubaneswar.craigslist.co.in/mis/index.rss)<br/>
[chandigarh](http://chandigarh.craigslist.co.in/mis/index.rss)<br/>
[chennai-(madras)](http://chennai.craigslist.co.in/mis/index.rss)<br/>
[delhi](http://delhi.craigslist.co.in/mis/index.rss)<br/>
[goa](http://goa.craigslist.co.in/mis/index.rss)<br/>
[hyderabad](http://hyderabad.craigslist.co.in/mis/index.rss)<br/>
[indore](http://indore.craigslist.co.in/mis/index.rss)<br/>
[jaipur](http://jaipur.craigslist.co.in/mis/index.rss)<br/>
[kerala](http://kerala.craigslist.co.in/mis/index.rss)<br/>
[kolkata-(calcutta)](http://kolkata.craigslist.co.in/mis/index.rss)<br/>
[lucknow](http://lucknow.craigslist.co.in/mis/index.rss)<br/>
[mumbai](http://mumbai.craigslist.co.in/mis/index.rss)<br/>
[pune](http://pune.craigslist.co.in/mis/index.rss)<br/>
[surat-surat](http://surat.craigslist.co.in/mis/index.rss)<br/>
[indonesia](http://jakarta.craigslist.org/mis/index.rss)<br/>
[iran](http://tehran.craigslist.org/mis/index.rss)<br/>
[iraq](http://baghdad.craigslist.org/mis/index.rss)<br/>
[haifa](http://haifa.craigslist.org/mis/index.rss)<br/>
[jerusalem](http://jerusalem.craigslist.org/mis/index.rss)<br/>
[tel-aviv](http://telaviv.craigslist.org/mis/index.rss)<br/>
[west-bank](http://ramallah.craigslist.org/mis/index.rss)<br/>
[fukuoka](http://fukuoka.craigslist.jp/mis/index.rss)<br/>
[hiroshima](http://hiroshima.craigslist.jp/mis/index.rss)<br/>
[nagoya](http://nagoya.craigslist.jp/mis/index.rss)<br/>
[okinawa](http://okinawa.craigslist.jp/mis/index.rss)<br/>
[osaka-kobe-kyoto](http://osaka.craigslist.jp/mis/index.rss)<br/>
[sapporo](http://sapporo.craigslist.jp/mis/index.rss)<br/>
[sendai](http://sendai.craigslist.jp/mis/index.rss)<br/>
[tokyo](http://tokyo.craigslist.jp/mis/index.rss)<br/>
[seoul](http://seoul.craigslist.co.kr/mis/index.rss)<br/>
[kuwait](http://kuwait.craigslist.org/mis/index.rss)<br/>
["beirut](http://beirut.craigslist.org/mis/index.rss)<br/>
[malaysia](http://malaysia.craigslist.org/mis/index.rss)<br/>
[pakistan](http://pakistan.craigslist.org/mis/index.rss)<br/>
[bacolod](http://bacolod.craigslist.com.ph/mis/index.rss)<br/>
[bicol-region](http://naga.craigslist.com.ph/mis/index.rss)<br/>
[cagayan-de-oro](http://cdo.craigslist.com.ph/mis/index.rss)<br/>
[cebu](http://cebu.craigslist.com.ph/mis/index.rss)<br/>
[davao-city](http://davaocity.craigslist.com.ph/mis/index.rss)<br/>
[iloilo](http://iloilo.craigslist.com.ph/mis/index.rss)<br/>
[manila](http://manila.craigslist.com.ph/mis/index.rss)<br/>
[pampanga](http://pampanga.craigslist.com.ph/mis/index.rss)<br/>
[zamboanga](http://zamboanga.craigslist.com.ph/mis/index.rss)<br/>
[singapore](http://singapore.craigslist.com.sg/mis/index.rss)<br/>
[taiwan](http://taipei.craigslist.com.tw/mis/index.rss)<br/>
[thailand](http://bangkok.craigslist.co.th/mis/index.rss)<br/>
[united-arab-emirates](http://dubai.craigslist.org/mis/index.rss)<br/>
[vietnam](http://vietnam.craigslist.org/mis/index.rss)<br/>
[adelaide](http://adelaide.craigslist.com.au/mis/index.rss)<br/>
[brisbane](http://brisbane.craigslist.com.au/mis/index.rss)<br/>
[cairns](http://cairns.craigslist.com.au/mis/index.rss)<br/>
[canberra](http://canberra.craigslist.com.au/mis/index.rss)<br/>
[darwin](http://darwin.craigslist.com.au/mis/index.rss)<br/>
[gold-coast](http://goldcoast.craigslist.com.au/mis/index.rss)<br/>
[melbourne](http://melbourne.craigslist.com.au/mis/index.rss)<br/>
["newcastle](http://ntl.craigslist.com.au/mis/index.rss)<br/>
[perth](http://perth.craigslist.com.au/mis/index.rss)<br/>
[sydney](http://sydney.craigslist.com.au/mis/index.rss)<br/>
[tasmania](http://hobart.craigslist.com.au/mis/index.rss)<br/>
[wollongong](http://wollongong.craigslist.com.au/mis/index.rss)<br/>
[auckland](http://auckland.craigslist.org/mis/index.rss)<br/>
[christchurch](http://christchurch.craigslist.org/mis/index.rss)<br/>
[dunedin](http://dunedin.craigslist.co.nz/mis/index.rss)<br/>
[wellington](http://wellington.craigslist.org/mis/index.rss)<br/>
[caribbean-islands](http://caribbean.craigslist.org/mis/index.rss)<br/>
[buenos-aires](http://buenosaires.craigslist.org/mis/index.rss)<br/>
[bolivia](http://lapaz.craigslist.org/mis/index.rss)<br/>
[belo-horizonte](http://belohorizonte.craigslist.org/mis/index.rss)<br/>
[brasilia](http://brasilia.craigslist.org/mis/index.rss)<br/>
[curitiba](http://curitiba.craigslist.org/mis/index.rss)<br/>
[fortaleza](http://fortaleza.craigslist.org/mis/index.rss)<br/>
[porto-alegre](http://portoalegre.craigslist.org/mis/index.rss)<br/>
[recife](http://recife.craigslist.org/mis/index.rss)<br/>
[rio-de-janeiro](http://rio.craigslist.org/mis/index.rss)<br/>
["salvador](http://salvador.craigslist.org/mis/index.rss)<br/>
[sao-paulo](http://saopaulo.craigslist.org/mis/index.rss)<br/>
[chile](http://santiago.craigslist.org/mis/index.rss)<br/>
[colombia](http://colombia.craigslist.org/mis/index.rss)<br/>
[costa-rica](http://costarica.craigslist.org/mis/index.rss)<br/>
[dominican-republic](http://santodomingo.craigslist.org/mis/index.rss)<br/>
[ecuador](http://quito.craigslist.org/mis/index.rss)<br/>
[el-salvador](http://elsalvador.craigslist.org/mis/index.rss)<br/>
[guatemala](http://guatemala.craigslist.org/mis/index.rss)<br/>
[acapulco](http://acapulco.craigslist.com.mx/mis/index.rss)<br/>
[baja-california-sur](http://bajasur.craigslist.com.mx/mis/index.rss)<br/>
[chihuahua](http://chihuahua.craigslist.com.mx/mis/index.rss)<br/>
[ciudad-juarez](http://juarez.craigslist.com.mx/mis/index.rss)<br/>
[guadalajara](http://guadalajara.craigslist.com.mx/mis/index.rss)<br/>
[guanajuato](http://guanajuato.craigslist.com.mx/mis/index.rss)<br/>
[hermosillo](http://hermosillo.craigslist.com.mx/mis/index.rss)<br/>
[mazatlan](http://mazatlan.craigslist.com.mx/mis/index.rss)<br/>
[mexico-city](http://mexicocity.craigslist.com.mx/mis/index.rss)<br/>
[monterrey](http://monterrey.craigslist.com.mx/mis/index.rss)<br/>
[oaxaca](http://oaxaca.craigslist.com.mx/mis/index.rss)<br/>
[puebla](http://puebla.craigslist.com.mx/mis/index.rss)<br/>
[puerto-vallarta](http://pv.craigslist.com.mx/mis/index.rss)<br/>
[tijuana](http://tijuana.craigslist.com.mx/mis/index.rss)<br/>
[veracruz](http://veracruz.craigslist.com.mx/mis/index.rss)<br/>
[yucatan](http://yucatan.craigslist.com.mx/mis/index.rss)<br/>
[nicaragua](http://managua.craigslist.org/mis/index.rss)<br/>
[panama](http://panama.craigslist.org/mis/index.rss)<br/>
[peru](http://lima.craigslist.org/mis/index.rss)<br/>
[montevideo](http://montevideo.craigslist.org/mis/index.rss)<br/>
[venezuela](http://caracas.craigslist.org/mis/index.rss)<br/>
[virgin-islands](http://virgin.craigslist.org/mis/index.rss)<br/>
[egypt](http://cairo.craigslist.org/mis/index.rss)<br/>
[ethiopia](http://addisababa.craigslist.org/mis/index.rss)<br/>
[ghana](http://accra.craigslist.org/mis/index.rss)<br/>
[kenya](http://kenya.craigslist.org/mis/index.rss)<br/>
[morocco](http://casablanca.craigslist.org/mis/index.rss)<br/>
[cape-town](http://capetown.craigslist.co.za/mis/index.rss)<br/>
[durban](http://durban.craigslist.co.za/mis/index.rss)<br/>
[johannesburg](http://johannesburg.craigslist.co.za/mis/index.rss)<br/>
[pretoria](http://pretoria.craigslist.co.za/mis/index.rss)<br/>
[tunisia](http://tunis.craigslist.org/mis/index.rss)<br/>


midiutil
------------------------------------------------------------------------------------------------
**Title**: midiutil

**Web address**: https://api.github.com/repos/csv/midiutil

**Date**: September 2013

**Description**: ========
MIDIUtil
========

------------
Introduction
------------

MIDIUtil is a pure Python library that allows one to write muti-track
Musical Instrument Digital Interface (MIDI) files from within Python
programs. It is object-oriented and allows one to create and write these
files with a minimum of fuss.

MIDIUtil isn't a full implementation of the MIDI specification. The actual
specification is a large, sprawling document which has organically grown
over the course of decades. I have selectively implemented some of the
more useful and common aspects of the specification. The choices have
been somewhat idiosyncratic; I largely implemented what I needed. When
I decided that it could be of use to other people I fleshed it out a bit,
but there are still things missing. Regardless, the code is fairly easy to
understand and well structured. Additions can be made to the library by
anyone with a good working knowledge of the MIDI file format and a good,
working knowledge of Python. Documentation for extending the library
is provided.

This software was originally developed with Python 2.5.2 and it makes use
of some features that were introduced in 2.5. I have used it extensively
in Python 2.6, so it should work in this or any later versions (but I
have not tested it on Python 3).

This software is distributed under an Open Source license and you are
free to use it as you see fit, provided that attribution is maintained.
See License.txt in the source distribution for details.

------------
Installation
------------

To use the library one can either install it on one's system or
copy the midiutil directory of the source distribution to your
project's directory (or to any directory pointed toA  by the PYTHONPATH
environment variable). For the Windows platforms an executable installer
is provided. Alternately the source distribution can be downloaded,
un-zipped (or un-tarred), and installed in the standard way:

    python setup.py install

On non-Windows platforms (Linux, MacOS, etc.) the software should be
installed in this way. MIDIUtil is pure Python and should work on any
platform to which Python has been ported.

If you do not wish to install in on your system, just copy the
src/midiutil directory to your project's directory or elsewhere on
your PYTHONPATH. If you're using this software in your own projects
you may want to consider distributing the library bundled with yours;
the library is small and self-contained, and such bundling makes things
more convenient for your users. The best way of doing this is probably
to copy the midiutil directory directly to your package directory and
then refer to it with a fully qualified name. This will prevent it from
conflicting with any version of the software that may be installed on
the target system.

-----------
Quick Start
-----------

Using the software is easy:

    o The package must be imported into your namespace
    o A MIDIFile object is created
    o Events (notes, tempo-changes, etc.) are added to the object
    o The MIDI file is written to disk.

Detailed documentation is provided; what follows is a simple example
to get you going quickly. In this example we'll create a one track MIDI
File, assign a name and tempo to the track, add a one beat middle-C to
the track, and write it to disk.

        #Import the library
        from midiutil.MidiFile import MIDIFile

        # Create the MIDIFile Object with 1 track
        MyMIDI = MIDIFile(1)

        # Tracks are numbered from zero. Times are measured in beats.
        track = 0
        time = 0

        # Add track name and tempo.
        MyMIDI.addTrackName(track,time,"Sample Track")
        MyMIDI.addTempo(track,time,120)

        # Add a note. addNote expects the following information:
        track = 0
        channel = 0
        pitch = 60
        time = 0
        duration = 1
        volume = 100

        # Now add the note.
        MyMIDI.addNote(track,channel,pitch,time,duration,volume)

        # And write it to disk.
        binfile = open("output.mid", 'wb')
        MyMIDI.writeFile(binfile)
        binfile.close()

There are several additional event types that can be added and there are
various options available for creating the MIDIFile object, but the above
is sufficient to begin using the library and creating note sequences.

The above code is found in machine-readable form in the examples directory.
A detailed class reference and documentation describing how to extend
the library is provided in the documentation directory.

Have fun!



scraperwiki-spreadsheet-download
------------------------------------------------------------------------------------------------
**Title**: scraperwiki-spreadsheet-download

**Web address**: https://api.github.com/repos/csv/scraperwiki-spreadsheet-download

**Date**: May 2013

**Description**: 

sparse-data-sonification
------------------------------------------------------------------------------------------------
**Title**: sparse-data-sonification

**Web address**: https://api.github.com/repos/csv/sparse-data-sonification

**Date**: September 2013

**Description**: Ambient music as a paridigm for sparse data sonification


transit-ridership
------------------------------------------------------------------------------------------------
**Title**: transit-ridership

**Web address**: https://api.github.com/repos/csv/transit-ridership

**Date**: June 2013

**Description**: Transit ridership
======
This is a music video about the ridership of Chicago buses and New York subways
for the past few years.

Each musical beat is a day, as is each vertical line. One musical instrument
represents the daily ridership of Chicago buses, and the other represens the daily
ridership of New York subways. (Both are measured at fare collection points.)

The vertical line on the graph also represents the daily ridership. It is colored
according to the city whose ridership was proportionately higher for that day.
The top of the line is the rate for the city with proportionately higher ridership,
and the bottom of the line is for the one with proportionately lower ridership.

Daily ridership is also represented by the white/grey ticks at the left and right
of the video. The brightest tick is for the present day, and the duller ticks are
for progressively earlier days going back seven days.

## How to
Generate the music. (This unnecessarily requires a graphical display.)

    Rscript music.r

Generate the video.

    Rscript video.r

Merge them.

    sh merge.sh

The resulting video is called `transit.webm`.

## File organization
Data import is in `data.r`.
Music stuff goes in `music.r`, and video stuff goes in `video.r`.
The resulting files are merged into a webm video in `merge.sh`.

## Legend


## Analysis


treasuryiorb
------------------------------------------------------------------------------------------------
**Title**: treasuryiorb

**Web address**: https://api.github.com/repos/csv/treasuryiorb

**Date**: July 2013

**Description**: 

all-my-charts
------------------------------------------------------------------------------------------------
**Title**: all-my-charts

**Web address**: https://api.github.com/repos/csvsoundsystem/all-my-charts

**Date**: September 2013

**Description**: # All My Charts

## A jQuery plugin to create dynamic multi-series HighCharts

### <a href="http://csvsoundsystem.github.io/all-my-charts/" target="_blank">View the example gallery</a>

### Usage

This plugin requires <a href="" target="_blank">`jQuery`</a>, <a href="https://github.com/misoproject/dataset" target="_blank">`miso.dataset`</a> with dependencies, and <a href="http://www.highcharts.com/" target="_blank">`highcharts`</a>. Require them and the main plugin like so:

````
<script src="path/to/jquery.1.10.1.min.js"></script>
<script src="path/to/miso.ds.deps.ie.0.4.1.js"></script>
<script src="path/to/highcharts.js"></script>
<script src="path/to/jquery.dynamic-highchart.js"></script>
````

````
$('#container').dynamicHighchart({
	data: "https://premium.scraperwiki.com/cc7znvq/47d80ae900e04f2/sql/?q=SELECT * FROM t2 WHERE year = 2012 AND type = 'withdrawal' AND (month = 1 OR month = 2) AND is_total = 0",
	chart_type: 'datetime',
	series: 'item',
	x: 'date',
	y: 'today',
	title: 'Jan and Feb withdrawals (2012)',
	y_axis_label: 'Today (millions)',
  min_datetick_interval: 24 * 3600 * 1000 // Don't let it go less than a day
});
````
Note: If you're not a non-profit, HighCharts has [some extra Terms & Conditions](http://shop.highsoft.com/highcharts.html).


#### Options

### data_format
Defaults to `'json'`. Other options: `'csv'`.

A string describing your data format. 

### delimiter
Defaults to `','`. Can be any string.
 
A string of your delimiter.

### data
Defaults to [treasury.io](http://www.treasury.io) endpoint `"https://premium.scraperwiki.com/cc7znvq/47d80ae900e04f2/sql/?q=SELECT * FROM t2 WHERE year = 2012 AND transaction_type = 'withdrawal' AND (month = 1 OR month = 2) AND is_total = 0"`

Can be either a string of the path to your data or a json object (an array of objects) itself. If it's a string it can point to a local path `data: '/data/my_data.csv` or a remote URL to call `data: 'http://data.com/endpoint'`.

### chart_type
Defaults to `'datetime'`. Can be `'datetime'` or `'categorical'`.

A string describing the chart you want. Choose `'datetime'` the former if you have an x-axis that is dates, i.e. a line chart. Choose `'categorical'` if you have categories, i.e. a bar chart.

### series
Defaults to blank, i.e. `''`.

A string of the column name that has all of the names of the things you want to chart, e.g. `'program_name'`. Can also be left blank if you only have one series or want to treat all series as one.

##### y
Defaults to `'today'`.

A string of the column name that holds your Y-axis values. 

### x
Defaults to `'date'`.
A string of the column name that holds your X-axis values. This field is only used for ``datetime`` charts). Dates are expected to be in [ISO-8601](http://en.wikipedia.org/wiki/ISO_8601) i.e. `YYYY-MM-DD`.

### container
Not set by default.

A string CSS selector, usually an id, for the div in which your chart will be created. Make sure this div has a set height and width. 

### title
Defaults to `'Chart Title'`.

A string that will be the title of your chart. Set this to an empty string, `''`, to kill that section (useful if you're short on space).

### y_axis_label
Defaults to `'Y-axis label'`. 

A string that will be the Y-axis label of your chart. Set this to an empty string, `''`, to kill that section (useful if you're short on space).

### min_datetick_interval

A number, in milliseconds, of the minimum tick interval for `datetime` charts. Defaults to `0`, (no limit). To make it so the x-axis never breaks down to intervals less than a day, set it to `24 * 3600 * 1000`, or the number of milliseconds in one day.

### color_palette
Defaults to [20 categeorical ColorBrewer colors](https://github.com/mbostock/d3/wiki/Ordinal-Scales#categorical-colors).

An array of hex code strings to color your data series.

#### Why HighCharts?

The aim of this library is to make it as easy as possible to create multi-series datetime and categorical charts. 

HighCharts has a [nicely documented API](http://api.highcharts.com/) and you get a lot of stuff for free like tooltips and a legend with clickable items that will show/hide your data series and resize your Y-scale. It's also easy to do things like [have all of your data series share one tooltip](http://api.highcharts.com/highcharts#tooltip.shared) just by editing the JSON config. In other words, customizing your chart doesn't require any JS coding, making it more deadline friendly. 

Other recent chart builder libraries such as [ChartBuilder](https://github.com/Quartz/Chartbuilder) and [NVD3](https://github.com/novus/nvd3) use D3 to make charts. Although D3 is extremely powerful, it also has browser compatibility issues and some of these libraries (in their current versions) lack the customization that Highcharts offers, or are designed to have a narrower focus. I've tested HighCharts down to IE8 and it works well. 


comma-people
------------------------------------------------------------------------------------------------
**Title**: comma-people

**Web address**: https://api.github.com/repos/csvsoundsystem/comma-people

**Date**: December 2012

**Description**: ## comma people ##

```
She came to J-School with a thirst for data.
She went to Susan's tutorial at Strata.
That's where I,
Caught her eye.
She told me that excel was drama.
I said "In that case, awk dash F quote comma."
She said "Fine."
And in thirty milliseconds time,
She said,

"I want to live like comma people,
I want to do whatever comma people do,
I want to code with comma people,
I want to code with comma people, like you."
Well what else could i do? -
I said "I'll add you to the google group"

I took her to a media start up.
I don't know why but I had to start it somewhere,
so it started there.
I said pretend you cant use D3.
She just laughed and said,
"Oh you're so funny."
I said "yeah?
Well I can't see anyone rendering SVG in here"

Are you sure you want to live like comma people
you want to speak however comma people speak
you want to hack with comma people
you want to hack with comma people
like me
But she was kind of out of wits,
she just smiled and read my gists

Rent a server in the cloud.
Tweet some articles on Tow.
Style some divs and push repos.
Pretend you never use S.O.
But still you'll never get it right
Cause when you're hacking late at night
watching errors fill your log
if you paid MTurkers they could do your job

You'll never code like the comma people.
You'll never do whatever comma people do.
You'll never live like comma people.
You'll never watch your tuesdays slip out view,
and hack at think with brews.
because there's nothing better to do.
```


csvsoundsystem.com
------------------------------------------------------------------------------------------------
**Title**: csvsoundsystem.com

**Web address**: https://api.github.com/repos/csvsoundsystem/csvsoundsystem.com

**Date**: August 2013

**Description**: csvsoundsystem.com
==================

Build like so

    ./index.html.sh > index.html

To add flashing lights and a CSV sticker for your site, add this to the bottom
of your site's HTML.

    <script src="http://csvsoundsystem.com/csv.js" type="text/javascript"></script>


federal-treasury-api
------------------------------------------------------------------------------------------------
**Title**: federal-treasury-api

**Web address**: https://api.github.com/repos/csvsoundsystem/federal-treasury-api

**Date**: October 2013

**Description**: # Federal Treasury API
```
   a1
   a!     |    |     a!a!a!a!a$?A+-a1a!a!a!a!a$?A+-a1
   a!    .| -- | -+
   a!   ' |    |  |  a!a!a!a!a$?A+-A+-a$?a!a!a!a!a!a!a!a!a!a!a!a!a$?A+-A+-a$?a!a!a!a!a!a$?A+-
   a!    `|    |
   a!     |`.  |     a!a!a!a!a$?A+-A+-a$?a!a!a!a!a!a!a!a!a!a!a!a!a$?A+-A+-a$?a!a!a!a!a!a$?A+-
   a!     |  `.|
   a!     |    |`.   a!a!a!a!a$?A+-A+-a$?a!a!a!a!a!a!a!a!a!a!a!a!a$?A+-A+-a$?a!a!a!a!a!a$?A+-
   a! +   |    |   '
   a! | _ | __ | _'   a!a!a!a$?A+-A+-a$?a!a!a!a!a!a!a!a!a!a!a!a!a$?A+-A+-a$?a!a!a!a!a!a$?A+-
   a!     |    |
   a!a!a!a!a!a!a!a!a!a!a!a!a!a!a!a!a!a!a!a!a!a!a$?A+-A+-a$?a!a!a!a!a!a!a!a!a!a!a!a!a$?A+-A+-a$?a!a!a!a!a!a$?A+-
   a!
   a!a!a!a!a!a!a!a!a!a!a!a!a!a!a!a!a!a!a!a!a!a!a$?A+-A+-a$?a!a!a!a!a!a!a!a!a!a!a!a!a$?A+-A+-a$?a!a!a!a!a!a$?A+-
   a!
   a!                      i!A+-a$?a!a!a!a!a!a!a!a!a!a!a!a!a$?A+-A+-a$?a!a!a!a!a!a$?A+-
   a!                                      i!A+-a$?a!a!a!a!a!a$?A+-
   a!
   a!
```

## About this branch
The `master` branch of `federal-treasury-api` contains code specific to running our project on [ScraperWiki](http://www.scraperwiki.com). the [`just-the-api`](https://github.com/csvsoundsystem/federal-treasury-api/tree/just-the-api) branch contains only the code needed to download the data locally and launch a queryable api. In other words, if you're just looking to get and host the database, go [here](https://github.com/csvsoundsystem/federal-treasury-api/tree/just-the-api).

## About the API
`federal-treasury-api` is the first-ever electronically-searchable database of the Federal government's daily cash spending and borrowing. It updates daily and the data can be exported in various formats and loaded into various systems.

## About the data
There are eight tables.

* I. Operating Cash Balance (`t1`)
* II. Deposits and Withdrawals (`t2`)
* IIIa. Public Debt Transactions (`t3a`)
* IIIb. Adjustment of Public Dept Transactions to Cash Basis (`t3b`)
* IIIc. Debt Subject to Limit (`t3c`)
* IV. Federal Tax Deposits (`t4`)
* V. Short-term Cash Investments (`t5`)
* VI. Incom Tax Refunds Issued (`t6`)

Check out thre comprehensive [data dictionary](https://github.com/csvsoundsystem/federal-treasury-api/wiki/Treasury.io-Data-Dictionary) and [treasury.io](http://treasury.io) for more information.

## Obtaining the data
Optionally set up a virtualenv. (You need this on ScraperWiki.)
Run this from the root of the current repository.

    virtualenv env

Install dependencies.

    pip install -r requirements.pip

Enable the git post-merge hook.

    cd .git/hooks
    ln -s ../../utils/post-merge .

### POSIX
This one command downloads the (new) fixies and converts them to an SQLite3 database.

    ./run.sh

### Windows
Run everything

    cd parser
    python download_and_parse_fms_fixies.py

## Testing the data
Various tests are contained in `tests`

Tests are run everyday with `./run.sh` and  the results are emailed to `csvsoundsystem@gmail.com`

## Cron
Run everything each day around 4:30 - right after the data has been released.

```
30 16 * * * cd path/to/federal-treasury-api && ./run.sh
```

####Optional: set up logging
```
30 16 * * * cd path/to/federal-treasury-api && ./run.sh >> run.log 2>> err.log
```

## Deploying to ScraperWiki
You can run this on any number of servers, but we happen to be using ScraperWiki.
You can check out their documentation [here](https://beta.scraperwiki.com/help/developer/)

### SSH
To use ScraperWiki, log in [here](https://beta.scraperwiki.com/login),
make a project, click the "SSH in" link, add your SSH key and SSH in.
Then you can SSH to the box like so.

    ssh cc7znvq@premium.scraperwiki.com

Or add this to your `~/.ssh/config`

    Host fms
    HostName premium.scraperwiki.com
    User cc7znvq

and just run

    ssh fms

### What this ScraperWiki account is
Some notes about how ScraperWiki works:

* We have a user account in a chroot jail.
* We don't have root, so we install Python packages in a virtualenv.
* Files in `/home/http` get served on the web.
* The database `/home/scraperwiki.sqlite` gets served from the SQLite web API.
    - NOTE: the `home/scraperwiki.sqlite` is simply a symbolic link to `data/treasury_data.db` generated by this command:
    ```ln -s data/treasury_data.db scraperwiki.sqlite```

The directions below still apply for any other service, of course.



fms-symphony
------------------------------------------------------------------------------------------------
**Title**: fms-symphony

**Web address**: https://api.github.com/repos/csvsoundsystem/fms-symphony

**Date**: September 2013

**Description**: FMS Symphony
====

* [`/`](http://csvsoundsystem.github.com/fms-symphony/)is the final symphony.
* [`/,`](http://csvsoundsystem.github.com/fms-symphony/,)is the analysis with plots and stuff.
* [`/net-dep-with/all_graphs.html`](http://csvsoundsystem.github.com/fms-symphony/net-dep-with/all_graphs.html)is a legacy url.
* [`/v0.1`](http://csvsoundsystem.github.com/fms-symphony/v0.1)is an earlier prototype.

Blogs

* [KA1/4chenstudio](http://www.kuechenstud.io/datenschau/podcast/ds008/)
* [Sunlight Foundation](http://sunlightfoundation.com/blog/2013/02/04/datafest-amazing-things-can-happen-in-a-very-short-time/)

Tweets

* [Michael Young](https://twitter.com/myoung/status/298213206130827264)
* [Marni Usheroff](https://twitter.com/musheroff/status/298169351750705154)
* [Michelle Holmes](https://twitter.com/mlh_holmes/status/298191817311129602)
* [Luis Daniel](https://twitter.com/luisdaniel12/status/298941357177851904)
* [Jake Hofman](https://twitter.com/jakehofman/status/298441664110944258)
* [Teresa Mahoney](https://twitter.com/TeresaMahoney/status/298200778512932864)
* [Teresa Bouza](https://twitter.com/TereBouza/status/298192475103834113)
* [Rebecca Williams](https://twitter.com/internetrebecca/status/300045421361897472)
* [Martha Garvey Jr.](https://twitter.com/NerdgrrlGarvey/status/299596129266446336)
* [Steven Romalewski](https://twitter.com/SR_spatial/status/299311902469791745)
* [Zach Seward](https://twitter.com/zseward/status/298956973385338880)
* [Matt Brehmer](https://twitter.com/mattbrehmer/status/298841163677450240)
* [Al Shaw](https://twitter.com/A_L/status/298818396676567040)
* [Harmony Institute](https://twitter.com/HInstitute/status/298469154955087872)
* [Hanna Sender](https://twitter.com/no_such_zone/status/298171932522708992)
* [Marni Usheroff](https://twitter.com/musheroff/status/298169351750705154)


fms-treasury-statements
------------------------------------------------------------------------------------------------
**Title**: fms-treasury-statements

**Web address**: https://api.github.com/repos/csvsoundsystem/fms-treasury-statements

**Date**: February 2013

**Description**: FMS Treasury Statement Parser
-----
This uses documentation-driven development; not all of this is implemented.

Load the `./archive` submodule; this repository saves the downloaded files

    git submodule init
    git submodule update

Activate the environment

    . activate

This provides `download`, `parse` and `test` functions.

    # Download today's file
    download $(date)

After you download, you might want to commit the submodule

    cd archive
    git add .
    git commit . -m downloaded\ a\ file
    git push
    cd ..
    git commit archive -m downloaded\ a\ file

The downloaded file gets saved in `./archive`.

Run the simple parser that just gets total withdrawals, total deposits and
net change

    parse-simple

Try the complete parser that doesn't work yet.

    parse ./archive/$filename

The tests expect fixtures to be in `./fixtures`.

    test

The `run` script does all of the downloading and parsing.

    run


CSV schems
--------

Parser just for Table II for now, in two sections: Deposits and Withdrawals

For Deposits:

- Take each line item inder "Federal Reserve Account" and write to separate line with Subitem column blank, except for: 
    - Deposits by States
    - Other Deposits
    For each of these, set Item = the main line item (Deposits by States, Other Deposits)
    Then set populate column Subitem with the indented line items that roll up into the item:
- Set isTotal = 0 for all line items except:
    - Total Other Deposits
    - Total Federal Reserve Account
    - Total Deposits (excluding transfers)
    Set isTotal = 1 to flag these items as subtotals of the other items
    Keep type as Deposit for all items 






An email
----
Hope the soundsystem is blaring without me . . . stuck in the office on deadline for the banks project so unfortunately won't be able to make it tonight. 

Anyway, just wanted to pass along the scraping prototype I've created (also uploaded in our Dropbox folder), so you guys can discuss if you want. 

Open to suggestions obviously on how to structure it differently, but I think it we get a scraper to pick out the data from the text file in this or a similar 
format, we'll get pretty much everything we need off of the deposits and withdrawals tables.

I will work on a prototype for the debt tables next, which should be even simpler than this.

And will scrape the FMS directory for all the text files so we have them handy in one place. 

Anyhow, have a look and maybe we can get together on the weekend or sometime next week to push ahead with the scraping.

Take care,

Cezary


On Fri, Nov 30, 2012 at 3:21 PM, Cezary Podkul wrote:

    Gents,

    I haven't had a chance go create that CSV yet so Tom it's probably best if you go to your toilet thing tomorrow.

    Should have it done by Tuesday night so we can pick things up again then. Mike just save the code from this week and we can hook up the scraped code to SQL 
lite and copare against the CSV output then.

    Have a good weekend!

    Cezary




    On Wed, Nov 28, 2012 at 12:14 AM, Thomas Levine wrote:

        Here are the steps I plan for the parsing bit. This is the order
        that I would do them in, but if either of you are up for starting
        before our party, start with step 1 or step 4.

        1. Someone(s) manually converts one source file to eight csv files
            (one per table).
        2. Someone (probably me) writes code to load those csv files into a sqlite
            database. This is really simple; it's just the schema and
            some flags for the sqlite3 command.
        3. Someone (probably me) writes code to run SQL on two different
            databases and compare the result.
        4. Someone writes tests using the above SQL thingy. Write SQL
            queries to be run on the dataset for one table or one day. The
            result should be the same regardless of whether we run them on
            the manually parsed data or the automatically parsed data.

        After that, we write the parser, and the parser saves the data to
        an SQLite database. We use the above tests for writing the parser.


        On 2012-11-27 21:19, Cezary Podkul wrote:

            Here is the directory:

            http://www.fms.treas.gov/dts/index.html [1]


            And here is what the text files look like:


            https://www.fms.treas.gov/fmsweb/viewDTSFiles?dir=w&fname=12112600.txt
            [2]

            This might help:

            http://www.ruby-forum.com/topic/184294#805303 [3]


            Lots of great stories to be done with it!

            If we scrape it we can do them!




fms-treasury-statements-analysis
------------------------------------------------------------------------------------------------
**Title**: fms-treasury-statements-analysis

**Web address**: https://api.github.com/repos/csvsoundsystem/fms-treasury-statements-analysis

**Date**: February 2013

**Description**: Given some data created by
[fms-treasury-statements](https://github.com/csvsoundsystem/fms-treasury-statements),
make something cool.

Load data from capitol words

    ./capitolwords-download.sh spending
    ./capitolwords-insert.py spending

## Questions

What spending shoots up in the middle of the month?

Why do we have this bimodal distribution? Why does spending stop at
$50 billion, and what causes it to jump past that?

## For the wiki

We used various R sonification, processing and visualization libraries to produce
our multisensory information interactive. These libraries include

* plyr
* reshape2
* csvsoundsystem
* aplpack

Me sought to produce an interactive that simultaneously displayed highly dimensional
data. We started with a 55-dimensional dataset

* 1 date
* 52 daily line items
* 1 daily interest rate
* 1 debt ceiling

We added a few variables to assist with our analyses.

* Day of week
* Day of month
* Rolling mean
* Rolling z-score
* Daily balance
* Variance of the 52 line items

And then reduced the now-61-dimensional dataset to an interactive.

We used principal component analysis to rotate the 52 line-items, and we plotted
the 15 highest-loaded components as Chernoff faces. We plotted interest rate and
balance, with an error width.

We represented similar data in audio. We selected chords based on the derivative
of cash balance, and we composed a melody based on the interest rate.


fms-treasury-statements-archive
------------------------------------------------------------------------------------------------
**Title**: fms-treasury-statements-archive

**Web address**: https://api.github.com/repos/csvsoundsystem/fms-treasury-statements-archive

**Date**: February 2013

**Description**: 

fms-treasury-statements-capitolwords
------------------------------------------------------------------------------------------------
**Title**: fms-treasury-statements-capitolwords

**Web address**: https://api.github.com/repos/csvsoundsystem/fms-treasury-statements-capitolwords

**Date**: February 2013

**Description**: Import the data

    ./download [phrase]
    sqlite3 /tmp/capitolwords.db < schema.sql
    ./insert [phrase]


fms-treasury-statements-site
------------------------------------------------------------------------------------------------
**Title**: fms-treasury-statements-site

**Web address**: https://api.github.com/repos/csvsoundsystem/fms-treasury-statements-site

**Date**: February 2013

**Description**: This website presents the data in light and sound.


haikubot
------------------------------------------------------------------------------------------------
**Title**: haikubot

**Web address**: https://api.github.com/repos/csvsoundsystem/haikubot

**Date**: March 2013

**Description**: ![haikubot](haikubot.png) [@ohhaikubot](http://www.twitter.com/ohhaikubot)
========================
-------------------------------------
# Generate a haiku bot on twitter.
-------------------------------------
Set this script up on an EC2 by first downloading the script into the default user directory:
```
wget http://raw.github.com/csvsoundsystem/haikubot/master/haikubot.py
```
Install pip:
```
sudo easy_install pip
```
Install python dependencies:
```
sudo pip install tweepy nltk 
```
Open up a python shell and run:
```
import nltk
nltk.download()
```
This will open a prompt to install `nltk` add-ons.  Select `cmudict` and download it.
<br/>
<br/>
Exit python and create a cronjob by entering:
```
sudo crontab -u ec2-user -e
```
This will open a vi screen where you'll insert:
```
*/30 * * * * /usr/bin/python /home/ec2-user/haikubot.py
```
Exit this screen and save the cron job by pressing `ZZ`
<br/>
<br/>
The script will send out no more than 5 haikus every half-hour, each spaced apart by 5 minutes.

## License
<a rel="license" href="http://creativecommons.org/licenses/by/3.0/deed.en_US"><img alt="Creative Commons License" style="border-width:0" src="http://i.creativecommons.org/l/by/3.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by/3.0/deed.en_US">Creative Commons Attribution 3.0 Unported License</a>.


haikugrams
------------------------------------------------------------------------------------------------
**Title**: haikugrams

**Web address**: https://api.github.com/repos/csvsoundsystem/haikugrams

**Date**: July 2013

**Description**: haikugrams
==========
_searching for those rarest of birds_
# About:

you've seen [haikubots](http://ohhaikubot.tumblr.com/) on tumblr, you've seem [anagram bots](http://twitter.com/anagramatron) on twitter. I want to find haikus that are anagrams of eachother. help me out! 

## install
```
git clone https://github.com/abelsonlive/haikugrams.git
```
## dependencies
```
git clone https://github.com/tumblr/pytumblr.git
cd pytumblr
sudo python setup.py install 
```
```
git clone https://github.com/tweepy/tweepy.git
cd tweepy
sudo python setup.py install 
```
```
sudo pip install nltk
```
```
python
>>> import nltk
>>> nltk.download()
>>> # select "cmudict" and "stopwords"
```
## config:
`haikugrams_tumblr.yml`:
```
blog: <your_tumblr_account_name>
consumer_key: xxxxxxxxxxxxxxxxxxxxxx
consumer_secret: xxxxxxxxxxxxxxxxxxxxxx
oauth_token: xxxxxxxxxxxxxxxxxxxxxx
oauth_token_secret: xxxxxxxxxxxxxxxxxxxxxx
```
`haikugrams_twitter.yml`:
```
consumer_key: xxxxxxxxxxxxxxxxxxxxxx
consumer_secret: xxxxxxxxxxxxxxxxxxxxxx
access_token: xxxxxxxxxxxxxxxxxxxxxx
access_token_secret: xxxxxxxxxxxxxxxxxxxxxx
```
## crontab
```
0,15,30,45 * * * * python <path_to_haikugrams_dir>/haikugrams.py 
```

## [tumblr](http://haikugrams.tumblr.com/)
## [twitter](http://twitter.com/haikugrams)



highchARRRts
------------------------------------------------------------------------------------------------
**Title**: highchARRRts

**Web address**: https://api.github.com/repos/csvsoundsystem/highchARRRts

**Date**: March 2013

**Description**: <center>![highChaRRRts](imgs/pirate-arr.png)</center>
##generate interactive plots in AAAARRRRRRRR#
============
_[by csv soundsystem](http://www.csvsoundsystem.com/)_

An R package to create interactive HighCharts in JavaScript from a dataframe. You'll want to use dcast in the reshape2 package to format it nicely. [More on that](http://www.cookbook-r.com/Manipulating_data/Converting_data_between_wide_and_long_format/)

<p>Currently supported chart types: bar, line, area, column.<p>
<p>Chart types to come: datetime x-axis (coming soon), scatterplot and others</p>
<p><strong>Terms of service note: This library uses the HighCharts.js library. Which, in addition to being awesome, has some particular terms of use if you are a for-profit venture. Before using it on your site, we suggest <a href="http://shop.highsoft.com/highcharts.html" target="_blank">you make sure you are squared with their terms</a>.</strong></p>
## Install the package

```
library("devtools")
install_github("highChaRRRts", "csvsoundsystem")
library("highChaRRRts")
```

## What data structure does it need?

There are two general data formats when dealing with dataframes and we'll use the terminology "long" and "wide". For reference, read [the R Cookbook explanation](http://www.cookbook-r.com/Manipulating_data/Converting_data_between_wide_and_long_format/) on the difference between the two. Generally, a long format is one where each row contains all the information for a given record so you can append records one at a time. 
```
name,variable,value
John,apples,2
John,oranges,3
Jane,pears,1
```
A wide format is one that is column dependent
```
name,apples,oranges,pears
John,2,3,0
Jane,0,01
```

highchARRRts prefers your data to be in long format except for categorical data, like the example above.

To convert from long to wide, use the dcast method in the reshape2 package for R. The [R Cookpost post](http://www.cookbook-r.com/Manipulating_data/Converting_data_between_wide_and_long_format/) can tell you about all the different parameters you might need to use depending on the complexity of your data. 

For everything else, long format is what you'll want, which is great because your data is probably already in long format.

## Usage

highchARRRts has the following required parameters
...

```
highchaRRRts(dcasted_df, type, main, xlab, ylab, output, pal)

```

<p>Then in your console, cd to the directory it just created, default name "chart_output". Then create a simple web server such as</p>
```
python -m SimpleHTTPServer
```
<p>Then point your browser to 0.0.0.0:8000</p>


## So...

```
highChaRRRts(dcasted_df, type="line", main="Title of the Chart", xlab="X Label", ylab="Y Label, output="chart_output", pal="RdYlBu")
```

## And a colorful example...

```
library("devtools")
install_github("highChaRRRts", "csvsoundsystem")
library("highChaRRRts")

# generate example data...
group <- paste0("group_", 1:11)
var1 <- rnorm(11, mean=100, sd=30)
var2 <- rnorm(11, mean=100, sd=30)
var3 <- rnorm(11, mean=100, sd=30)
var4 <- rnorm(11, mean=100, sd=30)
var5 <- rnorm(11, mean=100, sd=30)
df <- data.frame(group, var1, var2, var3, var4, var5)

highChaRRRts(df,
             type="column",
             main="Title of the chart",
             xlab="X Label",
             ylab="Y Label",
             output = "chart_output",
             pal="Spectral")
```
Check out the charts [here](http://csvsoundsystem.github.com/highchARRRts/example_templates/rainbow-bar-chart-column-line.html)

## Questions?
Ask the developers / maintainers:
- [@brianabelson](http://www.twitter.com/brianabelson)
- [@mhkeller](http://www.twitter.com/mhkeller)


highChartRuse
------------------------------------------------------------------------------------------------
**Title**: highChartRuse

**Web address**: https://api.github.com/repos/csvsoundsystem/highChartRuse

**Date**: October 2012

**Description**: highChartRuse
=============

hipstogram
------------------------------------------------------------------------------------------------
**Title**: hipstogram

**Web address**: https://api.github.com/repos/csvsoundsystem/hipstogram

**Date**: August 2012

**Description**: Hipstogram
======

> [Big Data Hipster](https://twitter.com/#!/bigdatahipster/status/202879778313867264): [@beaucronin](https://twitter.com/beaucronin) Pretty sure the hipstogram is a better visualization tool than the histogram. It's a photo of a histogram taken with Hipstamatic

    source('https://raw.github.com/tlevine/hipstogram/master/hipstogram.r')
    hipstogram(rnorm(42), filename='hipstogram.png')


one-year-anniversary-fireworks
------------------------------------------------------------------------------------------------
**Title**: one-year-anniversary-fireworks

**Web address**: https://api.github.com/repos/csvsoundsystem/one-year-anniversary-fireworks

**Date**: June 2013

**Description**: ## Exploding Firework Street View Greeting Card

As the name says, this creates a greeting card that, when you click the button, creates exploding fireworks above the building / house of your choosing from Google Street View.

To use, just replace the street view embed code in the body with the embed code of your choosing. You might also want to change the text color on the ``body`` from ``white`` to ``black`` depending on the color of the sky / building in street view.

## The card

<a href="https://csvsoundsystem.github.com/one-year-anniversary-fireworks" target="_blank">View the card</a>.

## Make your own!

[With this](https://github.com/mhkeller/exploding-firework-street-view-greeting-card).



OpenOpenNewsNews
------------------------------------------------------------------------------------------------
**Title**: OpenOpenNewsNews

**Web address**: https://api.github.com/repos/csvsoundsystem/OpenOpenNewsNews

**Date**: June 2013

**Description**: OpenOpenNewsNews
=================

pytreasuryio
------------------------------------------------------------------------------------------------
**Title**: pytreasuryio

**Web address**: https://api.github.com/repos/csvsoundsystem/pytreasuryio

**Date**: June 2013

**Description**: pytreasuryio
======
Access .. _treasury.io: http://treasury.io from Python.

This is a package consisting of a single, simple function for submitting ``SQL`` queries to .. _treasury.io: http://treasury.io from ``python``. While you could simply copy-and-paste the function from script-to-script, this makes it quicker and easier to get up and running!

It also has some helpers to make a Twitter bot from the treasury.io data.

Installation
--------
Install with pip.::

    pip install treasuryio

Example
---------

Basic query
~~~~~~~~~
Send an SQL query and receive a pandas data frame.::

    # Operating cash balances for May 22, 2013
    import treasuryio
    sql = 'SELECT * FROM "t1" WHERE "date" = \'2013-05-22\';'
    treasuryio.query(sql)

Twitter bot
~~~~~~~~~
Write a ``~/.twitter.yml`` file.::

    consumer_key: oeshaoduhsaousaoeuhts
    consumer_secret: b233tsao-enuhsaoehsunoesudtuhoelaouhs2uo
    access_token: 2349081293-astoehusatoehusaoeustahoeuhh2AOEUTAouhc
    access_token_secret: 9023uonshesuaHONETuoeuoeouo0eOHNEuhOuoeu
    
Define a function that produces the text of the tweet, and decorate it with the
``@treasurio.tweet`` decorator.::

    import treasuryio
    import humanize
    import math

    MIL = 1e6

    # Helpers to humanize numbers / dates
    def human_number(num):
        return humanize.intword(int(math.ceil(num))).lower()

    def human_date(date):
        return humanize.naturalday(date).title()

    @treasuryio.tweet
    def total_debt_tweet():
        df = treasuryio.query('SELECT date, close_today FROM t3c WHERE (item LIKE \'%subject to limit%\' AND year = 2013 AND month >=1) ORDER BY date DESC')

        # determine length of DataFrame
        end = len(df)-1

        # extract current amount and amount at the beginning of the year
        current_amt = df['close_today'][0]*MIL
        previous_amt = df['close_today'][end]*MIL

        # calculate change
        delta = abs(current_amt - previous_amt)

        # generate word to represnet the direction of change
        if current_amt > previous_amt:
            change = "increased"
        elif current_amt < previous_amt:
            change = "decreased"

        # humanize values
        # Notice the included ``human_date`` and ``human_number`` functions which simplify these values for you
        current_date = human_date(df['date'][0])
        amt = human_number(current_amt)
        delta = human_number(delta)
        previous_date = human_date(df['date'][end])

        # generate tweet
        vals = (current_date, amt, change, previous_date, 'http://treasury.io')
        return "As of %s, the US Gov is $%s in debt. This amount has %s since %s - %s" % vals

Then just run it.::

    total_debt_tweet()

You can get fancy by switching the functions that you use.::

    import treasuryio
    import random

    @treasurio.tweet
    def tweet_a():
        # ...

    @treasurio.tweet
    def tweet_b():
        # ...

    @treasurio.tweet
    def tweet_c():
        # ...

    random.choice([tweet_a, tweet_b, tweet_c])()


Rtreasuryio
------------------------------------------------------------------------------------------------
**Title**: Rtreasuryio

**Web address**: https://api.github.com/repos/csvsoundsystem/Rtreasuryio

**Date**: June 2013

**Description**: # Rtreasuryio
_Access [treasury.io](http://treasury.io) from `R`_

This is a package consisting of a single, simple function for submitting `SQL` queries to [treasury.io](http://treasury.io) from `R`. While you could simply copy-and-paste the function from script-to-script, this makes it quicker and easier to get up and running!

## Installation
```
library('devtools')
install_github('Rtreasuryio', 'csvsoundsystem')
library('Rtreasuryio')
```

## Example
```
# Operating cash balances for May 22, 2013
sql <- 'SELECT * FROM "t1" WHERE "date" = \'2013-05-22\';'
treasuryio(sql)
```

## Source Code
```
library(plyr)
library(utils)
library(RJSONIO)
library(RCurl)

treasuryio <- function(sql) {
  url = paste('https://premium.scraperwiki.com/cc7znvq/47d80ae900e04f2/sql/?q=', URLencode(sql), sep = '')
  handle <- getCurlHandle()
  body <- getURL(url, curl = handle)
  if (200 == getCurlInfo(handle)$response.code) {
    ldply(
      fromJSON(body),
      function(row) {as.data.frame(t(row))}
    )
  } else {
    stop(body)
  }
}
```


storyark
------------------------------------------------------------------------------------------------
**Title**: storyark

**Web address**: https://api.github.com/repos/csvsoundsystem/storyark

**Date**: December 2012

**Description**: 

symphio
------------------------------------------------------------------------------------------------
**Title**: symphio

**Web address**: https://api.github.com/repos/csvsoundsystem/symphio

**Date**: March 2013

**Description**: 

TheSoundOfQuicksort
------------------------------------------------------------------------------------------------
**Title**: TheSoundOfQuicksort

**Web address**: https://api.github.com/repos/csvsoundsystem/TheSoundOfQuicksort

**Date**: October 2012

**Description**: This is a visualization of quicksort, with sound. 
The pitch of the audio is changed according to which elements are shifting places. 

Programmed in C++ with OpenGL, SDL and Audiere.

Link to visualization: http://www.youtube.com/watch?v=m1PS8IR6Td0

treasury.io
------------------------------------------------------------------------------------------------
**Title**: treasury.io

**Web address**: https://api.github.com/repos/csvsoundsystem/treasury.io

**Date**: September 2013

**Description**: ```
  _|                                                                                _|
_|_|_|_|  _|  _|_|    _|_|      _|_|_|    _|_|_|  _|    _|  _|  _|_|  _|    _|            _|_|
  _|      _|_|      _|_|_|_|  _|    _|  _|_|      _|    _|  _|_|      _|    _|      _|  _|    _|
  _|      _|        _|        _|    _|      _|_|  _|    _|  _|        _|    _|      _|  _|    _|
    _|_|  _|          _|_|_|    _|_|_|  _|_|_|      _|_|_|  _|          _|_|_|  _|  _|    _|_|
                                                                            _|
                                                                        _|_|
```
===========
This is the source code for <a href="http://www.treasury.io" target="_blank">treasury.io</a>, the front-end for [federal-treasury-api](https://github.com/csvsoundsystem/federal-treasury-api/).

To preview the site locally, clone the repo and run `python -m SimpleHTTPServer` in the root directory. Then navigate to `localhost:8000`

Issues and wiki are disabled on for this repository on GitHub; if you'd like to submit to either, please use the
[federal-treasury-api](https://github.com/csvsoundsystem/federal-treasury-api) repository.


treasuryiojs
------------------------------------------------------------------------------------------------
**Title**: treasuryiojs

**Web address**: https://api.github.com/repos/csvsoundsystem/treasuryiojs

**Date**: July 2013

**Description**: treasuryiojs
=====
This is a [treasury.io](http://treasury.io) client for JavaScript. It can return a `csv` or a `json` response depending on whether you pass either string as the second argument..

## Install

    npm install treasuryio

## Run with a `csv` response

    var treasuryio = require('treasuryio')
    treasuryio('SELECT * FROM t1 LIMIT 10', 'csv', function(response){
        console.log(response);
    });

## Run with a `json` response

    var treasuryio = require('treasuryio')
    treasuryio('SELECT * FROM t1 LIMIT 10', 'json', function(response){
        console.log(response);
    });

## Test
To test that it works in the browser, open [this page](test/index.html).
You should see the first ten rows of of `t1`, Operating Cash Balance.

To test that it works in node, run

    cd test
    npm install treasuryio
    node test.js

It should print the first ten rows from `t1`, "Operating Cash Balance.


cache-attribution
------------------------------------------------------------------------------------------------
**Title**: cache-attribution

**Web address**: https://api.github.com/repos/appgen/cache-attribution

**Date**: April 2013

**Description**: 

cache-collabfinder
------------------------------------------------------------------------------------------------
**Title**: cache-collabfinder

**Web address**: https://api.github.com/repos/appgen/cache-collabfinder

**Date**: April 2013

**Description**: 

cache-comestibles
------------------------------------------------------------------------------------------------
**Title**: cache-comestibles

**Web address**: https://api.github.com/repos/appgen/cache-comestibles

**Date**: May 2013

**Description**: pantry
======

cache-deliveries
------------------------------------------------------------------------------------------------
**Title**: cache-deliveries

**Web address**: https://api.github.com/repos/appgen/cache-deliveries

**Date**: April 2013

**Description**: pantry
======

cache-flickr
------------------------------------------------------------------------------------------------
**Title**: cache-flickr

**Web address**: https://api.github.com/repos/appgen/cache-flickr

**Date**: April 2013

**Description**: pantry
======

cache-socrata
------------------------------------------------------------------------------------------------
**Title**: cache-socrata

**Web address**: https://api.github.com/repos/appgen/cache-socrata

**Date**: April 2013

**Description**: 

cache-wikimedia
------------------------------------------------------------------------------------------------
**Title**: cache-wikimedia

**Web address**: https://api.github.com/repos/appgen/cache-wikimedia

**Date**: April 2013

**Description**: 

cache-wikipedia
------------------------------------------------------------------------------------------------
**Title**: cache-wikipedia

**Web address**: https://api.github.com/repos/appgen/cache-wikipedia

**Date**: April 2013

**Description**: pantry
======

kitchen
------------------------------------------------------------------------------------------------
**Title**: kitchen

**Web address**: https://api.github.com/repos/appgen/kitchen

**Date**: June 2013

**Description**: Kitchen: Convert raw data into app-level data
====

Run `main.py` to build three files per app in the `comestibles`
directory. This file runs everything else in the directory.

## Architecture
Data from Socrata comes as views and rows.
We group datasets from Socrata based on their unionability, joinabality, &c.
We combine each group into *enriched datasets*.

We use other data sources to generate copy for each dataset. Specifically,
we currently use Collabfinder submissions. This is incorporated into the
enriched datasets.

Enriched datasets are saved as three files, each with the same base name.
The base name is a number that is used as a random seed. The extensions
are `csv` and `geojson`, both for the table, and `json` for associated
metadata. These files go into the `comestibles` directory.

## Copy-writing approach
For the copy-writing, I had envisioned parsing the text into a grammatical
tree, formulating the various trees into a grammar and generating text that
matched that structure, weighting the random text generation towards the
app name and the app topic for appropriate parts of speech. These pages might
help.

* http://nltk.googlecode.com/svn/trunk/doc/book/ch09.html
* http://stackoverflow.com/questions/15009656/how-to-use-nltk-to-generate-sentences-from-an-induced-grammar

A simpler approach might be an n-gram model on the part-of-speech-tagged
sentences, randomly switching appropriate words for the app name and topic.
I'd build one model for the first sentences and a separate model for the
full dataset, and I'd use the output of the first model as a context for
the second.

## Joining datasets
To do: Allow for fuzzier column name matches. For example, "Building Address"
should match "building_address", "NEIGHBORHOOD" should match "Neighborhood" and
"Building Tax Expenses" should match "Bldg tax expenses".

## Other notes
The metadata field in the view json is crazy.


main
------------------------------------------------------------------------------------------------
**Title**: main

**Web address**: https://api.github.com/repos/appgen/main

**Date**: June 2013

**Description**: AppGen
==========
**AppGen** generates and serves big apps for New York City's Big Apps
competition. This repository is the main repository for all of AppGen's
sub-repositories. Once we get tired of using Facebook for project management,
we might decide to use the present repository's issue tracker for the overall
project.

## How to

Clone like so.

    git clone --recursive git@github.com:appgen/main.git appgen

With nested submodules, you'll probably want to commit a few submodules after
you've made a bunch of changes.

    ./commit

## Architecture
AppGen is divided into three major parts, each of which is a submodule
of the present repository. They are listed below in the order by which
data pass through them.

1. Pantry
2. Kitchen
3. Menu

Each major part contains more git submodules. We divide these submodules
into *input* and *output* submodules. A major part edits the contents of
its output submodules. The following major part includes this output
submodule as an input submodule. Each major part can be checked out
independently of the present repository. The structure of the submodules
enforces the immutability of data.

Large and dynamic data are stored in S3 rather than in git submodules.
This includes

* Raw CSV files from Socrata (rows.appgen.me)
* App parameters (comestibles.appgen.me)

### Components

#### Pantry
The pantry downloads data and saves it in the most raw possible form.
Downloader scripts are stored in the pantry repository, and the actual
downloads are stored in subdirectories. Almost **all** external data should be
downloaded in the pantry. One exception is interactive data requests
inside of the menu (web application), like calling the Facebook API. The
pantry reads some of the data it downloads, but only enough to download more
data.

#### Kitchen
The kitchen takes the pantry (with its various data submodules) as input and
converts them into a form that the generated web applications can use. This
involves randomization. For example, geoJSON files and app names are produced
in the kitchen. These data are all stored in comestibles.appgen.me, with two
files per seed. `:seed.csv` is the combined table, and `:seed.json` is the
metadata, including all of the view json files for all of the parent datasets.

The `:seed.json` file is a serialized object with following schema. The main
app parameters are at the root of object.

* `seed` is the identifier. It's actually the hash of the schema, and it's used
    to seed random text generation in the kitchen.
* `name` is the app name
* `collabfinder_{need,what,why}` is the answers to the collabfinder questions.
* `combined_title` is a long title that was formed by combining the descriptions
    of the parent datasets.
* `keywords` is a list of keywords drawn from tags and column names.
* `logo` is empty for now.

`sources` is all of the metadata objects for all of the parent datasets.

The `:seed.csv` file is the union of all of the datasets with an additional
`source_dataset` column, which is the Socrata 4x4 identifier for the particular
dataset that that row came from.

#### Menu
The menu takes the comestibles as input and displays a webpage wherein people
can browse apps, claim an app, deploy the app to Heroku and submit the app to
CollabFinder. It saves the information that people enter (their names and the
apps that they claimed) in a database of sorts.

It also serves the various specified apps based on the comestibles and the
claims. (It uses placeholder for apps that have not yet been claimed.)

It randomizes things that don't depend on the data, such as the map's tile
server and the overall app's theme; these things depend only on the random
seed, so they don't need to look at the earlier data.

It queries some external services. These include

* Facebook (for authentication)
* Url2PNG (for taking screenshots of generated apps)

It also contains a mechanism for rendering the various apps as static files.
These static files are saved in `deliveries`.

I (Tom) had considered separating the menu into separate apps for the generated
apps and the apps browser, but there seemed to be enough two-way communication
between these two components that they should be seen as one.

### Submodule conventions
All git repositories are stored on GitHub under the appgen organization.
Output submodules are specified (in `.gitmodules`) with the standard SSH
protocal. Input submodules are specified with git-ssh (`git://`).

When pushed to GitHub, repositories used for passing data between components 
should be prefixed with the string `cache-`. For example, a `socrata` submodule
inside of the `pantry`, with the path "`pantry/socrata`" in the present
repository, is named [pantry-socrata](https://github.com:appgen/cache-socrata).

The present repository is named [main](https://github.com:appgen/main).
Other repositories belonging to the appgen organization but not inside of the
present super-repository should use names that begin with a period (`.`).
For example, if we conduct an analysis of our server logs, we might name it
"`.server-log-analysis`".

### Software that each component uses
The pantry contains many tiny programs for downloading data. They are quite
independent, and each can be written in a different language.

The various datasets are connected in the kitchen, so it is more important that
they be able to interface nicely and that software be written in a way that
makes debugging easy. Also, much computation occurs in the kitchen, so the
language must be reasonably fast. Finally, some natural language processing is
going to happen. Considering all of these things, the pantry will probably
be written in Python, with NLTK, pandas, numpy, scikit-learn, &c. That said,
it wouldn't be hard to write some of this in other languages.

The menu is one Sinatra application. Let's keep all of this in Ruby.

## Further explanation of business logic

### Copywriting
We download appidy app pitches and the like, then we parse them to produce a
grammar. Separately, we download Wikipedia articles and the like about the
various dataset and competition topics; we use these word frequencies as the
dictionary. We fill in this grammar with this dictionary, throwing in the app
name where we can.

## Other resources
Big Apps information

* http://www.nycedc.com/services/nyc-bigapps/past-competitions
* http://nycbigapps.com/prizes
* http://nycbigapps.com/rules
* http://2010.nycbigapps.com

Federal data is now allowed. It's also Socrata.

https://explore.data.gov/catalog/raw



manual-questions
------------------------------------------------------------------------------------------------
**Title**: manual-questions

**Web address**: https://api.github.com/repos/appgen/manual-questions

**Date**: April 2013

**Description**: Manual app questions
================
Here we copy answers to the following questions

* __What__ are you making?
* __Why__ are you making it?
* What do you __need__ help with?

Each directory within the present directory contains to the three answers for
a particular entity (app, person, company, department, &c.), in files named
`what`, `why` and `need`. The directory name is used as a unique identifier.

The following search finds LinkedIn InMails and creates directories for them.

    notmuch show from:hit-reply@linkedin.com |
    sed -n 's/.*id:\([^ ]*\) .*/\1/p' |
    xargs mkdir


menu
------------------------------------------------------------------------------------------------
**Title**: menu

**Web address**: https://api.github.com/repos/appgen/menu

**Date**: August 2013

**Description**: AppGen: Menu
===
## How to
To start webapp
```
$ bundle install
$ bundle exec shotgun
```

Then visit...
http://localhost:9393/

#Production
  * http://appgen.me (via appgen.herokuapp.com, mirrored at bigappgen.herokuapp.com)
  * To browse apps: /browse/a
  * Standalone apps: /a
  * NYC Open Data Portal Audit: /audit


## Important details

### App name
As we develop AppGen, existing apps will randomly change. That is fine; it
reflects the improvements on the apps. However, the name of the app should
should not change very much. This is because each app will be associated with
a particular domain name that we're going to tell people about.

In order to keep the name consistent, we are setting the random seed inside of
`FakeApp.getRandomName`. **Don't change this**. In fact, you should probably.
avoid changing anything about this method.

## Things to randomize

* Identity
  * ~~App name (from a startup name generator)~~
  * ~~App text (from the TED talk generator)~~
  * App logo (from a logo generator)
  * Url file extension (php, asp, cgi)
  * ~~Server header~~
* Chrome
  * ~~Font size of title~~
  * ~~Typeface~~
  * Background texture
  * ~~Colors~~
  * Footer stickiness
  * ~~Tile server~~
  * ~~Button curviness~~
  * ~~Map size~~

I propose these routes

    /
    /a -> Display titles, screenshots, &c. of a few apps with links to the apps.
    /a/:seed -> An app
    /a/:seed/map -> An app's map page
    /a/:seed/contact -> An app's contact page
    /a/:seed/styles.css -> And so on



pantry
------------------------------------------------------------------------------------------------
**Title**: pantry

**Web address**: https://api.github.com/repos/appgen/pantry

**Date**: April 2013

**Description**: AppGen Pantry
====

Run everything

    ./run

## Socrata SODA API
Get the schema and metadata of a dataset (with httpie).

    http --json https://data.cityofnewyork.us/views/f4yq-wry5

More about the API

    http://dev.socrata.com/docs/endpoints

We can probably use the API directly in our apps.

## Appy websites
* New York Times (Joshua Brustein)
* Mashable Apps
* Y Combinator
* Hacker News
* The Next Web
* ReadWriteWeb
* Code for America
* BetaBeat
* The Onion
* Bloomberg Tech
* Venture Beat
* Ars Technica
* Good Technology
* GitHub readme files
* Indigogo Technology
* Wired
* Gizmodo
* Sunlight Foundation

## Image source ideas
We want an image source for app picture backgrounds,
stock photography, &c. Ideas:

* http://wikimedia.7.x6.nabble.com/can-I-use-the-API-to-search-for-images-in-commons-wikimedia-org-td2549464.html
* http://meta.wikimedia.org/wiki/User-Agent_policy
* http://stackoverflow.com/questions/1467336/downloading-images-from-wikimedia-commons
* https://commons.wikimedia.org/wiki/Commons:API/MediaWiki
* http://www.mediawiki.org/wiki/API:Allimages
* http://en.wikipedia.org/w/api.php?action=query&list=allimages&aiprop=url&format=xml&ailimit=10&aifrom=Albert


proxy
------------------------------------------------------------------------------------------------
**Title**: proxy

**Web address**: https://api.github.com/repos/appgen/proxy

**Date**: June 2013

**Description**: 

risley.org
------------------------------------------------------------------------------------------------
**Title**: risley.org

**Web address**: https://api.github.com/repos/risley/risley.org

**Date**: September 2011

**Description**: Risley website
=============

This is a scrape of the [official website](http://risley.org)
from Monday, September 19, 2011 with
with all .php file extensions were changed to .html.

It is hosted [here](http://risley.thomaslevine.com/homepage/)
in case weird things happen at the official site
or in case we want to add unofficial things to it.
It also [loads faster](http://whichloadsfaster.com/?l=risley.org&r=risley.thomaslevine.com/homepage).

We can go back in time minus images
[here](http://wayback.archive.org/web/*/http://risley.org/*).


test
------------------------------------------------------------------------------------------------
**Title**: test

**Web address**: https://api.github.com/repos/risley/test

**Date**: September 2011

**Description**: # the risley purity test

 * Home: http://risleyhall.org/test/
 * Maintainer: david.schoonover@gmail.com


ideas
------------------------------------------------------------------------------------------------
**Title**: ideas

**Web address**: https://api.github.com/repos/mapshit/ideas

**Date**: December 2012

**Description**: 

mapshit.github.com
------------------------------------------------------------------------------------------------
**Title**: mapshit.github.com

**Web address**: https://api.github.com/repos/mapshit/mapshit.github.com

**Date**: December 2012

**Description**: This is the outcome of the [@mapshit](http://twitter.com/mapshit) team's work at the
[#Sanitation Hackathon](http://toilethackers.org) that ran in New York City from December 2nd,
2012 to December 3rd, 2012. This team won the first place.


pri-data-parser
------------------------------------------------------------------------------------------------
**Title**: pri-data-parser

**Web address**: https://api.github.com/repos/mapshit/pri-data-parser

**Date**: December 2012

**Description**: We start at [this page](http://nirmalgrampuraskar.nic.in/Report/RptGPAwardedSummaryTill2010.aspx).
That has a bunch of state names. Clicking one state gets to another table.
That other table has a "Total" link at the end. We want that page.
We want these columns in the final dataset

* Year
* State Name
* District Name
* Block Name
* Panchayat Name 

We downloaded the pages manually, and we have a thingy to convert them to the
msewage format. Install the
[msewage importer](https://github.com/jcmuller/msewage-importer).

    gem install msewage-importer

Then run something like this.

    . activate
    cd data
    find . -exec msewage.sh {} \;

Or to generate a csv

    . activate
    cd data
    echo Year,State Name,District Name,Block Name,Panchayat Name > ../pri.csv
    find . -name *.html -exec csv.sh {} \; >> ../pri.csv

This string is useful for geocoding in CartoDB.

    {panchayat_name}, {block_name}, {district_name}, {state_name}, India

[Here](http://tlevine.cartodb.com/tables/pri/embed_map?title=true&description=true&search=false&shareable=false&sql=&zoom=3&center_lat=25.958044673317843&center_lon=75.76171875)'s the map.


-----------------------------------------------------------------------------

Except as indicated above on this exhibit, I have no inventions, improvements or original works to disclose
pursuant to Section 4(a) of this Agreement and no agreements to disclose pursuant to Section 10(b) of this
Agreement.

EMPLOYEE:
